{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from preprocessing import data_transformation\n",
    "from similarity import calculate_similarity_matrix\n",
    "\n",
    "from model import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root=\"/\", name=\"MUTAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split: Train test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```train_dataset```: for training model<br/>\n",
    "```val_dataset```: evaluate model for hyperparameter tunning<br/>\n",
    "```test_dataset```: testing model after complete training<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ts, vl = 0.8, 0.1, 0.1\n",
    "dslen = len(dataset)\n",
    "tri = round(tr*dslen)\n",
    "tsi = round((tr+ts)*dslen)\n",
    "train_dataset = dataset[:tri]\n",
    "test_dataset = dataset[tri:tsi]\n",
    "val_dataset = dataset[tsi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)\n",
    "test_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)\n",
    "val_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0])\n",
      "val loader\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0])\n",
      "test loader\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print('train loader')\n",
    "for data in train_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('val loader')\n",
    "for data in val_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('test loader')\n",
    "for data in test_loader:\n",
    "    print(data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base(\n",
       "  (conv1): GCNConv(7, 64)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = Base(dataset, 64)\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9110, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_base(model, loader):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for data in loader:\n",
    "        emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(h, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss\n",
    "    #     print(h[0])\n",
    "    # print(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_base(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        pred = h.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct/len(loader.dataset)\n",
    "\n",
    "base = Base(dataset, 64)\n",
    "train_base(base, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; loss: 0.94; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 1; loss: 0.92; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 2; loss: 0.92; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 3; loss: 0.91; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 4; loss: 0.9; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 5; loss: 0.91; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 6; loss: 0.89; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 7; loss: 0.89; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 8; loss: 0.88; train_acc: 0.72; test_acc: 0.58\n",
      "epoch 9; loss: 0.89; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 10; loss: 0.8; train_acc: 0.73; test_acc: 0.58\n",
      "epoch 11; loss: 0.95; train_acc: 0.68; test_acc: 0.68\n",
      "epoch 12; loss: 0.62; train_acc: 0.34; test_acc: 0.37\n",
      "epoch 13; loss: 0.97; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 14; loss: 0.66; train_acc: 0.73; test_acc: 0.58\n",
      "epoch 15; loss: 0.92; train_acc: 0.75; test_acc: 0.63\n",
      "epoch 16; loss: 0.72; train_acc: 0.65; test_acc: 0.53\n",
      "epoch 17; loss: 0.93; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 18; loss: 0.64; train_acc: 0.67; test_acc: 0.63\n",
      "epoch 19; loss: 0.92; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 20; loss: 0.65; train_acc: 0.68; test_acc: 0.63\n",
      "epoch 21; loss: 0.91; train_acc: 0.75; test_acc: 0.63\n",
      "epoch 22; loss: 0.63; train_acc: 0.65; test_acc: 0.53\n",
      "epoch 23; loss: 0.9; train_acc: 0.73; test_acc: 0.58\n",
      "epoch 24; loss: 0.69; train_acc: 0.71; test_acc: 0.58\n",
      "epoch 25; loss: 0.88; train_acc: 0.73; test_acc: 0.58\n",
      "epoch 26; loss: 0.7; train_acc: 0.67; test_acc: 0.53\n",
      "epoch 27; loss: 0.9; train_acc: 0.75; test_acc: 0.58\n",
      "epoch 28; loss: 0.62; train_acc: 0.67; test_acc: 0.63\n",
      "epoch 29; loss: 0.91; train_acc: 0.75; test_acc: 0.63\n",
      "epoch 30; loss: 0.66; train_acc: 0.71; test_acc: 0.58\n",
      "epoch 31; loss: 0.89; train_acc: 0.75; test_acc: 0.63\n",
      "epoch 32; loss: 0.65; train_acc: 0.71; test_acc: 0.63\n",
      "epoch 33; loss: 0.9; train_acc: 0.73; test_acc: 0.58\n",
      "epoch 34; loss: 0.83; train_acc: 0.77; test_acc: 0.63\n",
      "epoch 35; loss: 0.83; train_acc: 0.73; test_acc: 0.58\n",
      "epoch 36; loss: 0.81; train_acc: 0.73; test_acc: 0.58\n",
      "epoch 37; loss: 0.82; train_acc: 0.73; test_acc: 0.53\n",
      "epoch 38; loss: 0.77; train_acc: 0.77; test_acc: 0.63\n",
      "epoch 39; loss: 0.84; train_acc: 0.73; test_acc: 0.58\n",
      "epoch 40; loss: 0.76; train_acc: 0.73; test_acc: 0.63\n",
      "epoch 41; loss: 0.87; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 42; loss: 0.71; train_acc: 0.69; test_acc: 0.58\n",
      "epoch 43; loss: 0.89; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 44; loss: 0.8; train_acc: 0.77; test_acc: 0.63\n",
      "epoch 45; loss: 0.85; train_acc: 0.74; test_acc: 0.53\n",
      "epoch 46; loss: 0.8; train_acc: 0.76; test_acc: 0.58\n",
      "epoch 47; loss: 0.83; train_acc: 0.73; test_acc: 0.53\n",
      "epoch 48; loss: 0.78; train_acc: 0.77; test_acc: 0.63\n",
      "epoch 49; loss: 0.85; train_acc: 0.73; test_acc: 0.58\n",
      "epoch 50; loss: 0.77; train_acc: 0.75; test_acc: 0.68\n",
      "epoch 51; loss: 0.88; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 52; loss: 0.71; train_acc: 0.73; test_acc: 0.63\n",
      "epoch 53; loss: 0.87; train_acc: 0.74; test_acc: 0.63\n",
      "epoch 54; loss: 0.77; train_acc: 0.73; test_acc: 0.63\n",
      "epoch 55; loss: 0.9; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 56; loss: 0.77; train_acc: 0.73; test_acc: 0.63\n",
      "epoch 57; loss: 0.88; train_acc: 0.73; test_acc: 0.53\n",
      "epoch 58; loss: 0.84; train_acc: 0.76; test_acc: 0.58\n",
      "epoch 59; loss: 0.86; train_acc: 0.71; test_acc: 0.53\n",
      "epoch 60; loss: 0.8; train_acc: 0.76; test_acc: 0.58\n",
      "epoch 61; loss: 0.87; train_acc: 0.73; test_acc: 0.53\n",
      "epoch 62; loss: 0.78; train_acc: 0.76; test_acc: 0.68\n",
      "epoch 63; loss: 0.87; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 64; loss: 0.7; train_acc: 0.71; test_acc: 0.63\n",
      "epoch 65; loss: 0.97; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 66; loss: 0.81; train_acc: 0.76; test_acc: 0.68\n",
      "epoch 67; loss: 0.87; train_acc: 0.77; test_acc: 0.63\n",
      "epoch 68; loss: 0.84; train_acc: 0.77; test_acc: 0.63\n",
      "epoch 69; loss: 0.88; train_acc: 0.73; test_acc: 0.47\n",
      "epoch 70; loss: 0.79; train_acc: 0.76; test_acc: 0.68\n",
      "epoch 71; loss: 0.92; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 72; loss: 0.78; train_acc: 0.77; test_acc: 0.68\n",
      "epoch 73; loss: 0.83; train_acc: 0.73; test_acc: 0.53\n",
      "epoch 74; loss: 0.82; train_acc: 0.77; test_acc: 0.58\n",
      "epoch 75; loss: 0.87; train_acc: 0.71; test_acc: 0.47\n",
      "epoch 76; loss: 0.79; train_acc: 0.76; test_acc: 0.68\n",
      "epoch 77; loss: 0.9; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 78; loss: 0.73; train_acc: 0.69; test_acc: 0.58\n",
      "epoch 79; loss: 0.92; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 80; loss: 0.81; train_acc: 0.76; test_acc: 0.68\n",
      "epoch 81; loss: 0.9; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 82; loss: 0.73; train_acc: 0.76; test_acc: 0.68\n",
      "epoch 83; loss: 0.89; train_acc: 0.73; test_acc: 0.53\n",
      "epoch 84; loss: 0.87; train_acc: 0.76; test_acc: 0.58\n",
      "epoch 85; loss: 0.89; train_acc: 0.71; test_acc: 0.53\n",
      "epoch 86; loss: 0.8; train_acc: 0.77; test_acc: 0.63\n",
      "epoch 87; loss: 0.92; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 88; loss: 0.76; train_acc: 0.71; test_acc: 0.63\n",
      "epoch 89; loss: 0.93; train_acc: 0.71; test_acc: 0.47\n",
      "epoch 90; loss: 0.8; train_acc: 0.69; test_acc: 0.58\n",
      "epoch 91; loss: 0.92; train_acc: 0.74; test_acc: 0.58\n",
      "epoch 92; loss: 0.73; train_acc: 0.77; test_acc: 0.68\n",
      "epoch 93; loss: 0.85; train_acc: 0.76; test_acc: 0.58\n",
      "epoch 94; loss: 0.78; train_acc: 0.76; test_acc: 0.63\n",
      "epoch 95; loss: 0.89; train_acc: 0.71; test_acc: 0.53\n",
      "epoch 96; loss: 0.78; train_acc: 0.78; test_acc: 0.68\n",
      "epoch 97; loss: 0.86; train_acc: 0.71; test_acc: 0.58\n",
      "epoch 98; loss: 0.72; train_acc: 0.69; test_acc: 0.58\n",
      "epoch 99; loss: 0.97; train_acc: 0.73; test_acc: 0.58\n",
      "Accuracy: 0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "base = Base(dataset, 64)\n",
    "train_base(base, train_loader)\n",
    "\n",
    "# Train\n",
    "for _ in range(epoch):\n",
    "    loss = round(train_base(base, train_loader).item(), 2)\n",
    "    train_acc = round(test_base(base, train_loader), 2)\n",
    "    val_acc = round(test_base(base, val_loader), 2)\n",
    "    \n",
    "    print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; test_acc: {val_acc}')\n",
    "\n",
    "# Test\n",
    "test = test_base(base, test_loader)\n",
    "print(f'Accuracy: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        G = data_transformation(edge_index, feature_emb)\n",
    "        S = calculate_similarity_matrix(G)\n",
    "        # clustering = AffinityPropagation(affinity='precomputed', random_state=123, max_iter=200).fit(S)\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h\n",
    "\n",
    "    def data_transformation():\n",
    "        print('s')\n",
    "        \n",
    "\n",
    "\n",
    "experiment = Experiment(dataset, 64)\n",
    "# train_base(experiment, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 2590], x=[1168, 7], edge_attr=[2590, 4], y=[64], batch=[1168], ptr=[65])\n",
      "tensor([ 0,  0,  0,  ..., 63, 63, 63])\n",
      "edge_index tensor([[   0,    0,    1,  ..., 1165, 1166, 1167],\n",
      "        [   1,    5,    0,  ..., 1167, 1165, 1165]])\n",
      "batch tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.]])\n",
      "ptr tensor([   0,   17,   30,   43,   62,   73,  101,  117,  137,  149,  166,  183,\n",
      "         203,  225,  238,  257,  279,  290,  307,  320,  338,  356,  373,  396,\n",
      "         423,  440,  453,  476,  493,  516,  539,  561,  585,  608,  621,  638,\n",
      "         652,  669,  684,  699,  712,  729,  742,  761,  778,  790,  813,  835,\n",
      "         852,  872,  888,  914,  940,  959,  978,  992, 1009, 1030, 1055, 1078,\n",
      "        1097, 1114, 1125, 1148, 1168])\n"
     ]
    }
   ],
   "source": [
    "batch1 = None\n",
    "for batch in train_loader:\n",
    "    batch1 = batch\n",
    "    break\n",
    "print(batch1)\n",
    "print(batch1.batch)\n",
    "print(\"edge_index\", batch1.edge_index)\n",
    "print(\"batch\",batch1.edge_attr)\n",
    "print(\"ptr\",batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1167)\n",
      "tensor(1167)\n",
      "tensor([[ 0,  0,  1,  1,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  6,  6,  7,  7,\n",
      "          8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14,\n",
      "         15, 16],\n",
      "        [ 1,  5,  0,  2,  1,  3,  2,  4,  9,  3,  5,  6,  0,  4,  4,  7,  6,  8,\n",
      "          7,  9, 13,  3,  8, 10,  9, 11, 10, 12, 11, 13, 14,  8, 12, 12, 15, 16,\n",
      "         14, 14]])\n",
      "tensor([   0,   17,   30,   43,   62,   73,  101,  117,  137,  149,  166,  183,\n",
      "         203,  225,  238,  257,  279,  290,  307,  320,  338,  356,  373,  396,\n",
      "         423,  440,  453,  476,  493,  516,  539,  561,  585,  608,  621,  638,\n",
      "         652,  669,  684,  699,  712,  729,  742,  761,  778,  790,  813,  835,\n",
      "         852,  872,  888,  914,  940,  959,  978,  992, 1009, 1030, 1055, 1078,\n",
      "        1097, 1114, 1125, 1148, 1168]) ; len: 65\n"
     ]
    }
   ],
   "source": [
    "print(max(batch1.edge_index[0]))\n",
    "print(max(batch1.edge_index[1]))\n",
    "print((dataset[0].edge_index))\n",
    "print((batch1.ptr), '; len:', len(batch1.ptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 0 - 17\n",
      "1. 17 - 30\n",
      "2. 30 - 43\n",
      "3. 43 - 62\n",
      "4. 62 - 73\n",
      "5. 73 - 101\n",
      "6. 101 - 117\n",
      "7. 117 - 137\n",
      "8. 137 - 149\n",
      "9. 149 - 166\n",
      "10. 166 - 183\n",
      "11. 183 - 203\n",
      "12. 203 - 225\n",
      "13. 225 - 238\n",
      "14. 238 - 257\n",
      "15. 257 - 279\n",
      "16. 279 - 290\n",
      "17. 290 - 307\n",
      "18. 307 - 320\n",
      "19. 320 - 338\n",
      "20. 338 - 356\n",
      "21. 356 - 373\n",
      "22. 373 - 396\n",
      "23. 396 - 423\n",
      "24. 423 - 440\n",
      "25. 440 - 453\n",
      "26. 453 - 476\n",
      "27. 476 - 493\n",
      "28. 493 - 516\n",
      "29. 516 - 539\n",
      "30. 539 - 561\n",
      "31. 561 - 585\n",
      "32. 585 - 608\n",
      "33. 608 - 621\n",
      "34. 621 - 638\n",
      "35. 638 - 652\n",
      "36. 652 - 669\n",
      "37. 669 - 684\n",
      "38. 684 - 699\n",
      "39. 699 - 712\n",
      "40. 712 - 729\n",
      "41. 729 - 742\n",
      "42. 742 - 761\n",
      "43. 761 - 778\n",
      "44. 778 - 790\n",
      "45. 790 - 813\n",
      "46. 813 - 835\n",
      "47. 835 - 852\n",
      "48. 852 - 872\n",
      "49. 872 - 888\n",
      "50. 888 - 914\n",
      "51. 914 - 940\n",
      "52. 940 - 959\n",
      "53. 959 - 978\n",
      "54. 978 - 992\n",
      "55. 992 - 1009\n",
      "56. 1009 - 1030\n",
      "57. 1030 - 1055\n",
      "58. 1055 - 1078\n",
      "59. 1078 - 1097\n",
      "60. 1097 - 1114\n",
      "61. 1114 - 1125\n",
      "62. 1125 - 1148\n",
      "63. 1148 - 1168\n"
     ]
    }
   ],
   "source": [
    "graph_bound = {}\n",
    "\n",
    "for i in range(len(batch1.ptr)-1):\n",
    "    graph_bound[i] = [batch1.ptr[i].item(), batch1.ptr[i+1].item()]\n",
    "    print(str(i)+\".\", batch1.ptr[i].item(), \"-\", batch1.ptr[i+1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Graph 0 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "[0 0 0 0 1 0 1 3 3 3 3 2 3 3 2 2 2]\n",
      "{0, 1, 2, 3}\n",
      "communities {0: [0, 1, 2, 3, 5], 1: [4, 6], 3: [7, 8, 9, 10, 12, 13], 2: [11, 14, 15, 16]}\n",
      "=== Graph 1 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[2 1 1 1 0 0 0 1 2 2 2 2 2]\n",
      "{0, 1, 2}\n",
      "communities {2: [0, 8, 9, 10, 11, 12], 1: [1, 2, 3, 7], 0: [4, 5, 6]}\n",
      "=== Graph 2 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[1 1 0 1 1 0 0 1 2 2 2 2 2]\n",
      "{0, 1, 2}\n",
      "communities {1: [0, 1, 3, 4, 7], 0: [2, 5, 6], 2: [8, 9, 10, 11, 12]}\n",
      "=== Graph 3 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[0 0 0 0 1 0 1 1 1 2 2 2 0 3 3 1 3 3 3]\n",
      "{0, 1, 2, 3}\n",
      "communities {0: [0, 1, 2, 3, 5, 12], 1: [4, 6, 7, 8, 15], 2: [9, 10, 11], 3: [13, 14, 16, 17, 18]}\n",
      "=== Graph 4 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[0 1 1 2 2 0 0 2 2 2 1]\n",
      "{0, 1, 2}\n",
      "communities {0: [0, 5, 6], 1: [1, 2, 10], 2: [3, 4, 7, 8, 9]}\n",
      "=== Graph 5 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "[2 0 0 0 1 1 1 2 3 2 2 2 3 3 3 1 3 3 3 2 2 2 1 1 1 0 0 0]\n",
      "{0, 1, 2, 3}\n",
      "communities {2: [0, 7, 9, 10, 11, 19, 20, 21], 0: [1, 2, 3, 25, 26, 27], 1: [4, 5, 6, 15, 22, 23, 24], 3: [8, 12, 13, 14, 16, 17, 18]}\n",
      "=== Graph 6 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[1 1 1 1 1 0 0 1 0 0 2 2 2 2 2 2]\n",
      "{0, 1, 2}\n",
      "communities {1: [0, 1, 2, 3, 4, 7], 0: [5, 6, 8, 9], 2: [10, 11, 12, 13, 14, 15]}\n",
      "=== Graph 7 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[0 0 0 0 1 1 1 1 1 1 1 3 3 3 2 3 3 2 2 2]\n",
      "{0, 1, 2, 3}\n",
      "communities {0: [0, 1, 2, 3], 1: [4, 5, 6, 7, 8, 9, 10], 3: [11, 12, 13, 15, 16], 2: [14, 17, 18, 19]}\n",
      "=== Graph 8 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "[0 1 1 1 1 2 2 2 0 0 0 0]\n",
      "{0, 1, 2}\n",
      "communities {0: [0, 8, 9, 10, 11], 1: [1, 2, 3, 4], 2: [5, 6, 7]}\n",
      "=== Graph 9 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "[0 0 0 0 0 0 0 2 2 2 2 1 2 2 1 1 1]\n",
      "{0, 1, 2}\n",
      "communities {0: [0, 1, 2, 3, 4, 5, 6], 2: [7, 8, 9, 10, 12, 13], 1: [11, 14, 15, 16]}\n",
      "=== Graph 10 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "masalah disini bro\n"
     ]
    }
   ],
   "source": [
    "idx_from = 0\n",
    "idx_to = 0\n",
    "graph_counter = 0\n",
    "graph_bound\n",
    "edge_index = [[],[]]\n",
    "Gs = []\n",
    "\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "print(f'=== Graph {graph_counter} ===')\n",
    "for i, (src, dst) in enumerate(zip(batch1.edge_index[0], batch1.edge_index[1])):\n",
    "    # if (graph_counter < len(batch1.ptr)):\n",
    "    lower_bound = graph_bound[graph_counter][0]\n",
    "    upper_bound = graph_bound[graph_counter][1]\n",
    "    if ((src >= lower_bound and src < upper_bound) or\n",
    "        (dst >= lower_bound and dst < upper_bound)):\n",
    "        # print(i,src.item()-lower_bound, dst.item()-lower_bound)\n",
    "        edge_index[0].append(src - lower_bound)\n",
    "        edge_index[1].append(dst - lower_bound)\n",
    "    else:\n",
    "        # print(edge_index)\n",
    "        embs = []\n",
    "        # make new graph\n",
    "        for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "            if (b == graph_counter):\n",
    "                # print(i, emb)\n",
    "                embs.append(emb)\n",
    "        \n",
    "        G = data_transformation(edge_index, embs)\n",
    "        Gs.append(G)\n",
    "        print(sorted(list(G.nodes)))\n",
    "        # print('pre', precalc_shortest_path_length)\n",
    "        # for node in sorted(list(G.nodes)):\n",
    "        #     print(G.nodes[node])\n",
    "        \n",
    "        \n",
    "        # testt()\n",
    "        if graph_counter == 10:\n",
    "            print('masalah disini bro')\n",
    "            break\n",
    "        \n",
    "        # Calculate S matrix\n",
    "        S = calculate_similarity_matrix(G)\n",
    "        \n",
    "        # AP Clustering\n",
    "        clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "\n",
    "        print(clustering.labels_)\n",
    "        # print(clustering.)\n",
    "        \n",
    "        communities = {}\n",
    "        print(set(clustering.labels_))\n",
    "        # communities init\n",
    "        for lab in clustering.labels_:\n",
    "            communities[lab] = []\n",
    "        \n",
    "        for nd, clust in enumerate(clustering.labels_):\n",
    "            communities[clust].append(nd)\n",
    "        print(\"communities\", communities) \n",
    "            \n",
    "        edge_index = [[],[]]\n",
    "        graph_counter+=1\n",
    "        \n",
    "        print(f'=== Graph {graph_counter} ===')\n",
    "        \n",
    "    if i == len(batch1.edge_index[0]) - 1:\n",
    "        embs = []\n",
    "        # make new graph\n",
    "        for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "            if (b == graph_counter):\n",
    "                # print(i, emb)\n",
    "                embs.append(emb)\n",
    "        \n",
    "        G = data_transformation(edge_index, embs)\n",
    "        Gs.append(G)\n",
    "        \n",
    "        S = calculate_similarity_matrix(G)\n",
    "        # AP Clustering        \n",
    "        clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "        \n",
    "        print(sorted(list(G.nodes)))\n",
    "        print(clustering.labels_)\n",
    "        \n",
    "        # print(edge_index)\n",
    "        print('udh di akhir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3].x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([0., 0., 1., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([0., 1., 0., 0., 0., 0., 0.], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "for node in Gs[6].nodes:\n",
    "    print(Gs[0].nodes[node])\n",
    "    # Udah bisa tambah nodes per batch, tinggal masukin ke algo utama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  5,  5,  5,  6,  6,  6,\n",
       "          7,  8,  9,  9,  9, 10, 11, 11, 11, 12, 12, 12, 13, 13, 14, 14, 15, 15,\n",
       "         15, 16, 16, 17, 17, 17, 18, 19],\n",
       "        [ 1,  5,  0,  2,  1,  3, 12,  2,  4,  9,  3,  5,  0,  4,  6,  5,  7,  8,\n",
       "          6,  6,  3, 10, 11,  9,  9, 12, 16,  2, 11, 13, 12, 14, 13, 15, 14, 16,\n",
       "         17, 11, 15, 15, 18, 19, 17, 17]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[63].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "1 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "2 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "3 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "4 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "5 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "6 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "7 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "8 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "9 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "10 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "11 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "12 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "13 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "14 tensor([0., 1., 0., 0., 0., 0., 0.])\n",
      "15 tensor([0., 0., 1., 0., 0., 0., 0.])\n",
      "16 tensor([0., 0., 1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "    if (b == 0):\n",
    "        print(i, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
