{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from preprocessing import data_transformation\n",
    "from similarity import calculate_similarity_matrix\n",
    "\n",
    "from model import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root=\"/\", name=\"MUTAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split: Train test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```train_dataset```: for training model<br/>\n",
    "```val_dataset```: evaluate model for hyperparameter tunning<br/>\n",
    "```test_dataset```: testing model after complete training<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ts, vl = 0.8, 0.1, 0.1\n",
    "dslen = len(dataset)\n",
    "tri = round(tr*dslen)\n",
    "tsi = round((tr+ts)*dslen)\n",
    "train_dataset = dataset[:tri]\n",
    "test_dataset = dataset[tri:tsi]\n",
    "val_dataset = dataset[tsi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)\n",
    "test_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)\n",
    "val_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0])\n",
      "val loader\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0])\n",
      "test loader\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print('train loader')\n",
    "for data in train_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('val loader')\n",
    "for data in val_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('test loader')\n",
    "for data in test_loader:\n",
    "    print(data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv3(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base(\n",
       "  (conv1): GCNConv(7, 64)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (conv3): GCNConv(64, 64)\n",
       "  (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = Base(dataset, 64)\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1258, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_base(model, loader, experiment_mode=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(h, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss\n",
    "    #     print(h[0])\n",
    "    # print(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_base(model, loader, experiment_mode=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        pred = h.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct/len(loader.dataset)\n",
    "\n",
    "base = Base(dataset, 64)\n",
    "train_base(base, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; loss: 0.83; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 1; loss: 1.33; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 2; loss: 1.77; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 3; loss: 0.97; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 4; loss: 1.45; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 5; loss: 1.67; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 6; loss: 0.76; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 7; loss: 0.76; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 8; loss: 0.79; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 9; loss: 0.81; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 10; loss: 0.91; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 11; loss: 1.03; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 12; loss: 1.21; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 13; loss: 1.1; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 14; loss: 1.38; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 15; loss: 1.09; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 16; loss: 1.54; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 17; loss: 2.34; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 18; loss: 0.72; train_acc: 0.34; test_acc: 0.37\n",
      "epoch 19; loss: 0.72; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 20; loss: 0.75; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 21; loss: 0.78; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 22; loss: 0.85; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 23; loss: 1.05; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 24; loss: 1.21; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 25; loss: 1.64; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 26; loss: 0.77; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 27; loss: 0.78; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 28; loss: 0.9; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 29; loss: 1.22; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 30; loss: 1.78; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 31; loss: 0.77; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 32; loss: 0.78; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 33; loss: 0.88; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 34; loss: 1.78; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 35; loss: 2.83; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 36; loss: 0.7; train_acc: 0.34; test_acc: 0.37\n",
      "epoch 37; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 38; loss: 1.73; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 39; loss: 0.71; train_acc: 0.35; test_acc: 0.37\n",
      "epoch 40; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 41; loss: 0.69; train_acc: 0.35; test_acc: 0.37\n",
      "epoch 42; loss: 0.67; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 43; loss: 0.67; train_acc: 0.35; test_acc: 0.37\n",
      "epoch 44; loss: 0.62; train_acc: 0.67; test_acc: 0.63\n",
      "epoch 45; loss: 0.65; train_acc: 0.35; test_acc: 0.37\n",
      "epoch 46; loss: 0.63; train_acc: 0.45; test_acc: 0.37\n",
      "epoch 47; loss: 1.39; train_acc: 0.75; test_acc: 0.74\n",
      "epoch 48; loss: 0.66; train_acc: 0.35; test_acc: 0.37\n",
      "epoch 49; loss: 0.66; train_acc: 0.75; test_acc: 0.74\n",
      "Accuracy: 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "epoch = 50\n",
    "\n",
    "base = Base(dataset, 128)\n",
    "train_base(base, train_loader)\n",
    "\n",
    "# Train\n",
    "for _ in range(epoch):\n",
    "    loss = round(train_base(base, train_loader).item(), 2)\n",
    "    train_acc = round(test_base(base, train_loader), 2)\n",
    "    val_acc = round(test_base(base, val_loader), 2)\n",
    "    \n",
    "    print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; test_acc: {val_acc}')\n",
    "\n",
    "# Test\n",
    "test = test_base(base, test_loader)\n",
    "print(f'Accuracy: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        G = data_transformation(edge_index, feature_emb)\n",
    "        S = calculate_similarity_matrix(G)\n",
    "        # clustering = AffinityPropagation(affinity='precomputed', random_state=123, max_iter=200).fit(S)\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h\n",
    "\n",
    "    def data_transformation():\n",
    "        print('s')\n",
    "        \n",
    "\n",
    "\n",
    "experiment = Experiment(dataset, 64)\n",
    "# train_base(experiment, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 5210], x=[2347, 7], edge_attr=[5210, 4], y=[128], batch=[2347], ptr=[129])\n",
      "tensor([  0,   0,   0,  ..., 127, 127, 127])\n",
      "edge_index tensor([[   0,    0,    1,  ..., 2345, 2346, 2346],\n",
      "        [   1,    5,    0,  ..., 2346, 2334, 2345]])\n",
      "batch tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "ptr tensor([   0,   17,   30,   43,   62,   73,  101,  117,  137,  149,  166,  183,\n",
      "         203,  225,  238,  257,  279,  290,  307,  320,  338,  356,  373,  396,\n",
      "         423,  440,  453,  476,  493,  516,  539,  561,  585,  608,  621,  638,\n",
      "         652,  669,  684,  699,  712,  729,  742,  761,  778,  790,  813,  835,\n",
      "         852,  872,  888,  914,  940,  959,  978,  992, 1009, 1030, 1055, 1078,\n",
      "        1097, 1114, 1125, 1148, 1168, 1184, 1200, 1220, 1243, 1262, 1276, 1302,\n",
      "        1318, 1334, 1357, 1375, 1385, 1401, 1417, 1434, 1453, 1465, 1490, 1506,\n",
      "        1517, 1540, 1563, 1579, 1591, 1604, 1627, 1652, 1671, 1694, 1713, 1732,\n",
      "        1757, 1775, 1788, 1803, 1819, 1842, 1868, 1887, 1910, 1927, 1947, 1972,\n",
      "        1991, 2019, 2043, 2054, 2069, 2082, 2098, 2110, 2120, 2141, 2164, 2185,\n",
      "        2198, 2223, 2244, 2261, 2272, 2291, 2311, 2332, 2347])\n"
     ]
    }
   ],
   "source": [
    "batch1 = None\n",
    "for batch in train_loader:\n",
    "    batch1 = batch\n",
    "    break\n",
    "print(batch1)\n",
    "print(batch1.batch)\n",
    "print(\"edge_index\", batch1.edge_index)\n",
    "print(\"batch\",batch1.edge_attr)\n",
    "print(\"ptr\",batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2346)\n",
      "tensor(2346)\n",
      "tensor([[ 0,  0,  1,  1,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  6,  6,  7,  7,\n",
      "          8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14,\n",
      "         15, 16],\n",
      "        [ 1,  5,  0,  2,  1,  3,  2,  4,  9,  3,  5,  6,  0,  4,  4,  7,  6,  8,\n",
      "          7,  9, 13,  3,  8, 10,  9, 11, 10, 12, 11, 13, 14,  8, 12, 12, 15, 16,\n",
      "         14, 14]])\n",
      "tensor([   0,   17,   30,   43,   62,   73,  101,  117,  137,  149,  166,  183,\n",
      "         203,  225,  238,  257,  279,  290,  307,  320,  338,  356,  373,  396,\n",
      "         423,  440,  453,  476,  493,  516,  539,  561,  585,  608,  621,  638,\n",
      "         652,  669,  684,  699,  712,  729,  742,  761,  778,  790,  813,  835,\n",
      "         852,  872,  888,  914,  940,  959,  978,  992, 1009, 1030, 1055, 1078,\n",
      "        1097, 1114, 1125, 1148, 1168, 1184, 1200, 1220, 1243, 1262, 1276, 1302,\n",
      "        1318, 1334, 1357, 1375, 1385, 1401, 1417, 1434, 1453, 1465, 1490, 1506,\n",
      "        1517, 1540, 1563, 1579, 1591, 1604, 1627, 1652, 1671, 1694, 1713, 1732,\n",
      "        1757, 1775, 1788, 1803, 1819, 1842, 1868, 1887, 1910, 1927, 1947, 1972,\n",
      "        1991, 2019, 2043, 2054, 2069, 2082, 2098, 2110, 2120, 2141, 2164, 2185,\n",
      "        2198, 2223, 2244, 2261, 2272, 2291, 2311, 2332, 2347]) ; len: 129\n"
     ]
    }
   ],
   "source": [
    "print(max(batch1.edge_index[0]))\n",
    "print(max(batch1.edge_index[1]))\n",
    "print((dataset[0].edge_index))\n",
    "print((batch1.ptr), '; len:', len(batch1.ptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 0 - 17\n",
      "1. 17 - 30\n",
      "2. 30 - 43\n",
      "3. 43 - 62\n",
      "4. 62 - 73\n",
      "5. 73 - 101\n",
      "6. 101 - 117\n",
      "7. 117 - 137\n",
      "8. 137 - 149\n",
      "9. 149 - 166\n",
      "10. 166 - 183\n",
      "11. 183 - 203\n",
      "12. 203 - 225\n",
      "13. 225 - 238\n",
      "14. 238 - 257\n",
      "15. 257 - 279\n",
      "16. 279 - 290\n",
      "17. 290 - 307\n",
      "18. 307 - 320\n",
      "19. 320 - 338\n",
      "20. 338 - 356\n",
      "21. 356 - 373\n",
      "22. 373 - 396\n",
      "23. 396 - 423\n",
      "24. 423 - 440\n",
      "25. 440 - 453\n",
      "26. 453 - 476\n",
      "27. 476 - 493\n",
      "28. 493 - 516\n",
      "29. 516 - 539\n",
      "30. 539 - 561\n",
      "31. 561 - 585\n",
      "32. 585 - 608\n",
      "33. 608 - 621\n",
      "34. 621 - 638\n",
      "35. 638 - 652\n",
      "36. 652 - 669\n",
      "37. 669 - 684\n",
      "38. 684 - 699\n",
      "39. 699 - 712\n",
      "40. 712 - 729\n",
      "41. 729 - 742\n",
      "42. 742 - 761\n",
      "43. 761 - 778\n",
      "44. 778 - 790\n",
      "45. 790 - 813\n",
      "46. 813 - 835\n",
      "47. 835 - 852\n",
      "48. 852 - 872\n",
      "49. 872 - 888\n",
      "50. 888 - 914\n",
      "51. 914 - 940\n",
      "52. 940 - 959\n",
      "53. 959 - 978\n",
      "54. 978 - 992\n",
      "55. 992 - 1009\n",
      "56. 1009 - 1030\n",
      "57. 1030 - 1055\n",
      "58. 1055 - 1078\n",
      "59. 1078 - 1097\n",
      "60. 1097 - 1114\n",
      "61. 1114 - 1125\n",
      "62. 1125 - 1148\n",
      "63. 1148 - 1168\n",
      "64. 1168 - 1184\n",
      "65. 1184 - 1200\n",
      "66. 1200 - 1220\n",
      "67. 1220 - 1243\n",
      "68. 1243 - 1262\n",
      "69. 1262 - 1276\n",
      "70. 1276 - 1302\n",
      "71. 1302 - 1318\n",
      "72. 1318 - 1334\n",
      "73. 1334 - 1357\n",
      "74. 1357 - 1375\n",
      "75. 1375 - 1385\n",
      "76. 1385 - 1401\n",
      "77. 1401 - 1417\n",
      "78. 1417 - 1434\n",
      "79. 1434 - 1453\n",
      "80. 1453 - 1465\n",
      "81. 1465 - 1490\n",
      "82. 1490 - 1506\n",
      "83. 1506 - 1517\n",
      "84. 1517 - 1540\n",
      "85. 1540 - 1563\n",
      "86. 1563 - 1579\n",
      "87. 1579 - 1591\n",
      "88. 1591 - 1604\n",
      "89. 1604 - 1627\n",
      "90. 1627 - 1652\n",
      "91. 1652 - 1671\n",
      "92. 1671 - 1694\n",
      "93. 1694 - 1713\n",
      "94. 1713 - 1732\n",
      "95. 1732 - 1757\n",
      "96. 1757 - 1775\n",
      "97. 1775 - 1788\n",
      "98. 1788 - 1803\n",
      "99. 1803 - 1819\n",
      "100. 1819 - 1842\n",
      "101. 1842 - 1868\n",
      "102. 1868 - 1887\n",
      "103. 1887 - 1910\n",
      "104. 1910 - 1927\n",
      "105. 1927 - 1947\n",
      "106. 1947 - 1972\n",
      "107. 1972 - 1991\n",
      "108. 1991 - 2019\n",
      "109. 2019 - 2043\n",
      "110. 2043 - 2054\n",
      "111. 2054 - 2069\n",
      "112. 2069 - 2082\n",
      "113. 2082 - 2098\n",
      "114. 2098 - 2110\n",
      "115. 2110 - 2120\n",
      "116. 2120 - 2141\n",
      "117. 2141 - 2164\n",
      "118. 2164 - 2185\n",
      "119. 2185 - 2198\n",
      "120. 2198 - 2223\n",
      "121. 2223 - 2244\n",
      "122. 2244 - 2261\n",
      "123. 2261 - 2272\n",
      "124. 2272 - 2291\n",
      "125. 2291 - 2311\n",
      "126. 2311 - 2332\n",
      "127. 2332 - 2347\n"
     ]
    }
   ],
   "source": [
    "graph_bound = {}\n",
    "\n",
    "for i in range(len(batch1.ptr)-1):\n",
    "    graph_bound[i] = [batch1.ptr[i].item(), batch1.ptr[i+1].item()]\n",
    "    print(str(i)+\".\", batch1.ptr[i].item(), \"-\", batch1.ptr[i+1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below --> Subgraph extractor with batch information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Graph 0 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "[0 0 0 0 0 0 2 2 2 2 2 1 2 2 1 1 1]\n",
      "{0, 1, 2}\n",
      "communities {0: [0, 1, 2, 3, 4, 5], 2: [6, 7, 8, 9, 10, 12, 13], 1: [11, 14, 15, 16]}\n",
      "embs [tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# idx_from = 0\n",
    "# idx_to = 0\n",
    "graph_counter = 0\n",
    "graph_bound\n",
    "edge_index = [[],[]]\n",
    "Gs = []\n",
    "\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "print(f'=== Graph {graph_counter} ===')\n",
    "for i, (src, dst) in enumerate(zip(batch1.edge_index[0], batch1.edge_index[1])):\n",
    "    # if (graph_counter < len(batch1.ptr)):\n",
    "    lower_bound = graph_bound[graph_counter][0]\n",
    "    upper_bound = graph_bound[graph_counter][1]\n",
    "    if ((src >= lower_bound and src < upper_bound) or\n",
    "        (dst >= lower_bound and dst < upper_bound)):\n",
    "        # print(i,src.item()-lower_bound, dst.item()-lower_bound)\n",
    "        edge_index[0].append(src - lower_bound)\n",
    "        edge_index[1].append(dst - lower_bound)\n",
    "    else:\n",
    "        # print(edge_index)\n",
    "        embs = []\n",
    "        # make new graph\n",
    "        for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "            if (b == graph_counter):\n",
    "                # print(i, emb)\n",
    "                embs.append(emb)\n",
    "        \n",
    "        G = data_transformation(edge_index, embs)\n",
    "        Gs.append(G)\n",
    "        print(sorted(list(G.nodes)))\n",
    "        # print('pre', precalc_shortest_path_length)\n",
    "        # for node in sorted(list(G.nodes)):\n",
    "        #     print(G.nodes[node])\n",
    "        \n",
    "        \n",
    "        # testt()\n",
    "        if graph_counter == 10:\n",
    "            print('masalah disini bro')\n",
    "            break\n",
    "        \n",
    "        # Calculate S matrix\n",
    "        S = calculate_similarity_matrix(G)\n",
    "        \n",
    "        # AP Clustering\n",
    "        clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "\n",
    "        print(clustering.labels_)\n",
    "        # print(clustering.)\n",
    "        \n",
    "        communities = {}\n",
    "        print(set(clustering.labels_))\n",
    "        # communities init\n",
    "        for lab in clustering.labels_:\n",
    "            communities[lab] = []\n",
    "        \n",
    "        for nd, clust in enumerate(clustering.labels_):\n",
    "            communities[clust].append(nd)\n",
    "        print(\"communities\", communities) \n",
    "            \n",
    "        edge_index = [[],[]]\n",
    "        graph_counter+=1\n",
    "        print(\"embs\",embs)\n",
    "        # print(S)\n",
    "        break\n",
    "        print(f'=== Graph {graph_counter} ===')\n",
    "        \n",
    "    if i == len(batch1.edge_index[0]) - 1:\n",
    "        embs = []\n",
    "        # make new graph\n",
    "        for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "            if (b == graph_counter):\n",
    "                # print(i, emb)\n",
    "                embs.append(emb)\n",
    "        \n",
    "        G = data_transformation(edge_index, embs)\n",
    "        Gs.append(G)\n",
    "        \n",
    "        S = calculate_similarity_matrix(G)\n",
    "        # AP Clustering        \n",
    "        clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "        \n",
    "        print(sorted(list(G.nodes)))\n",
    "        print(clustering.labels_)\n",
    "        \n",
    "        # print(edge_index)\n",
    "        print('udh di akhir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering = AffinityPropagation(affinity='precomputed', damping=0.7, random_state=42, convergence_iter=15, max_iter=1000)\n",
    "clustering.fit(S)\n",
    "clustering.labels_\n",
    "# clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [\n",
    "[\n",
    "0.8,\n",
    "994.158268226589,\n",
    "7.9207682265892885,\n",
    "101.35826822658926,\n",
    "561.9082682265891,\n",
    "1.12076822658929,\n",
    "10.80680355293715,\n",
    "8.106803552937151,\n",
    "73933.09430355292,\n",
    "720.1082682265892,\n",
    "35.22076822658928,\n",
    "177.12076822658926,\n",
    "572.5582682265891,\n",
    "88.30826822658926,\n",
    "423.30826822658923,\n",
    "16.308268226589284,\n",
    "],\n",
    "[\n",
    "994.158268226589,\n",
    "0.8,\n",
    "59.80680355293714,\n",
    "159.72076822658926,\n",
    "96.90826822658927,\n",
    "110.55826822658926,\n",
    "120.15826822658926,\n",
    "13.720768226589287,\n",
    "68123.92242855292,\n",
    "556.6207682265892,\n",
    "177.12076822658926,\n",
    "567.406803552937,\n",
    "8.558268226589288,\n",
    "414.1582682265892,\n",
    "744.3082682265891,\n",
    "101.54430355293712,\n",
    "],\n",
    "[\n",
    "7.9207682265892885,\n",
    "59.80680355293714,\n",
    "0.8,\n",
    "1575.4207682265887,\n",
    "11.358268226589288,\n",
    "13.720768226589287,\n",
    "36.55826822658928,\n",
    "409.62076822658923,\n",
    "93229.80680355291,\n",
    "561.9082682265891,\n",
    "561.9082682265891,\n",
    "99.30680355293713,\n",
    "48.15826822658928,\n",
    "423.30826822658923,\n",
    "296.55826822658923,\n",
    "525.6068035529371,\n",
    "],\n",
    "[\n",
    "101.35826822658926,\n",
    "159.72076822658926,\n",
    "1575.4207682265887,\n",
    "0.8,\n",
    "1411.3082682265888,\n",
    "541.094303552937,\n",
    "400.62076822658923,\n",
    "270.22076822658926,\n",
    "69738.34430355292,\n",
    "13.90680355293715,\n",
    "1411.3082682265888,\n",
    "2553.9082682265885,\n",
    "9.908268226589287,\n",
    "702.406803552937,\n",
    "285.12076822658923,\n",
    "165.42076822658927,\n",
    "],\n",
    "[\n",
    "561.9082682265891,\n",
    "96.90826822658927,\n",
    "11.358268226589288,\n",
    "1411.3082682265888,\n",
    "0.8,\n",
    "180.10826822658927,\n",
    "56.22076822658928,\n",
    "0.72076822658929,\n",
    "81409.09430355292,\n",
    "556.6207682265892,\n",
    "0.49430355293715383,\n",
    "51.30826822658928,\n",
    "42.34430355293714,\n",
    "1377.908268226589,\n",
    "7.4943035529371524,\n",
    "891.2207682265891,\n",
    "],\n",
    "[\n",
    "1.12076822658929,\n",
    "110.55826822658926,\n",
    "13.720768226589287,\n",
    "541.094303552937,\n",
    "180.10826822658927,\n",
    "0.8,\n",
    "54.55826822658928,\n",
    "738.2207682265891,\n",
    "77937.90680355292,\n",
    "49.90680355293714,\n",
    "96.90826822658927,\n",
    "409.62076822658923,\n",
    "304.30826822658923,\n",
    "39.49430355293714,\n",
    "195.42076822658925,\n",
    "0.15826822658929016,\n",
    "],\n",
    "[\n",
    "10.80680355293715,\n",
    "120.15826822658926,\n",
    "36.55826822658928,\n",
    "400.62076822658923,\n",
    "56.22076822658928,\n",
    "54.55826822658928,\n",
    "0.8,\n",
    "97.09430355293713,\n",
    "68416.04742855292,\n",
    "316.12076822658923,\n",
    "405.1082682265892,\n",
    "262.92076822658925,\n",
    "13.720768226589287,\n",
    "6.720768226589289,\n",
    "51.30826822658928,\n",
    "94.72076822658927,\n",
    "],\n",
    "[\n",
    "8.106803552937151,\n",
    "13.720768226589287,\n",
    "409.62076822658923,\n",
    "270.22076822658926,\n",
    "0.72076822658929,\n",
    "738.2207682265891,\n",
    "97.09430355293713,\n",
    "0.8,\n",
    "77128.49430355291,\n",
    "40.72076822658928,\n",
    "195.42076822658925,\n",
    "427.9207682265892,\n",
    "2184.158268226589,\n",
    "690.4207682265892,\n",
    "37.92076822658928,\n",
    "12.908268226589287,\n",
    "],\n",
    "[\n",
    "73933.09430355292,\n",
    "68123.92242855292,\n",
    "93229.80680355291,\n",
    "69738.34430355292,\n",
    "81409.09430355292,\n",
    "77937.90680355292,\n",
    "68416.04742855292,\n",
    "77128.49430355291,\n",
    "0.8,\n",
    "77626.09430355292,\n",
    "81409.09430355292,\n",
    "77377.09430355292,\n",
    "67832.42242855292,\n",
    "69915.60680355292,\n",
    "72390.82242855291,\n",
    "70033.90680355292,\n",
    "],\n",
    "[\n",
    "720.1082682265892,\n",
    "556.6207682265892,\n",
    "561.9082682265891,\n",
    "13.90680355293715,\n",
    "556.6207682265892,\n",
    "49.90680355293714,\n",
    "316.12076822658923,\n",
    "40.72076822658928,\n",
    "77626.09430355292,\n",
    "0.8,\n",
    "583.3082682265891,\n",
    "99.12076822658926,\n",
    "42.15826822658928,\n",
    "103.80680355293713,\n",
    "540.9082682265891,\n",
    "234.72076822658926,\n",
    "],\n",
    "[\n",
    "35.22076822658928,\n",
    "177.12076822658926,\n",
    "561.9082682265891,\n",
    "1411.3082682265888,\n",
    "0.49430355293715383,\n",
    "96.90826822658927,\n",
    "405.1082682265892,\n",
    "195.42076822658925,\n",
    "81409.09430355292,\n",
    "583.3082682265891,\n",
    "0.8,\n",
    "208.12076822658926,\n",
    "270.4068035529371,\n",
    "177.12076822658926,\n",
    "97.09430355293713,\n",
    "159.72076822658926,\n",
    "],\n",
    "[\n",
    "177.12076822658926,\n",
    "567.406803552937,\n",
    "99.30680355293713,\n",
    "2553.9082682265885,\n",
    "51.30826822658928,\n",
    "409.62076822658923,\n",
    "262.92076822658925,\n",
    "427.9207682265892,\n",
    "77377.09430355292,\n",
    "99.12076822658926,\n",
    "208.12076822658926,\n",
    "0.8,\n",
    "374.2207682265892,\n",
    "270.22076822658926,\n",
    "94.72076822658927,\n",
    "904.806803552937,\n",
    "],\n",
    "[\n",
    "572.5582682265891,\n",
    "8.558268226589288,\n",
    "48.15826822658928,\n",
    "9.908268226589287,\n",
    "42.34430355293714,\n",
    "304.30826822658923,\n",
    "13.720768226589287,\n",
    "2184.158268226589,\n",
    "67832.42242855292,\n",
    "42.15826822658928,\n",
    "270.4068035529371,\n",
    "374.2207682265892,\n",
    "0.8,\n",
    "0.3082682265892901,\n",
    "1702.3068035529368,\n",
    "37.92076822658928,\n",
    "],\n",
    "[\n",
    "88.30826822658926,\n",
    "414.1582682265892,\n",
    "423.30826822658923,\n",
    "702.406803552937,\n",
    "1377.908268226589,\n",
    "39.49430355293714,\n",
    "6.720768226589289,\n",
    "690.4207682265892,\n",
    "69915.60680355292,\n",
    "103.80680355293713,\n",
    "177.12076822658926,\n",
    "270.22076822658926,\n",
    "0.3082682265892901,\n",
    "0.8,\n",
    "10.620768226589288,\n",
    "1702.1207682265888,\n",
    "],\n",
    "[\n",
    "423.30826822658923,\n",
    "744.3082682265891,\n",
    "296.55826822658923,\n",
    "285.12076822658923,\n",
    "7.4943035529371524,\n",
    "195.42076822658925,\n",
    "51.30826822658928,\n",
    "37.92076822658928,\n",
    "72390.82242855291,\n",
    "540.9082682265891,\n",
    "97.09430355293713,\n",
    "94.72076822658927,\n",
    "1702.3068035529368,\n",
    "10.620768226589288,\n",
    "0.8,\n",
    "9.908268226589287,\n",
    "],\n",
    "[\n",
    "16.308268226589284,\n",
    "101.54430355293712,\n",
    "525.6068035529371,\n",
    "165.42076822658927,\n",
    "891.2207682265891,\n",
    "0.15826822658929016,\n",
    "94.72076822658927,\n",
    "12.908268226589287,\n",
    "70033.90680355292,\n",
    "234.72076822658926,\n",
    "159.72076822658926,\n",
    "904.806803552937,\n",
    "37.92076822658928,\n",
    "1702.1207682265888,\n",
    "9.908268226589287,\n",
    "0.8,\n",
    "],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.00000000e-01 1.04474286e+01 8.55826823e+00 1.03399180e+04\n",
      "  1.09045583e+04 6.30297429e+02 1.18867955e+02 2.08027153e+02\n",
      "  4.65178904e+01 4.56027153e+02 1.16335154e+01 1.20144830e+01\n",
      "  1.16288545e+01 1.20144830e+01 2.68370102e-04 9.87278433e-05\n",
      "  9.87278433e-05]\n",
      " [1.04474286e+01 8.00000000e-01 9.83797429e+02 8.88322077e+03\n",
      "  5.33831795e+03 4.23308268e+02 1.27152511e-01 5.28178904e+01\n",
      "  1.12827153e+02 1.22465233e+03 1.98464653e+02 5.39035760e-03\n",
      "  1.20144830e+01 1.78903576e-02 1.16288545e+01 2.68370102e-04\n",
      "  2.68370102e-04]\n",
      " [8.55826823e+00 9.83797429e+02 8.00000000e-01 1.56662974e+04\n",
      "  5.46162077e+03 6.64167955e+02 4.46552330e+02 1.46525111e-02\n",
      "  1.16679547e+01 7.87620768e+02 4.89829655e-01 1.98464653e+02\n",
      "  2.98383515e+02 1.12827153e+02 1.20144830e+01 7.29505572e-04\n",
      "  4.65132295e+01]\n",
      " [1.03399180e+04 8.88322077e+03 1.56662974e+04 8.00000000e-01\n",
      "  4.23434430e+03 1.06031208e+04 1.88190827e+03 3.46567955e+02\n",
      "  1.04412077e+03 9.16724286e+01 1.37790827e+03 8.98296547e-02\n",
      "  2.11152715e+03 1.49179547e+01 1.73911789e+03 1.08114483e+02\n",
      "  4.80519830e+01]\n",
      " [1.09045583e+04 5.33831795e+03 5.46162077e+03 4.23434430e+03\n",
      "  8.00000000e-01 1.10335474e+04 4.75890680e+03 1.33672077e+03\n",
      "  6.84898297e+01 4.21582682e+01 6.13317955e+02 5.99527153e+02\n",
      "  9.69533515e+02 1.86064653e+02 1.92201983e+02 7.29505572e-04\n",
      "  1.04653855e+02]\n",
      " [6.30297429e+02 4.23308268e+02 6.64167955e+02 1.06031208e+04\n",
      "  1.10335474e+04 8.00000000e-01 8.00220768e+02 1.26408983e+03\n",
      "  4.46527153e+02 9.57429547e+01 1.20271525e+01 1.24085154e+01\n",
      "  1.20144830e+01 5.53903576e-02 4.65132295e+01 2.68370102e-04\n",
      "  2.68370102e-04]\n",
      " [1.18867955e+02 1.27152511e-01 4.46552330e+02 1.88190827e+03\n",
      "  4.75890680e+03 8.00220768e+02 8.00000000e-01 5.47443036e+01\n",
      "  5.20042077e+03 1.77665233e+03 1.12827153e+02 5.85908515e+02\n",
      "  2.43102715e+03 4.06523297e+01 2.37620539e+03 1.20144830e+01\n",
      "  1.08114483e+02]\n",
      " [2.08027153e+02 5.28178904e+01 1.46525111e-02 3.46567955e+02\n",
      "  1.33672077e+03 1.26408983e+03 5.47443036e+01 8.00000000e-01\n",
      "  7.78180680e+03 1.57542077e+03 1.49216795e+03 7.81264653e+02\n",
      "  6.65216795e+03 1.00122077e+03 1.76721465e+03 1.89117890e+02\n",
      "  9.76508515e+02]\n",
      " [4.65178904e+01 1.12827153e+02 1.16679547e+01 1.04412077e+03\n",
      "  6.84898297e+01 4.46527153e+02 5.20042077e+03 7.78180680e+03\n",
      "  8.00000000e-01 4.23443036e+01 4.95120768e+02 1.95352330e+02\n",
      "  1.61029208e+04 5.47428553e-01 1.38996680e+04 1.48782715e+03\n",
      "  1.49646465e+03]\n",
      " [4.56027153e+02 1.22465233e+03 7.87620768e+02 9.16724286e+01\n",
      "  4.21582682e+01 9.57429547e+01 1.77665233e+03 1.57542077e+03\n",
      "  4.23443036e+01 8.00000000e-01 7.49842243e+03 1.12920768e+02\n",
      "  1.29286523e+04 6.90420768e+02 7.16312715e+03 7.62617890e+02\n",
      "  2.34361789e+03]\n",
      " [1.16335154e+01 1.98464653e+02 4.89829655e-01 1.37790827e+03\n",
      "  6.13317955e+02 1.20271525e+01 1.12827153e+02 1.49216795e+03\n",
      "  4.95120768e+02 7.49842243e+03 8.00000000e-01 3.35434430e+03\n",
      "  3.28861583e+04 5.12398297e+01 3.06740680e+04 4.03281465e+03\n",
      "  3.55112715e+03]\n",
      " [1.20144830e+01 5.39035760e-03 1.98464653e+02 8.98296547e-02\n",
      "  5.99527153e+02 1.24085154e+01 5.85908515e+02 7.81264653e+02\n",
      "  1.95352330e+02 1.12920768e+02 3.35434430e+03 8.00000000e-01\n",
      "  8.26900943e+04 5.13082682e+01 6.12725583e+04 1.88651930e+04\n",
      "  3.00506680e+04]\n",
      " [1.16288545e+01 1.20144830e+01 2.98383515e+02 2.11152715e+03\n",
      "  9.69533515e+02 1.20144830e+01 2.43102715e+03 6.65216795e+03\n",
      "  1.61029208e+04 1.29286523e+04 3.28861583e+04 8.26900943e+04\n",
      "  8.00000000e-01 6.34504068e+04 4.34417243e+03 8.64720768e+02\n",
      "  1.45908268e+02]\n",
      " [1.20144830e+01 1.78903576e-02 1.12827153e+02 1.49179547e+01\n",
      "  1.86064653e+02 5.53903576e-02 4.06523297e+01 1.00122077e+03\n",
      "  5.47428553e-01 6.90420768e+02 5.12398297e+01 5.13082682e+01\n",
      "  6.34504068e+04 8.00000000e-01 4.63685583e+04 1.53319930e+04\n",
      "  2.65720898e+04]\n",
      " [2.68370102e-04 1.16288545e+01 1.20144830e+01 1.73911789e+03\n",
      "  1.92201983e+02 4.65132295e+01 2.37620539e+03 1.76721465e+03\n",
      "  1.38996680e+04 7.16312715e+03 3.06740680e+04 6.12725583e+04\n",
      "  4.34417243e+03 4.63685583e+04 8.00000000e-01 1.36636724e+04\n",
      "  4.47782243e+03]\n",
      " [9.87278433e-05 2.68370102e-04 7.29505572e-04 1.08114483e+02\n",
      "  7.29505572e-04 2.68370102e-04 1.20144830e+01 1.89117890e+02\n",
      "  1.48782715e+03 7.62617890e+02 4.03281465e+03 1.88651930e+04\n",
      "  8.64720768e+02 1.53319930e+04 1.36636724e+04 8.00000000e-01\n",
      "  4.00620768e+02]\n",
      " [9.87278433e-05 2.68370102e-04 4.65132295e+01 4.80519830e+01\n",
      "  1.04653855e+02 2.68370102e-04 1.08114483e+02 9.76508515e+02\n",
      "  1.49646465e+03 2.34361789e+03 3.55112715e+03 3.00506680e+04\n",
      "  1.45908268e+02 2.65720898e+04 4.47782243e+03 4.00620768e+02\n",
      "  8.00000000e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering = AffinityPropagation(affinity='precomputed', damping=0.7, random_state=42, convergence_iter=15, max_iter=1000)\n",
    "S = np.array(S)\n",
    "print(S)\n",
    "max_value = np.max(S)\n",
    "np.fill_diagonal(S, max_value)\n",
    "clustering.fit(S)\n",
    "clustering.labels_\n",
    "# clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3].x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sclab\\Documents\\Lab\\Subgraph and partitioning method\\graph-ap\\experiments\\3-pre-model-integration.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sclab/Documents/Lab/Subgraph%20and%20partitioning%20method/graph-ap/experiments/3-pre-model-integration.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m Gs[\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mnodes:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sclab/Documents/Lab/Subgraph%20and%20partitioning%20method/graph-ap/experiments/3-pre-model-integration.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(Gs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnodes[node])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for node in Gs[6].nodes:\n",
    "    print(Gs[0].nodes[node])\n",
    "    # Udah bisa tambah nodes per batch, tinggal masukin ke algo utama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  5,  5,  5,  6,  6,  6,\n",
       "          7,  8,  9,  9,  9, 10, 11, 11, 11, 12, 12, 12, 13, 13, 14, 14, 15, 15,\n",
       "         15, 16, 16, 17, 17, 17, 18, 19],\n",
       "        [ 1,  5,  0,  2,  1,  3, 12,  2,  4,  9,  3,  5,  0,  4,  6,  5,  7,  8,\n",
       "          6,  6,  3, 10, 11,  9,  9, 12, 16,  2, 11, 13, 12, 14, 13, 15, 14, 16,\n",
       "         17, 11, 15, 15, 18, 19, 17, 17]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[63].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "1 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "2 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "3 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "4 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "5 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "6 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "7 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "8 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "9 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "10 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "11 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "12 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "13 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "14 tensor([0., 1., 0., 0., 0., 0., 0.])\n",
      "15 tensor([0., 0., 1., 0., 0., 0., 0.])\n",
      "16 tensor([0., 0., 1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "    if (b == 0):\n",
    "        print(i, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 720], x=[337, 7], edge_attr=[720, 4], y=[22], batch=[337], ptr=[23])\n",
      "=== Graph 0 ===\n",
      "Graph with 14 nodes and 14 edges\n",
      "edge_idx [[tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(8), tensor(9), tensor(9), tensor(9), tensor(10), tensor(11), tensor(12), tensor(12), tensor(13)], [tensor(1), tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(12), tensor(2), tensor(4), tensor(9), tensor(3), tensor(5), tensor(0), tensor(4), tensor(6), tensor(5), tensor(7), tensor(8), tensor(6), tensor(6), tensor(3), tensor(10), tensor(11), tensor(9), tensor(9), tensor(2), tensor(13), tensor(12)]]\n",
      "13\n",
      "14\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "[0 0 2 1 1 0 0 0 0 1 1 1 2 2]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11]]\n",
      "=== Graph 1 ===\n",
      "Graph with 11 nodes and 11 edges\n",
      "edge_idx [[tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(7), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(10)], [tensor(0), tensor(2), tensor(1), tensor(3), tensor(7), tensor(2), tensor(4), tensor(3), tensor(5), tensor(4), tensor(6), tensor(5), tensor(7), tensor(2), tensor(6), tensor(8), tensor(7), tensor(9), tensor(10), tensor(8), tensor(8)]]\n",
      "10\n",
      "11\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[0 0 0 0 0 1 1 1 1 1 1]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24]]\n",
      "=== Graph 2 ===\n",
      "Graph with 19 nodes and 20 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(11), tensor(12), tensor(12), tensor(13), tensor(13), tensor(13), tensor(14), tensor(14), tensor(15), tensor(15), tensor(16), tensor(16), tensor(16), tensor(17), tensor(18)], [tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(2), tensor(4), tensor(6), tensor(3), tensor(5), tensor(0), tensor(4), tensor(3), tensor(7), tensor(6), tensor(8), tensor(7), tensor(9), tensor(10), tensor(8), tensor(8), tensor(11), tensor(15), tensor(10), tensor(12), tensor(11), tensor(13), tensor(12), tensor(14), tensor(16), tensor(13), tensor(15), tensor(10), tensor(14), tensor(13), tensor(17), tensor(18), tensor(16), tensor(16)]]\n",
      "18\n",
      "19\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 2 2 2 2]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43]]\n",
      "=== Graph 3 ===\n",
      "Graph with 11 nodes and 11 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(10)], [tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(2), tensor(4), tensor(8), tensor(3), tensor(5), tensor(6), tensor(0), tensor(4), tensor(4), tensor(7), tensor(6), tensor(3), tensor(9), tensor(10), tensor(8), tensor(8)]]\n",
      "10\n",
      "11\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[1 1 1 1 0 0 0 0 1 1 1]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51]]\n",
      "=== Graph 4 ===\n",
      "Graph with 21 nodes and 23 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(7), tensor(8), tensor(8), tensor(9), tensor(9), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(11), tensor(12), tensor(12), tensor(12), tensor(13), tensor(13), tensor(13), tensor(14), tensor(14), tensor(14), tensor(15), tensor(16), tensor(16), tensor(16), tensor(17), tensor(18), tensor(19), tensor(20), tensor(20)], [tensor(5), tensor(0), tensor(2), tensor(20), tensor(1), tensor(3), tensor(8), tensor(2), tensor(4), tensor(3), tensor(5), tensor(7), tensor(0), tensor(4), tensor(6), tensor(5), tensor(4), tensor(2), tensor(9), tensor(8), tensor(10), tensor(14), tensor(9), tensor(11), tensor(20), tensor(10), tensor(12), tensor(11), tensor(13), tensor(19), tensor(12), tensor(14), tensor(16), tensor(9), tensor(13), tensor(15), tensor(14), tensor(13), tensor(17), tensor(18), tensor(16), tensor(16), tensor(12), tensor(1), tensor(10)]]\n",
      "20\n",
      "21\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[0 2 1 1 1 0 0 1 1 1 2 2 2 4 3 3 4 4 4 2 0]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70]]\n",
      "=== Graph 5 ===\n",
      "Graph with 22 nodes and 25 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(9), tensor(9), tensor(10), tensor(10), tensor(11), tensor(11), tensor(11), tensor(12), tensor(12), tensor(13), tensor(13), tensor(13), tensor(14), tensor(14), tensor(15), tensor(15), tensor(16), tensor(16), tensor(16), tensor(17), tensor(18), tensor(19), tensor(19), tensor(19), tensor(20), tensor(21)], [tensor(9), tensor(0), tensor(2), tensor(1), tensor(3), tensor(7), tensor(2), tensor(4), tensor(3), tensor(5), tensor(19), tensor(4), tensor(6), tensor(5), tensor(7), tensor(15), tensor(2), tensor(6), tensor(8), tensor(7), tensor(9), tensor(13), tensor(0), tensor(8), tensor(10), tensor(9), tensor(11), tensor(10), tensor(12), tensor(16), tensor(11), tensor(13), tensor(8), tensor(12), tensor(14), tensor(13), tensor(15), tensor(6), tensor(14), tensor(11), tensor(17), tensor(18), tensor(16), tensor(16), tensor(4), tensor(20), tensor(21), tensor(19), tensor(19)]]\n",
      "21\n",
      "22\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[2 3 3 0 0 3 3 3 2 2 2 1 1 2 2 3 2 1 1 3 0 0]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88]]\n",
      "=== Graph 6 ===\n",
      "Graph with 11 nodes and 11 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(8), tensor(9), tensor(10)], [tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(10), tensor(2), tensor(4), tensor(9), tensor(3), tensor(5), tensor(0), tensor(4), tensor(6), tensor(5), tensor(7), tensor(8), tensor(6), tensor(6), tensor(3), tensor(2)]]\n",
      "10\n",
      "11\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[2 0 0 1 1 1 2 2 2 1 0]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87, 106, 104, 99, 100, 107, 101, 102], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88, 104, 105, 100, 108, 101, 102, 103]]\n",
      "=== Graph 7 ===\n",
      "Graph with 20 nodes and 23 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(9), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(11), tensor(11), tensor(12), tensor(12), tensor(12), tensor(13), tensor(13), tensor(14), tensor(14), tensor(15), tensor(15), tensor(16), tensor(17), tensor(17), tensor(17), tensor(18), tensor(19)], [tensor(13), tensor(0), tensor(2), tensor(1), tensor(3), tensor(11), tensor(2), tensor(4), tensor(3), tensor(5), tensor(4), tensor(6), tensor(10), tensor(5), tensor(7), tensor(17), tensor(6), tensor(8), tensor(7), tensor(9), tensor(16), tensor(8), tensor(10), tensor(15), tensor(5), tensor(9), tensor(11), tensor(2), tensor(10), tensor(12), tensor(11), tensor(13), tensor(14), tensor(0), tensor(12), tensor(12), tensor(15), tensor(9), tensor(14), tensor(8), tensor(6), tensor(18), tensor(19), tensor(17), tensor(17)]]\n",
      "19\n",
      "20\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[2 2 0 2 0 0 1 1 1 4 4 2 3 2 3 4 4 1 1 1]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87, 106, 104, 99, 100, 107, 101, 102, 109, 109, 113, 115, 115, 116, 126, 126, 118, 118, 121], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88, 104, 105, 100, 108, 101, 102, 103, 122, 110, 114, 116, 126, 117, 127, 128, 119, 124, 123]]\n",
      "=== Graph 8 ===\n",
      "Graph with 22 nodes and 25 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(7), tensor(8), tensor(8), tensor(9), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(11), tensor(11), tensor(12), tensor(12), tensor(12), tensor(13), tensor(13), tensor(14), tensor(14), tensor(14), tensor(15), tensor(15), tensor(15), tensor(16), tensor(16), tensor(16), tensor(17), tensor(18), tensor(19), tensor(19), tensor(19), tensor(20), tensor(21)], [tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(2), tensor(4), tensor(12), tensor(3), tensor(5), tensor(6), tensor(0), tensor(4), tensor(4), tensor(7), tensor(11), tensor(6), tensor(8), tensor(7), tensor(9), tensor(8), tensor(10), tensor(9), tensor(11), tensor(15), tensor(6), tensor(10), tensor(12), tensor(3), tensor(11), tensor(13), tensor(12), tensor(14), tensor(13), tensor(15), tensor(19), tensor(10), tensor(14), tensor(16), tensor(15), tensor(17), tensor(18), tensor(16), tensor(16), tensor(14), tensor(20), tensor(21), tensor(19), tensor(19)]]\n",
      "21\n",
      "22\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[0 0 0 0 1 0 1 1 2 2 2 1 0 3 3 2 2 2 2 3 3 3]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87, 106, 104, 99, 100, 107, 101, 102, 109, 109, 113, 115, 115, 116, 126, 126, 118, 118, 121, 129, 129, 130, 131, 132, 140, 133, 135, 137, 138, 139, 144, 145, 145, 142, 143, 148, 148], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88, 104, 105, 100, 108, 101, 102, 103, 122, 110, 114, 116, 126, 117, 127, 128, 119, 124, 123, 134, 130, 131, 132, 141, 135, 135, 136, 138, 139, 144, 145, 146, 147, 143, 148, 149, 150]]\n",
      "=== Graph 9 ===\n",
      "Graph with 13 nodes and 14 edges\n",
      "edge_idx [[tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(6), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(12)], [tensor(0), tensor(2), tensor(9), tensor(1), tensor(3), tensor(2), tensor(4), tensor(8), tensor(3), tensor(5), tensor(4), tensor(6), tensor(10), tensor(5), tensor(7), tensor(6), tensor(8), tensor(3), tensor(7), tensor(9), tensor(1), tensor(8), tensor(5), tensor(11), tensor(12), tensor(10), tensor(10)]]\n",
      "12\n",
      "13\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[2 2 0 0 0 1 1 1 2 2 0 0 0]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87, 106, 104, 99, 100, 107, 101, 102, 109, 109, 113, 115, 115, 116, 126, 126, 118, 118, 121, 129, 129, 130, 131, 132, 140, 133, 135, 137, 138, 139, 144, 145, 145, 142, 143, 148, 148, 151, 152, 159, 153, 154, 161, 161, 156, 157], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88, 104, 105, 100, 108, 101, 102, 103, 122, 110, 114, 116, 126, 117, 127, 128, 119, 124, 123, 134, 130, 131, 132, 141, 135, 135, 136, 138, 139, 144, 145, 146, 147, 143, 148, 149, 150, 152, 160, 160, 154, 155, 162, 163, 157, 158]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# idx_from = 0\n",
    "# idx_to = 0\n",
    "graph_counter = 0\n",
    "graph_bound\n",
    "edge_index = [[],[]]\n",
    "subgraph_edge_index = [[],[]]\n",
    "Gs = []\n",
    "sub_created = False\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "batches = []\n",
    "for b in train_loader:\n",
    "    batches.append(b)\n",
    "batch1 = batches[2]\n",
    "print(batch1)\n",
    "\n",
    "# return 0\n",
    "graph_bound = {}\n",
    "\n",
    "for i in range(len(batch1.ptr)-1):\n",
    "    graph_bound[i] = [batch1.ptr[i].item(), batch1.ptr[i+1].item()]\n",
    "    # print(str(i)+\".\", batch1.ptr[i].item(), \"-\", batch1.ptr[i+1].item())\n",
    "\n",
    "for i, (src, dst) in enumerate(zip(batch1.edge_index[0], batch1.edge_index[1])):\n",
    "    # if (graph_counter < len(batch1.ptr)):\n",
    "    lower_bound = graph_bound[graph_counter][0]\n",
    "    upper_bound = graph_bound[graph_counter][1]\n",
    "    if ((src >= lower_bound and src < upper_bound) or\n",
    "        (dst >= lower_bound and dst < upper_bound)):\n",
    "        # print(i,src.item()-lower_bound, dst.item()-lower_bound)\n",
    "        edge_index[0].append(src - lower_bound)\n",
    "        edge_index[1].append(dst - lower_bound)\n",
    "    else:\n",
    "        sub_created = True\n",
    "        # continue\n",
    "        \n",
    "        \n",
    "        # # print(edge_index)\n",
    "        # embs = []\n",
    "        # # make new graph\n",
    "        # for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "        #     if (b == graph_counter):\n",
    "        #         # print(i, emb)\n",
    "        #         embs.append(emb)\n",
    "        \n",
    "        # G = data_transformation(edge_index, embs)\n",
    "        # Gs.append(G)\n",
    "        # print(sorted(list(G.nodes)))\n",
    "        # # print('pre', precalc_shortest_path_length)\n",
    "        # # for node in sorted(list(G.nodes)):\n",
    "        # #     print(G.nodes[node])\n",
    "        \n",
    "        \n",
    "        # # testt()\n",
    "        # if graph_counter == 10:\n",
    "        #     print('masalah disini bro')\n",
    "        #     break\n",
    "        \n",
    "        # # Calculate S matrix\n",
    "        # S = calculate_similarity_matrix(G)\n",
    "        \n",
    "        # # AP Clustering\n",
    "        # clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "\n",
    "        # print(clustering.labels_)\n",
    "        # # print(clustering.)\n",
    "        \n",
    "        # communities = {}\n",
    "        # print(\"cluster labels:\", set(clustering.labels_))\n",
    "        # # communities init\n",
    "        # for lab in clustering.labels_:\n",
    "        #     communities[lab] = []\n",
    "        \n",
    "        # for nd, clust in enumerate(clustering.labels_):\n",
    "        #     communities[clust].append(nd)\n",
    "        # print(\"communities\", communities) \n",
    "            \n",
    "        # edge_index = [[],[]]\n",
    "        # graph_counter+=1\n",
    "        \n",
    "        # # make subgraph edge_index\n",
    "        # for c in communities:\n",
    "        #     w = G.subgraph(communities[c])\n",
    "        #     # print(\"edges subgraph\", w.edges)\n",
    "        #     for sub in w.edges:\n",
    "        #         # print(sub[0], sub[1])\n",
    "        #         subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "        #         subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                \n",
    "        # print(subgraph_edge_index)\n",
    "        # print(f'=== Graph {graph_counter} ===') \n",
    "        \n",
    "    if (i == len(batch1.edge_index[0]) - 1) or sub_created:\n",
    "        print(f'=== Graph {graph_counter} ===')\n",
    "        \n",
    "        sub_created = False\n",
    "        \n",
    "        embs = []\n",
    "        # make new graph\n",
    "        for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "            if (b == graph_counter):\n",
    "                # print(i, emb)\n",
    "                embs.append(emb)\n",
    "        \n",
    "        G = data_transformation(edge_index, embs)\n",
    "        print(G)\n",
    "        print(\"edge_idx\", edge_index)\n",
    "        print(max(G.nodes))\n",
    "        print(len(G.nodes))\n",
    "        \n",
    "        Gs.append(G)\n",
    "        \n",
    "        S = calculate_similarity_matrix(G)\n",
    "        # AP Clustering        \n",
    "        clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "        \n",
    "        print(sorted(list(G.nodes)))\n",
    "        print(clustering.labels_)\n",
    "        \n",
    "        # Modif disini nanti\n",
    "        #########\n",
    "        communities = {}\n",
    "        # print(\"cluster labels:\", set(clustering.labels_))\n",
    "        # communities init\n",
    "        for lab in clustering.labels_:\n",
    "            communities[lab] = []\n",
    "        \n",
    "        for nd, clust in enumerate(clustering.labels_):\n",
    "            communities[clust].append(nd)\n",
    "        # print(\"communities\", communities) \n",
    "            \n",
    "        edge_index = [[],[]]\n",
    "        graph_counter+=1\n",
    "        \n",
    "        # make subgraph edge_index\n",
    "        for c in communities:\n",
    "            w = G.subgraph(communities[c])\n",
    "            # print(\"edges subgraph\", w.edges)\n",
    "            for sub in w.edges:\n",
    "                # print(sub[0], sub[1])\n",
    "                subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "                subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                \n",
    "        print(subgraph_edge_index)\n",
    "        if (graph_counter == 10):\n",
    "            break\n",
    "        # print(f'=== Graph {graph_counter} ===')\n",
    "        # print(edge_index)\n",
    "        # print('udh di akhir')\n",
    "\n",
    "embeddings_used = []\n",
    "nodes_used = set(np.array(subgraph_edge_index).flatten())\n",
    "feat = batch1.x\n",
    "# for i, f in enumerate(feat):\n",
    "#     if(i in nodes_used):\n",
    "#         embeddings_used.append(feat[i].detach().numpy())\n",
    "        \n",
    "# print(len(embeddings_used))\n",
    "# z = torch.zeros(len(embeddings_used), len(embeddings_used[0]))\n",
    "# z = torch.tensor(embeddings_used)\n",
    "\n",
    "# print(torch.tensor(subgraph_edge_index).size())\n",
    "# nodes_used = set(np.array(subgraph_edge_index).flatten())\n",
    "# feat = batch1.x\n",
    "# for i, f in enumerate(feat):\n",
    "#     if(i in nodes_used):\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "\n",
    "\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # embeddings for subgraph\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # self.conv6 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels*2, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, ptr):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv3(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        # G = data_transformation(edge_index, feature_emb)\n",
    "        # S = calculate_similarity_matrix(G)\n",
    "        # clustering = AffinityPropagation(affinity='precomputed', random_state=123, max_iter=200).fit(S)\n",
    "        subgraph_edge_index, _ = self.subgraph_generator(feature_emb, edge_index, batch, ptr)\n",
    "        subgraph_embedding = self.conv4(embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        subgraph_embedding = self.conv5(subgraph_embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        # subgraph_embedding = self.conv1(x, subgraph_edge_index)\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        # subgraph_embedding = self.conv2(subgraph_embedding, subgraph_edge_index)\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        # subgraph_embedding = self.conv3(subgraph_embedding, subgraph_edge_index)\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        # print(subgraph_edge_index)\n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        # self.subgraph_pooling(\"\",\"\",\"\")\n",
    "        subgraph_embedding = global_max_pool(subgraph_embedding, batch)\n",
    "        \n",
    "        \n",
    "        h = torch.cat((embedding, subgraph_embedding), 1)\n",
    "        \n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin(h)\n",
    "        h = h.relu()\n",
    "        x = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h\n",
    "    \n",
    "    def subgraph_pooling(self, embeddings, batch, ptr):\n",
    "        print('subgraph pooling')\n",
    "\n",
    "    def subgraph_generator(self, embeddings, batch_edge_index, batch, ptr):\n",
    "        '''\n",
    "        Return subgraph_edge_index (edge_index of created subgraph)\n",
    "        '''\n",
    "        # print('processing subgraph_generator...')\n",
    "        graph_counter = 0\n",
    "        edge_index = [[],[]]\n",
    "        subgraph_edge_index = [[],[]]\n",
    "        # Gs = []\n",
    "        sub_created = False\n",
    "        graph_bound = {}\n",
    "\n",
    "        for i in range(len(ptr)-1):\n",
    "            graph_bound[i] = [ptr[i].item(), ptr[i+1].item()]\n",
    "        \n",
    "        for i, (src, dst) in enumerate(zip(batch_edge_index[0], batch_edge_index[1])):\n",
    "            lower_bound = graph_bound[graph_counter][0]\n",
    "            upper_bound = graph_bound[graph_counter][1]\n",
    "            if ((src >= lower_bound and src < upper_bound) or\n",
    "                (dst >= lower_bound and dst < upper_bound)):\n",
    "                \n",
    "                edge_index[0].append(src - lower_bound)\n",
    "                edge_index[1].append(dst - lower_bound)\n",
    "            else:\n",
    "                sub_created = True\n",
    "                \n",
    "            if (i == len(batch_edge_index[0]) - 1) or sub_created:\n",
    "                # print(f'=== Graph {graph_counter} ===')\n",
    "                \n",
    "                sub_created = False\n",
    "                \n",
    "                embs = []\n",
    "                # make new graph\n",
    "                for i, (b, emb) in enumerate(zip(batch, embeddings)):\n",
    "                    if (b == graph_counter):\n",
    "                        embs.append(emb)\n",
    "                \n",
    "                G = data_transformation(edge_index, embs)\n",
    "                # dont need this at the moment\n",
    "                # Gs.append(G)\n",
    "                \n",
    "                # Calculate similarity matrix\n",
    "                S = calculate_similarity_matrix(G)\n",
    "                \n",
    "                # AP Clustering        \n",
    "                # clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "                clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, convergence_iter=5, max_iter=100).fit(S)\n",
    "                \n",
    "                # Get community\n",
    "                communities = {}\n",
    "                for lab in clustering.labels_:\n",
    "                    communities[lab] = []\n",
    "                \n",
    "                for nd, clust in enumerate(clustering.labels_):\n",
    "                    communities[clust].append(nd)\n",
    "                \n",
    "                edge_index = [[],[]]\n",
    "                graph_counter+=1\n",
    "                \n",
    "                # Make subgraph edge_index\n",
    "                for c in communities:\n",
    "                    w = G.subgraph(communities[c])\n",
    "                    for sub in w.edges:\n",
    "                        subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "                        subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                        \n",
    "                # INI LUPA WOY\n",
    "                # if (graph_counter == 10):\n",
    "                #     break\n",
    "                \n",
    "                \n",
    "        # print(\"finished subgraph_generator\")\n",
    "        \n",
    "        # embeddings_used = []\n",
    "        # nodes_used = set(np.array(subgraph_edge_index).flatten())\n",
    "        \n",
    "        # for i, f in enumerate(embeddings):\n",
    "        #     if(i in nodes_used):\n",
    "        #         embeddings_used.append(embeddings[i].detach().numpy())\n",
    "                \n",
    "        \n",
    "        # print(\"nodes used\", len(nodes_used))\n",
    "        # print(\"nodes used\", (nodes_used))\n",
    "        # z = torch.tensor(embeddings_used)\n",
    "        # print(z.size())\n",
    "        \n",
    "        # return torch.tensor(subgraph_edge_index), z\n",
    "        # pakai embeddings yang awal\n",
    "        return torch.tensor(subgraph_edge_index), torch.tensor(embeddings)\n",
    "    # (embeddings)\n",
    "    \n",
    "btch = None\n",
    "experiment = Experiment(dataset, 64)\n",
    "bcount = 0\n",
    "for b in train_loader:\n",
    "    # print(\"batch count\",bcount)\n",
    "    bcount+=1\n",
    "    btch = b\n",
    "    # print(btch.ptr)\n",
    "    experiment(btch.x, btch.edge_index, btch.batch, btch.ptr)\n",
    "    break\n",
    "    # break\n",
    "    # experiment(btch.x, btch.edge_index, btch.batch)\n",
    "\n",
    "# print(experiment)\n",
    "# experiment(btch.x, btch.edge_index, btch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expTrain(train_loader, val_loader, test_loader, epoch = 2):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "    experiment = Experiment(dataset, 64)\n",
    "\n",
    "    # Train\n",
    "    print('process training')\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(experiment, train_loader, True).item(), 5)\n",
    "        train_acc = round(test_base(experiment, train_loader, True), 5)\n",
    "        val_acc = round(test_base(experiment, val_loader, True), 5)\n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; test_acc: {val_acc}')\n",
    "\n",
    "    # Test\n",
    "    print('process testing')\n",
    "    test = test_base(experiment, test_loader, True)\n",
    "    print(f'Accuracy: {test}')\n",
    "\n",
    "# expTrain(train_loader, val_loader, test_loader, epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseTrain(train_loader, val_loader, test_loader, epoch = 10):\n",
    "    base = Base(dataset, 64)\n",
    "\n",
    "    # Train\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(base, train_loader).item(), 5)\n",
    "        train_acc = round(test_base(base, train_loader), 5)\n",
    "        val_acc = round(test_base(base, val_loader), 5)\n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; val_acc: {val_acc}; test: {round(test_base(base, test_loader), 2)}')\n",
    "\n",
    "    # Test\n",
    "    test = test_base(base, test_loader)\n",
    "    print(f'Accuracy: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 1.84328; train_acc: 0.32593; val_acc: 0.4; test: 0.26\n",
      "epoch 1; loss: 0.84091; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 2; loss: 0.89693; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 3; loss: 1.08072; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 4; loss: 1.38027; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 5; loss: 0.69686; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 6; loss: 0.72891; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 7; loss: 0.82262; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 8; loss: 0.5762; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 9; loss: 0.67631; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 10; loss: 0.5856; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 11; loss: 0.59029; train_acc: 0.71111; val_acc: 0.66667; test: 0.68\n",
      "epoch 12; loss: 0.81153; train_acc: 0.71852; val_acc: 0.66667; test: 0.68\n",
      "epoch 13; loss: 1.25675; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 14; loss: 0.6397; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 15; loss: 0.55276; train_acc: 0.77037; val_acc: 0.73333; test: 0.79\n",
      "epoch 16; loss: 0.57848; train_acc: 0.77778; val_acc: 0.66667; test: 0.68\n",
      "epoch 17; loss: 1.42035; train_acc: 0.75556; val_acc: 0.66667; test: 0.68\n",
      "epoch 18; loss: 1.96551; train_acc: 0.74815; val_acc: 0.66667; test: 0.68\n",
      "epoch 19; loss: 3.21801; train_acc: 0.77778; val_acc: 0.66667; test: 0.79\n",
      "epoch 20; loss: 0.60077; train_acc: 0.68148; val_acc: 0.6; test: 0.74\n",
      "epoch 21; loss: 0.64884; train_acc: 0.77778; val_acc: 0.66667; test: 0.74\n",
      "epoch 22; loss: 1.43668; train_acc: 0.77778; val_acc: 0.66667; test: 0.74\n",
      "epoch 23; loss: 2.61952; train_acc: 0.77778; val_acc: 0.66667; test: 0.79\n",
      "epoch 24; loss: 0.54914; train_acc: 0.76296; val_acc: 0.66667; test: 0.74\n",
      "epoch 25; loss: 1.2199; train_acc: 0.76296; val_acc: 0.66667; test: 0.79\n",
      "epoch 26; loss: 1.26362; train_acc: 0.77778; val_acc: 0.66667; test: 0.74\n",
      "epoch 27; loss: 2.40328; train_acc: 0.77778; val_acc: 0.66667; test: 0.79\n",
      "epoch 28; loss: 0.57488; train_acc: 0.79259; val_acc: 0.66667; test: 0.74\n",
      "epoch 29; loss: 1.43859; train_acc: 0.76296; val_acc: 0.66667; test: 0.79\n",
      "epoch 30; loss: 0.5386; train_acc: 0.76296; val_acc: 0.66667; test: 0.79\n",
      "epoch 31; loss: 2.72388; train_acc: 0.77778; val_acc: 0.66667; test: 0.79\n",
      "epoch 32; loss: 0.60178; train_acc: 0.67407; val_acc: 0.6; test: 0.74\n",
      "epoch 33; loss: 0.65257; train_acc: 0.78519; val_acc: 0.66667; test: 0.74\n",
      "epoch 34; loss: 1.67026; train_acc: 0.74815; val_acc: 0.66667; test: 0.68\n",
      "epoch 35; loss: 2.94957; train_acc: 0.77778; val_acc: 0.66667; test: 0.79\n",
      "epoch 36; loss: 0.51224; train_acc: 0.77778; val_acc: 0.66667; test: 0.74\n",
      "epoch 37; loss: 1.0368; train_acc: 0.73333; val_acc: 0.66667; test: 0.74\n",
      "epoch 38; loss: 2.27043; train_acc: 0.78519; val_acc: 0.73333; test: 0.74\n",
      "epoch 39; loss: 0.47467; train_acc: 0.76296; val_acc: 0.66667; test: 0.68\n",
      "epoch 40; loss: 2.73784; train_acc: 0.58519; val_acc: 0.6; test: 0.53\n",
      "epoch 41; loss: 0.50537; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 42; loss: 0.56372; train_acc: 0.78519; val_acc: 0.66667; test: 0.74\n",
      "epoch 43; loss: 1.46095; train_acc: 0.74815; val_acc: 0.66667; test: 0.68\n",
      "epoch 44; loss: 0.84892; train_acc: 0.68889; val_acc: 0.6; test: 0.74\n",
      "epoch 45; loss: 2.10622; train_acc: 0.77778; val_acc: 0.66667; test: 0.74\n",
      "epoch 46; loss: 0.41097; train_acc: 0.77037; val_acc: 0.66667; test: 0.74\n",
      "epoch 47; loss: 0.86997; train_acc: 0.76296; val_acc: 0.66667; test: 0.68\n",
      "epoch 48; loss: 0.56845; train_acc: 0.77778; val_acc: 0.66667; test: 0.74\n",
      "epoch 49; loss: 0.88193; train_acc: 0.75556; val_acc: 0.66667; test: 0.79\n",
      "Accuracy: 0.7894736842105263\n",
      "Fold 1/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 2.60955; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 1; loss: 1.12844; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 2; loss: 1.42592; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 3; loss: 1.88493; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 4; loss: 0.67154; train_acc: 0.74815; val_acc: 0.93333; test: 0.74\n",
      "epoch 5; loss: 3.87703; train_acc: 0.66667; val_acc: 0.73333; test: 0.74\n",
      "epoch 6; loss: 0.68322; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 7; loss: 0.69168; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 8; loss: 0.74396; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 9; loss: 1.09367; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 10; loss: 1.44043; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 11; loss: 0.68887; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 12; loss: 0.68338; train_acc: 0.66667; val_acc: 0.66667; test: 0.74\n",
      "epoch 13; loss: 1.15209; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 14; loss: 0.61021; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 15; loss: 0.82318; train_acc: 0.76296; val_acc: 0.73333; test: 0.79\n",
      "epoch 16; loss: 1.36593; train_acc: 0.68148; val_acc: 0.8; test: 0.74\n",
      "epoch 17; loss: 0.68351; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 18; loss: 0.67086; train_acc: 0.66667; val_acc: 0.66667; test: 0.74\n",
      "epoch 19; loss: 0.68323; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 20; loss: 0.65251; train_acc: 0.66667; val_acc: 0.73333; test: 0.74\n",
      "epoch 21; loss: 0.68298; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 22; loss: 0.63047; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 23; loss: 0.68108; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 24; loss: 0.59078; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 25; loss: 0.65337; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 26; loss: 0.54913; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 27; loss: 0.58403; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 28; loss: 0.53605; train_acc: 0.64444; val_acc: 0.73333; test: 0.68\n",
      "epoch 29; loss: 0.70084; train_acc: 0.67407; val_acc: 0.8; test: 0.68\n",
      "epoch 30; loss: 0.67154; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 31; loss: 0.56834; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 32; loss: 0.62834; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 33; loss: 0.53605; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 34; loss: 0.56172; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 35; loss: 0.51964; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 36; loss: 0.60166; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 37; loss: 0.51404; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 38; loss: 0.52072; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 39; loss: 0.52016; train_acc: 0.67407; val_acc: 0.8; test: 0.74\n",
      "epoch 40; loss: 0.58571; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 41; loss: 0.49892; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 42; loss: 0.50993; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 43; loss: 0.49723; train_acc: 0.67407; val_acc: 0.8; test: 0.74\n",
      "epoch 44; loss: 0.55485; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 45; loss: 0.48212; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 46; loss: 0.5061; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 47; loss: 0.47754; train_acc: 0.67407; val_acc: 0.8; test: 0.74\n",
      "epoch 48; loss: 0.52069; train_acc: 0.35556; val_acc: 0.33333; test: 0.21\n",
      "epoch 49; loss: 0.46657; train_acc: 0.67407; val_acc: 0.8; test: 0.74\n",
      "Accuracy: 0.7368421052631579\n",
      "Fold 2/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 1.69246; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 1; loss: 0.71299; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 2; loss: 0.96173; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 3; loss: 0.58512; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 4; loss: 0.74433; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 5; loss: 0.79507; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 6; loss: 1.09559; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 7; loss: 0.71242; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 8; loss: 0.88922; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 9; loss: 0.69317; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 10; loss: 0.98351; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 11; loss: 0.74577; train_acc: 0.34074; val_acc: 0.33333; test: 0.26\n",
      "epoch 12; loss: 0.57731; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 13; loss: 0.56602; train_acc: 0.61481; val_acc: 0.73333; test: 0.53\n",
      "epoch 14; loss: 0.52125; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 15; loss: 0.61812; train_acc: 0.57037; val_acc: 0.6; test: 0.37\n",
      "epoch 16; loss: 0.46155; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 17; loss: 0.49255; train_acc: 0.57037; val_acc: 0.66667; test: 0.37\n",
      "epoch 18; loss: 0.40976; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 19; loss: 0.43876; train_acc: 0.51852; val_acc: 0.53333; test: 0.32\n",
      "epoch 20; loss: 0.3464; train_acc: 0.65926; val_acc: 0.66667; test: 0.74\n",
      "epoch 21; loss: 0.38222; train_acc: 0.64444; val_acc: 0.73333; test: 0.74\n",
      "epoch 22; loss: 0.31832; train_acc: 0.68889; val_acc: 0.73333; test: 0.74\n",
      "epoch 23; loss: 0.67709; train_acc: 0.71111; val_acc: 0.73333; test: 0.79\n",
      "epoch 24; loss: 0.31459; train_acc: 0.67407; val_acc: 0.66667; test: 0.74\n",
      "epoch 25; loss: 0.36719; train_acc: 0.57778; val_acc: 0.66667; test: 0.32\n",
      "epoch 26; loss: 0.25882; train_acc: 0.66667; val_acc: 0.66667; test: 0.74\n",
      "epoch 27; loss: 0.22688; train_acc: 0.72593; val_acc: 0.8; test: 0.79\n",
      "epoch 28; loss: 0.25474; train_acc: 0.68889; val_acc: 0.73333; test: 0.74\n",
      "epoch 29; loss: 0.28903; train_acc: 0.64444; val_acc: 0.8; test: 0.74\n",
      "epoch 30; loss: 0.22984; train_acc: 0.66667; val_acc: 0.73333; test: 0.74\n",
      "epoch 31; loss: 0.22253; train_acc: 0.66667; val_acc: 0.73333; test: 0.79\n",
      "epoch 32; loss: 0.20034; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 33; loss: 0.19889; train_acc: 0.7037; val_acc: 0.73333; test: 0.79\n",
      "epoch 34; loss: 0.18022; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 35; loss: 0.15532; train_acc: 0.72593; val_acc: 0.8; test: 0.79\n",
      "epoch 36; loss: 0.09946; train_acc: 0.68889; val_acc: 0.73333; test: 0.74\n",
      "epoch 37; loss: 0.24115; train_acc: 0.72593; val_acc: 0.86667; test: 0.79\n",
      "epoch 38; loss: 0.1835; train_acc: 0.6963; val_acc: 0.73333; test: 0.74\n",
      "epoch 39; loss: 0.23765; train_acc: 0.71111; val_acc: 0.73333; test: 0.79\n",
      "epoch 40; loss: 0.16405; train_acc: 0.68148; val_acc: 0.73333; test: 0.74\n",
      "epoch 41; loss: 0.13821; train_acc: 0.7037; val_acc: 0.73333; test: 0.79\n",
      "epoch 42; loss: 0.0588; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 43; loss: 0.08477; train_acc: 0.72593; val_acc: 0.8; test: 0.79\n",
      "epoch 44; loss: 0.05298; train_acc: 0.67407; val_acc: 0.73333; test: 0.74\n",
      "epoch 45; loss: 0.07884; train_acc: 0.72593; val_acc: 0.8; test: 0.79\n",
      "epoch 46; loss: 0.03924; train_acc: 0.68148; val_acc: 0.73333; test: 0.74\n",
      "epoch 47; loss: 0.06395; train_acc: 0.73333; val_acc: 0.8; test: 0.79\n",
      "epoch 48; loss: 0.03823; train_acc: 0.68148; val_acc: 0.73333; test: 0.74\n",
      "epoch 49; loss: 0.06879; train_acc: 0.73333; val_acc: 0.8; test: 0.79\n",
      "Accuracy: 0.7894736842105263\n",
      "Fold 3/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 1.80843; train_acc: 0.34815; val_acc: 0.26667; test: 0.26\n",
      "epoch 1; loss: 0.79064; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 2; loss: 0.80782; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 3; loss: 0.92812; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 4; loss: 0.52728; train_acc: 0.34815; val_acc: 0.26667; test: 0.26\n",
      "epoch 5; loss: 0.49853; train_acc: 0.65926; val_acc: 0.66667; test: 0.79\n",
      "epoch 6; loss: 0.65897; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 7; loss: 0.76798; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 8; loss: 0.82209; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 9; loss: 0.66725; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 10; loss: 0.69706; train_acc: 0.34815; val_acc: 0.26667; test: 0.26\n",
      "epoch 11; loss: 0.69252; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 12; loss: 0.68553; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 13; loss: 0.71543; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 14; loss: 0.74481; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 15; loss: 0.64773; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 16; loss: 0.7197; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 17; loss: 0.46965; train_acc: 0.68889; val_acc: 0.8; test: 0.84\n",
      "epoch 18; loss: 0.96801; train_acc: 0.65926; val_acc: 0.73333; test: 0.74\n",
      "epoch 19; loss: 0.74668; train_acc: 0.34815; val_acc: 0.26667; test: 0.26\n",
      "epoch 20; loss: 0.65007; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 21; loss: 0.69066; train_acc: 0.34815; val_acc: 0.26667; test: 0.26\n",
      "epoch 22; loss: 0.68936; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 23; loss: 0.62531; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 24; loss: 0.68422; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 25; loss: 0.68289; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 26; loss: 0.68451; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 27; loss: 0.64904; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 28; loss: 0.67738; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 29; loss: 0.66649; train_acc: 0.71111; val_acc: 0.73333; test: 0.68\n",
      "epoch 30; loss: 0.51216; train_acc: 0.74074; val_acc: 0.86667; test: 0.79\n",
      "epoch 31; loss: 0.33743; train_acc: 0.72593; val_acc: 0.86667; test: 0.74\n",
      "epoch 32; loss: 1.21656; train_acc: 0.73333; val_acc: 0.8; test: 0.79\n",
      "epoch 33; loss: 0.44742; train_acc: 0.6963; val_acc: 0.73333; test: 0.74\n",
      "epoch 34; loss: 0.53756; train_acc: 0.73333; val_acc: 0.8; test: 0.84\n",
      "epoch 35; loss: 0.41343; train_acc: 0.65926; val_acc: 0.73333; test: 0.74\n",
      "epoch 36; loss: 0.31628; train_acc: 0.73333; val_acc: 0.86667; test: 0.84\n",
      "epoch 37; loss: 0.36445; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 38; loss: 0.24808; train_acc: 0.73333; val_acc: 0.8; test: 0.84\n",
      "epoch 39; loss: 0.29051; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 40; loss: 0.26664; train_acc: 0.74074; val_acc: 0.86667; test: 0.84\n",
      "epoch 41; loss: 0.28543; train_acc: 0.71852; val_acc: 0.73333; test: 0.68\n",
      "epoch 42; loss: 0.67976; train_acc: 0.73333; val_acc: 0.8; test: 0.84\n",
      "epoch 43; loss: 0.31174; train_acc: 0.66667; val_acc: 0.73333; test: 0.74\n",
      "epoch 44; loss: 0.26027; train_acc: 0.74074; val_acc: 0.86667; test: 0.84\n",
      "epoch 45; loss: 0.27557; train_acc: 0.65926; val_acc: 0.73333; test: 0.74\n",
      "epoch 46; loss: 0.23451; train_acc: 0.74074; val_acc: 0.8; test: 0.84\n",
      "epoch 47; loss: 0.24638; train_acc: 0.66667; val_acc: 0.73333; test: 0.74\n",
      "epoch 48; loss: 0.20918; train_acc: 0.74815; val_acc: 0.86667; test: 0.89\n",
      "epoch 49; loss: 0.2502; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "Accuracy: 0.7368421052631579\n",
      "Fold 4/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 1.87608; train_acc: 0.22222; val_acc: 0.53333; test: 0.21\n",
      "epoch 1; loss: 0.80281; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 2; loss: 0.99372; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 3; loss: 1.07824; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 4; loss: 1.49784; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 5; loss: 0.69335; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 6; loss: 1.36815; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 7; loss: 1.71173; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 8; loss: 0.71062; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 9; loss: 1.04694; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 10; loss: 1.38297; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 11; loss: 0.56456; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 12; loss: 0.60873; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 13; loss: 0.6885; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 14; loss: 0.67996; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 15; loss: 0.62611; train_acc: 0.67407; val_acc: 0.6; test: 0.74\n",
      "epoch 16; loss: 1.47776; train_acc: 0.6963; val_acc: 0.6; test: 0.74\n",
      "epoch 17; loss: 0.73843; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 18; loss: 0.71551; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 19; loss: 0.69552; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 20; loss: 0.68498; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 21; loss: 0.68322; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 22; loss: 0.6882; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 23; loss: 0.69541; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 24; loss: 0.70148; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 25; loss: 0.7064; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 26; loss: 0.7106; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 27; loss: 0.71429; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 28; loss: 0.69697; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 29; loss: 0.69891; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 30; loss: 0.70429; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 31; loss: 0.70878; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 32; loss: 0.71267; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 33; loss: 0.69603; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 34; loss: 0.69808; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 35; loss: 0.70362; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 36; loss: 0.7082; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 37; loss: 0.71217; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 38; loss: 0.7157; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 39; loss: 0.69781; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 40; loss: 0.69966; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 41; loss: 0.70491; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 42; loss: 0.7093; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 43; loss: 0.71314; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 44; loss: 0.6963; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 45; loss: 0.69832; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 46; loss: 0.70381; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 47; loss: 0.70836; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 48; loss: 0.71231; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 49; loss: 0.71583; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "Accuracy: 0.7368421052631579\n",
      "Fold 5/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 2.5581; train_acc: 0.36296; val_acc: 0.13333; test: 0.26\n",
      "epoch 1; loss: 0.9771; train_acc: 0.64444; val_acc: 0.86667; test: 0.74\n",
      "epoch 2; loss: 1.02633; train_acc: 0.63704; val_acc: 0.86667; test: 0.74\n",
      "epoch 3; loss: 1.21321; train_acc: 0.63704; val_acc: 0.86667; test: 0.74\n",
      "epoch 4; loss: 0.68671; train_acc: 0.73333; val_acc: 0.8; test: 0.74\n",
      "epoch 5; loss: 0.72689; train_acc: 0.36296; val_acc: 0.13333; test: 0.26\n",
      "epoch 6; loss: 0.69135; train_acc: 0.36296; val_acc: 0.13333; test: 0.26\n",
      "epoch 7; loss: 0.77524; train_acc: 0.63704; val_acc: 0.86667; test: 0.74\n",
      "epoch 8; loss: 0.82391; train_acc: 0.77037; val_acc: 0.8; test: 0.68\n",
      "epoch 9; loss: 1.12949; train_acc: 0.63704; val_acc: 0.86667; test: 0.74\n",
      "epoch 10; loss: 0.67357; train_acc: 0.65185; val_acc: 0.86667; test: 0.74\n",
      "epoch 11; loss: 0.68095; train_acc: 0.36296; val_acc: 0.13333; test: 0.26\n",
      "epoch 12; loss: 0.69362; train_acc: 0.63704; val_acc: 0.86667; test: 0.74\n",
      "epoch 13; loss: 0.88733; train_acc: 0.75556; val_acc: 0.66667; test: 0.79\n",
      "epoch 14; loss: 1.95141; train_acc: 0.63704; val_acc: 0.86667; test: 0.74\n",
      "epoch 15; loss: 0.67106; train_acc: 0.36296; val_acc: 0.13333; test: 0.26\n",
      "epoch 16; loss: 0.66042; train_acc: 0.65926; val_acc: 0.86667; test: 0.74\n",
      "epoch 17; loss: 0.65967; train_acc: 0.62963; val_acc: 0.26667; test: 0.47\n",
      "epoch 18; loss: 1.07949; train_acc: 0.71111; val_acc: 0.86667; test: 0.68\n",
      "epoch 19; loss: 0.66364; train_acc: 0.36296; val_acc: 0.13333; test: 0.26\n",
      "epoch 20; loss: 0.56403; train_acc: 0.68148; val_acc: 0.86667; test: 0.74\n",
      "epoch 21; loss: 0.57959; train_acc: 0.36296; val_acc: 0.13333; test: 0.26\n",
      "epoch 22; loss: 0.49207; train_acc: 0.6963; val_acc: 0.86667; test: 0.74\n",
      "epoch 23; loss: 0.48976; train_acc: 0.48148; val_acc: 0.2; test: 0.26\n",
      "epoch 24; loss: 0.48088; train_acc: 0.71111; val_acc: 0.86667; test: 0.68\n",
      "epoch 25; loss: 0.49955; train_acc: 0.54815; val_acc: 0.26667; test: 0.32\n",
      "epoch 26; loss: 0.4683; train_acc: 0.6963; val_acc: 0.86667; test: 0.68\n",
      "epoch 27; loss: 0.43042; train_acc: 0.48889; val_acc: 0.2; test: 0.26\n",
      "epoch 28; loss: 0.40477; train_acc: 0.7037; val_acc: 0.86667; test: 0.68\n",
      "epoch 29; loss: 0.37137; train_acc: 0.48148; val_acc: 0.2; test: 0.26\n",
      "epoch 30; loss: 0.34255; train_acc: 0.7037; val_acc: 0.86667; test: 0.68\n",
      "epoch 31; loss: 0.30946; train_acc: 0.6963; val_acc: 0.46667; test: 0.79\n",
      "epoch 32; loss: 0.42364; train_acc: 0.7037; val_acc: 0.86667; test: 0.68\n",
      "epoch 33; loss: 0.29969; train_acc: 0.48889; val_acc: 0.2; test: 0.26\n",
      "epoch 34; loss: 0.29388; train_acc: 0.6963; val_acc: 0.86667; test: 0.74\n",
      "epoch 35; loss: 0.22682; train_acc: 0.38519; val_acc: 0.13333; test: 0.26\n",
      "epoch 36; loss: 0.23815; train_acc: 0.71111; val_acc: 0.8; test: 0.68\n",
      "epoch 37; loss: 0.22296; train_acc: 0.65926; val_acc: 0.4; test: 0.58\n",
      "epoch 38; loss: 0.34542; train_acc: 0.7037; val_acc: 0.8; test: 0.68\n",
      "epoch 39; loss: 0.20396; train_acc: 0.38519; val_acc: 0.13333; test: 0.26\n",
      "epoch 40; loss: 0.20634; train_acc: 0.71111; val_acc: 0.8; test: 0.74\n",
      "epoch 41; loss: 0.21047; train_acc: 0.72593; val_acc: 0.53333; test: 0.79\n",
      "epoch 42; loss: 0.44404; train_acc: 0.71111; val_acc: 0.8; test: 0.68\n",
      "epoch 43; loss: 0.23818; train_acc: 0.48148; val_acc: 0.2; test: 0.26\n",
      "epoch 44; loss: 0.22337; train_acc: 0.71111; val_acc: 0.8; test: 0.74\n",
      "epoch 45; loss: 0.19146; train_acc: 0.67407; val_acc: 0.4; test: 0.58\n",
      "epoch 46; loss: 0.30175; train_acc: 0.7037; val_acc: 0.8; test: 0.68\n",
      "epoch 47; loss: 0.18997; train_acc: 0.48889; val_acc: 0.2; test: 0.26\n",
      "epoch 48; loss: 0.21146; train_acc: 0.72593; val_acc: 0.8; test: 0.74\n",
      "epoch 49; loss: 0.17297; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "Accuracy: 0.7368421052631579\n",
      "Fold 6/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 1.83479; train_acc: 0.32593; val_acc: 0.46667; test: 0.26\n",
      "epoch 1; loss: 0.83823; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 2; loss: 1.0367; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 3; loss: 1.34308; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 4; loss: 1.87015; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 5; loss: 0.73871; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 6; loss: 1.38863; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 7; loss: 1.70341; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 8; loss: 0.66415; train_acc: 0.68148; val_acc: 0.53333; test: 0.74\n",
      "epoch 9; loss: 0.78826; train_acc: 0.32593; val_acc: 0.46667; test: 0.26\n",
      "epoch 10; loss: 0.65624; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 11; loss: 0.90256; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 12; loss: 0.48604; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 13; loss: 0.79449; train_acc: 0.32593; val_acc: 0.46667; test: 0.26\n",
      "epoch 14; loss: 0.61446; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 15; loss: 0.65342; train_acc: 0.32593; val_acc: 0.46667; test: 0.26\n",
      "epoch 16; loss: 0.57756; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 17; loss: 0.72131; train_acc: 0.32593; val_acc: 0.46667; test: 0.26\n",
      "epoch 18; loss: 0.55586; train_acc: 0.71852; val_acc: 0.6; test: 0.74\n",
      "epoch 19; loss: 2.41284; train_acc: 0.65926; val_acc: 0.8; test: 0.79\n",
      "epoch 20; loss: 0.56509; train_acc: 0.67407; val_acc: 0.53333; test: 0.74\n",
      "epoch 21; loss: 0.50262; train_acc: 0.34074; val_acc: 0.46667; test: 0.26\n",
      "epoch 22; loss: 0.48937; train_acc: 0.68148; val_acc: 0.53333; test: 0.74\n",
      "epoch 23; loss: 0.44173; train_acc: 0.34074; val_acc: 0.46667; test: 0.26\n",
      "epoch 24; loss: 0.43073; train_acc: 0.68148; val_acc: 0.53333; test: 0.74\n",
      "epoch 25; loss: 0.40166; train_acc: 0.65185; val_acc: 0.8; test: 0.79\n",
      "epoch 26; loss: 0.39717; train_acc: 0.6963; val_acc: 0.53333; test: 0.74\n",
      "epoch 27; loss: 0.3325; train_acc: 0.7037; val_acc: 0.8; test: 0.79\n",
      "epoch 28; loss: 0.38128; train_acc: 0.7037; val_acc: 0.6; test: 0.74\n",
      "epoch 29; loss: 0.36245; train_acc: 0.72593; val_acc: 0.8; test: 0.74\n",
      "epoch 30; loss: 0.38931; train_acc: 0.71852; val_acc: 0.6; test: 0.74\n",
      "epoch 31; loss: 0.6775; train_acc: 0.74815; val_acc: 0.8; test: 0.79\n",
      "epoch 32; loss: 0.36924; train_acc: 0.71852; val_acc: 0.6; test: 0.74\n",
      "epoch 33; loss: 0.97685; train_acc: 0.76296; val_acc: 0.8; test: 0.74\n",
      "epoch 34; loss: 0.37931; train_acc: 0.71852; val_acc: 0.6; test: 0.74\n",
      "epoch 35; loss: 0.65363; train_acc: 0.75556; val_acc: 0.8; test: 0.79\n",
      "epoch 36; loss: 0.2672; train_acc: 0.7037; val_acc: 0.53333; test: 0.74\n",
      "epoch 37; loss: 0.38991; train_acc: 0.76296; val_acc: 0.8; test: 0.79\n",
      "epoch 38; loss: 0.35876; train_acc: 0.72593; val_acc: 0.66667; test: 0.74\n",
      "epoch 39; loss: 1.05343; train_acc: 0.75556; val_acc: 0.8; test: 0.74\n",
      "epoch 40; loss: 0.36409; train_acc: 0.71852; val_acc: 0.53333; test: 0.74\n",
      "epoch 41; loss: 0.34474; train_acc: 0.74815; val_acc: 0.8; test: 0.68\n",
      "epoch 42; loss: 0.32263; train_acc: 0.73333; val_acc: 0.6; test: 0.74\n",
      "epoch 43; loss: 0.98481; train_acc: 0.77037; val_acc: 0.8; test: 0.79\n",
      "epoch 44; loss: 0.32192; train_acc: 0.72593; val_acc: 0.6; test: 0.74\n",
      "epoch 45; loss: 0.54215; train_acc: 0.75556; val_acc: 0.8; test: 0.74\n",
      "epoch 46; loss: 0.31557; train_acc: 0.72593; val_acc: 0.6; test: 0.74\n",
      "epoch 47; loss: 0.35118; train_acc: 0.77037; val_acc: 0.8; test: 0.79\n",
      "epoch 48; loss: 0.29485; train_acc: 0.73333; val_acc: 0.6; test: 0.74\n",
      "epoch 49; loss: 0.80682; train_acc: 0.76296; val_acc: 0.8; test: 0.79\n",
      "Accuracy: 0.7894736842105263\n",
      "Fold 7/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 2.55991; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 1; loss: 1.01227; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 2; loss: 1.07166; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 3; loss: 1.33858; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 4; loss: 0.7512; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 5; loss: 0.68104; train_acc: 0.77037; val_acc: 0.66667; test: 0.68\n",
      "epoch 6; loss: 2.20086; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 7; loss: 0.67121; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 8; loss: 0.67915; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 9; loss: 0.71363; train_acc: 0.68889; val_acc: 0.6; test: 0.74\n",
      "epoch 10; loss: 0.86739; train_acc: 0.76296; val_acc: 0.66667; test: 0.74\n",
      "epoch 11; loss: 1.11744; train_acc: 0.7037; val_acc: 0.6; test: 0.74\n",
      "epoch 12; loss: 1.16062; train_acc: 0.78519; val_acc: 0.6; test: 0.74\n",
      "epoch 13; loss: 1.86078; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 14; loss: 0.68738; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 15; loss: 0.60725; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 16; loss: 0.64909; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 17; loss: 0.57125; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 18; loss: 0.6627; train_acc: 0.68148; val_acc: 0.6; test: 0.74\n",
      "epoch 19; loss: 0.72804; train_acc: 0.76296; val_acc: 0.86667; test: 0.79\n",
      "epoch 20; loss: 1.19694; train_acc: 0.7037; val_acc: 0.6; test: 0.74\n",
      "epoch 21; loss: 0.86927; train_acc: 0.77037; val_acc: 0.6; test: 0.74\n",
      "epoch 22; loss: 1.19136; train_acc: 0.78519; val_acc: 0.6; test: 0.74\n",
      "epoch 23; loss: 2.04216; train_acc: 0.76296; val_acc: 0.86667; test: 0.74\n",
      "epoch 24; loss: 0.86971; train_acc: 0.71852; val_acc: 0.6; test: 0.74\n",
      "epoch 25; loss: 0.84634; train_acc: 0.6963; val_acc: 0.6; test: 0.74\n",
      "epoch 26; loss: 1.60141; train_acc: 0.75556; val_acc: 0.86667; test: 0.79\n",
      "epoch 27; loss: 1.08087; train_acc: 0.71111; val_acc: 0.6; test: 0.74\n",
      "epoch 28; loss: 1.10707; train_acc: 0.77778; val_acc: 0.6; test: 0.74\n",
      "epoch 29; loss: 1.81115; train_acc: 0.77037; val_acc: 0.86667; test: 0.74\n",
      "epoch 30; loss: 0.89708; train_acc: 0.78519; val_acc: 0.6; test: 0.74\n",
      "epoch 31; loss: 1.36311; train_acc: 0.75556; val_acc: 0.66667; test: 0.68\n",
      "epoch 32; loss: 2.16214; train_acc: 0.65185; val_acc: 0.66667; test: 0.74\n",
      "epoch 33; loss: 0.80279; train_acc: 0.68148; val_acc: 0.6; test: 0.74\n",
      "epoch 34; loss: 0.59875; train_acc: 0.79259; val_acc: 0.6; test: 0.74\n",
      "epoch 35; loss: 1.07268; train_acc: 0.75556; val_acc: 0.6; test: 0.68\n",
      "epoch 36; loss: 0.57223; train_acc: 0.77778; val_acc: 0.6; test: 0.74\n",
      "epoch 37; loss: 0.99571; train_acc: 0.74074; val_acc: 0.73333; test: 0.68\n",
      "epoch 38; loss: 0.55509; train_acc: 0.74815; val_acc: 0.73333; test: 0.74\n",
      "epoch 39; loss: 0.6591; train_acc: 0.75556; val_acc: 0.86667; test: 0.74\n",
      "epoch 40; loss: 0.85136; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 41; loss: 0.59508; train_acc: 0.77037; val_acc: 0.6; test: 0.68\n",
      "epoch 42; loss: 1.12402; train_acc: 0.74815; val_acc: 0.6; test: 0.74\n",
      "epoch 43; loss: 1.78531; train_acc: 0.76296; val_acc: 0.86667; test: 0.74\n",
      "epoch 44; loss: 1.33052; train_acc: 0.75556; val_acc: 0.86667; test: 0.74\n",
      "epoch 45; loss: 2.57768; train_acc: 0.77778; val_acc: 0.6; test: 0.74\n",
      "epoch 46; loss: 1.92391; train_acc: 0.76296; val_acc: 0.86667; test: 0.74\n",
      "epoch 47; loss: 0.81925; train_acc: 0.78519; val_acc: 0.6; test: 0.74\n",
      "epoch 48; loss: 1.12101; train_acc: 0.79259; val_acc: 0.6; test: 0.74\n",
      "epoch 49; loss: 1.84245; train_acc: 0.76296; val_acc: 0.86667; test: 0.74\n",
      "Accuracy: 0.7368421052631579\n",
      "Fold 8/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 2.43578; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 1; loss: 0.86754; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 2; loss: 1.00077; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 3; loss: 1.19885; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 4; loss: 0.70148; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 5; loss: 0.95086; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 6; loss: 1.39904; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 7; loss: 1.50926; train_acc: 0.53333; val_acc: 0.86667; test: 0.47\n",
      "epoch 8; loss: 2.27051; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 9; loss: 0.67225; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 10; loss: 0.67247; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 11; loss: 0.71242; train_acc: 0.66667; val_acc: 0.6; test: 0.74\n",
      "epoch 12; loss: 1.03879; train_acc: 0.7037; val_acc: 0.66667; test: 0.74\n",
      "epoch 13; loss: 0.71281; train_acc: 0.6963; val_acc: 0.66667; test: 0.74\n",
      "epoch 14; loss: 1.10851; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 15; loss: 0.64182; train_acc: 0.67407; val_acc: 0.66667; test: 0.74\n",
      "epoch 16; loss: 0.58157; train_acc: 0.33333; val_acc: 0.4; test: 0.26\n",
      "epoch 17; loss: 0.60978; train_acc: 0.68148; val_acc: 0.66667; test: 0.74\n",
      "epoch 18; loss: 0.575; train_acc: 0.63704; val_acc: 0.73333; test: 0.74\n",
      "epoch 19; loss: 1.41268; train_acc: 0.7037; val_acc: 0.66667; test: 0.74\n",
      "epoch 20; loss: 0.58665; train_acc: 0.74074; val_acc: 0.73333; test: 0.68\n",
      "epoch 21; loss: 0.52024; train_acc: 0.77037; val_acc: 0.73333; test: 0.79\n",
      "epoch 22; loss: 1.03335; train_acc: 0.74815; val_acc: 0.8; test: 0.74\n",
      "epoch 23; loss: 0.83845; train_acc: 0.74074; val_acc: 0.73333; test: 0.68\n",
      "epoch 24; loss: 1.16916; train_acc: 0.76296; val_acc: 0.8; test: 0.74\n",
      "epoch 25; loss: 2.00331; train_acc: 0.77037; val_acc: 0.73333; test: 0.79\n",
      "epoch 26; loss: 0.81126; train_acc: 0.77037; val_acc: 0.8; test: 0.74\n",
      "epoch 27; loss: 1.19351; train_acc: 0.74074; val_acc: 0.73333; test: 0.68\n",
      "epoch 28; loss: 1.83658; train_acc: 0.77037; val_acc: 0.73333; test: 0.79\n",
      "epoch 29; loss: 0.67244; train_acc: 0.7037; val_acc: 0.73333; test: 0.68\n",
      "epoch 30; loss: 0.59147; train_acc: 0.75556; val_acc: 0.8; test: 0.68\n",
      "epoch 31; loss: 0.83243; train_acc: 0.75556; val_acc: 0.8; test: 0.68\n",
      "epoch 32; loss: 1.43941; train_acc: 0.76296; val_acc: 0.8; test: 0.74\n",
      "epoch 33; loss: 0.75937; train_acc: 0.73333; val_acc: 0.73333; test: 0.74\n",
      "epoch 34; loss: 1.49085; train_acc: 0.74815; val_acc: 0.73333; test: 0.68\n",
      "epoch 35; loss: 2.02575; train_acc: 0.77037; val_acc: 0.73333; test: 0.79\n",
      "epoch 36; loss: 0.75002; train_acc: 0.68148; val_acc: 0.6; test: 0.74\n",
      "epoch 37; loss: 0.52591; train_acc: 0.76296; val_acc: 0.8; test: 0.74\n",
      "epoch 38; loss: 0.8136; train_acc: 0.76296; val_acc: 0.8; test: 0.68\n",
      "epoch 39; loss: 1.32169; train_acc: 0.77778; val_acc: 0.8; test: 0.79\n",
      "epoch 40; loss: 0.72469; train_acc: 0.77037; val_acc: 0.8; test: 0.74\n",
      "epoch 41; loss: 1.22632; train_acc: 0.77778; val_acc: 0.8; test: 0.74\n",
      "epoch 42; loss: 0.60387; train_acc: 0.77778; val_acc: 0.8; test: 0.79\n",
      "epoch 43; loss: 0.76447; train_acc: 0.77037; val_acc: 0.8; test: 0.74\n",
      "epoch 44; loss: 0.96199; train_acc: 0.77778; val_acc: 0.8; test: 0.74\n",
      "epoch 45; loss: 0.58359; train_acc: 0.7037; val_acc: 0.66667; test: 0.74\n",
      "epoch 46; loss: 0.71322; train_acc: 0.76296; val_acc: 0.8; test: 0.74\n",
      "epoch 47; loss: 1.38079; train_acc: 0.74815; val_acc: 0.73333; test: 0.68\n",
      "epoch 48; loss: 2.05484; train_acc: 0.76296; val_acc: 0.73333; test: 0.79\n",
      "epoch 49; loss: 0.72175; train_acc: 0.71852; val_acc: 0.8; test: 0.68\n",
      "Accuracy: 0.6842105263157895\n",
      "Fold 9/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 1.87446; train_acc: 0.34815; val_acc: 0.26667; test: 0.26\n",
      "epoch 1; loss: 0.79311; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 2; loss: 0.81852; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 3; loss: 0.88562; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 4; loss: 1.01629; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 5; loss: 0.69149; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 6; loss: 1.14419; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 7; loss: 1.14504; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 8; loss: 1.43946; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 9; loss: 0.68907; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 10; loss: 0.67885; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 11; loss: 0.92218; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 12; loss: 1.02833; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 13; loss: 1.48663; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 14; loss: 0.68681; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 15; loss: 0.68319; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 16; loss: 0.75603; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 17; loss: 0.72286; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 18; loss: 0.77656; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 19; loss: 0.77269; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 20; loss: 0.87958; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 21; loss: 0.66704; train_acc: 0.65185; val_acc: 0.8; test: 0.74\n",
      "epoch 22; loss: 0.80773; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 23; loss: 0.66209; train_acc: 0.65185; val_acc: 0.8; test: 0.74\n",
      "epoch 24; loss: 0.72023; train_acc: 0.36296; val_acc: 0.26667; test: 0.21\n",
      "epoch 25; loss: 0.66896; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 26; loss: 0.70978; train_acc: 0.36296; val_acc: 0.26667; test: 0.21\n",
      "epoch 27; loss: 0.6959; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 28; loss: 0.69787; train_acc: 0.36296; val_acc: 0.26667; test: 0.21\n",
      "epoch 29; loss: 0.58462; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 30; loss: 0.65149; train_acc: 0.36296; val_acc: 0.26667; test: 0.21\n",
      "epoch 31; loss: 0.6888; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 32; loss: 0.70533; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 33; loss: 0.6839; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 34; loss: 0.6871; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 35; loss: 1.10574; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 36; loss: 0.62578; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 37; loss: 0.55255; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 38; loss: 0.692; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 39; loss: 0.70048; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 40; loss: 0.82783; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 41; loss: 0.89184; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 42; loss: 0.68144; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 43; loss: 0.71122; train_acc: 0.36296; val_acc: 0.26667; test: 0.21\n",
      "epoch 44; loss: 0.68128; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 45; loss: 1.67243; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 46; loss: 0.68076; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 47; loss: 0.68288; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 48; loss: 1.33532; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "epoch 49; loss: 0.68422; train_acc: 0.65185; val_acc: 0.73333; test: 0.74\n",
      "Accuracy: 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "train_dataset\n",
    "test_dataset\n",
    "k = 10\n",
    "\n",
    "splits = KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "k_counter = 0\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "    # print('Fold {}'.format(fold + 1))\n",
    "    # print(f'Fold',fold,'Train_idx',train_idx,'Val_idx',val_idx)\n",
    "    print(f'Fold {fold}/{k}')\n",
    "    #if k_counter > 2:\n",
    "    #    break\n",
    "    \n",
    "    fold_train = []\n",
    "    for key in train_idx:\n",
    "        fold_train.append(dataset[key])\n",
    "\n",
    "    fold_val = [] \n",
    "    for key in val_idx:\n",
    "        fold_val.append(dataset[key])\n",
    "\n",
    "    tr = DataLoader(fold_train, batch_size=batch_size, shuffle=False)\n",
    "    vd = DataLoader(fold_val, batch_size=batch_size, shuffle=False)\n",
    "    ts = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Base model\n",
    "    print(\"=== Base model ===\")\n",
    "    baseTrain(tr, vd, ts, 50)\n",
    "    # print(\"=== Experiment model ===\")\n",
    "    # expTrain(tr, vd, ts, 10)\n",
    "    \n",
    "    k_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
