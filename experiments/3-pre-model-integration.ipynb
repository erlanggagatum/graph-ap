{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from preprocessing import data_transformation\n",
    "from similarity import calculate_similarity_matrix\n",
    "\n",
    "from model import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root=\"/\", name=\"MUTAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split: Train test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```train_dataset```: for training model<br/>\n",
    "```val_dataset```: evaluate model for hyperparameter tunning<br/>\n",
    "```test_dataset```: testing model after complete training<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ts, vl = 0.8, 0.1, 0.1\n",
    "dslen = len(dataset)\n",
    "tri = round(tr*dslen)\n",
    "tsi = round((tr+ts)*dslen)\n",
    "train_dataset = dataset[:tri]\n",
    "test_dataset = dataset[tri:tsi]\n",
    "val_dataset = dataset[tsi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)\n",
    "test_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)\n",
    "val_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0])\n",
      "val loader\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0])\n",
      "test loader\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print('train loader')\n",
    "for data in train_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('val loader')\n",
    "for data in val_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('test loader')\n",
    "for data in test_loader:\n",
    "    print(data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv3(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base(\n",
       "  (conv1): GCNConv(7, 64)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (conv3): GCNConv(64, 64)\n",
       "  (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = Base(dataset, 64)\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7054, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_base(model, loader, experiment_mode=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(h, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss\n",
    "    #     print(h[0])\n",
    "    # print(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_base(model, loader, experiment_mode=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        pred = h.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct/len(loader.dataset)\n",
    "\n",
    "base = Base(dataset, 64)\n",
    "train_base(base, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; loss: 0.69; train_acc: 0.34; test_acc: 0.37\n",
      "epoch 1; loss: 0.69; train_acc: 0.34; test_acc: 0.37\n",
      "epoch 2; loss: 0.69; train_acc: 0.34; test_acc: 0.37\n",
      "epoch 3; loss: 0.69; train_acc: 0.51; test_acc: 0.42\n",
      "epoch 4; loss: 0.69; train_acc: 0.7; test_acc: 0.53\n",
      "epoch 5; loss: 0.69; train_acc: 0.74; test_acc: 0.63\n",
      "epoch 6; loss: 0.69; train_acc: 0.67; test_acc: 0.63\n",
      "epoch 7; loss: 0.69; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 8; loss: 0.69; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 9; loss: 0.69; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 10; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 11; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 12; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 13; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 14; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 15; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 16; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 17; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 18; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 19; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 20; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 21; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 22; loss: 0.7; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 23; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 24; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 25; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 26; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 27; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 28; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 29; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 30; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 31; loss: 0.71; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 32; loss: 0.72; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 33; loss: 0.72; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 34; loss: 0.72; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 35; loss: 0.72; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 36; loss: 0.72; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 37; loss: 0.72; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 38; loss: 0.72; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 39; loss: 0.72; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 40; loss: 0.73; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 41; loss: 0.73; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 42; loss: 0.73; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 43; loss: 0.73; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 44; loss: 0.73; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 45; loss: 0.73; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 46; loss: 0.73; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 47; loss: 0.74; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 48; loss: 0.74; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 49; loss: 0.74; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 50; loss: 0.74; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 51; loss: 0.74; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 52; loss: 0.74; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 53; loss: 0.75; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 54; loss: 0.75; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 55; loss: 0.75; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 56; loss: 0.75; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 57; loss: 0.75; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 58; loss: 0.75; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 59; loss: 0.76; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 60; loss: 0.76; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 61; loss: 0.76; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 62; loss: 0.76; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 63; loss: 0.76; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 64; loss: 0.77; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 65; loss: 0.77; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 66; loss: 0.77; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 67; loss: 0.77; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 68; loss: 0.78; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 69; loss: 0.78; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 70; loss: 0.78; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 71; loss: 0.78; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 72; loss: 0.79; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 73; loss: 0.79; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 74; loss: 0.79; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 75; loss: 0.79; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 76; loss: 0.79; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 77; loss: 0.8; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 78; loss: 0.8; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 79; loss: 0.8; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 80; loss: 0.8; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 81; loss: 0.81; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 82; loss: 0.81; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 83; loss: 0.81; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 84; loss: 0.81; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 85; loss: 0.82; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 86; loss: 0.82; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 87; loss: 0.82; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 88; loss: 0.82; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 89; loss: 0.83; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 90; loss: 0.83; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 91; loss: 0.83; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 92; loss: 0.83; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 93; loss: 0.84; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 94; loss: 0.84; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 95; loss: 0.84; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 96; loss: 0.84; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 97; loss: 0.84; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 98; loss: 0.85; train_acc: 0.66; test_acc: 0.63\n",
      "epoch 99; loss: 0.85; train_acc: 0.66; test_acc: 0.63\n",
      "Accuracy: 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "base = Base(dataset, 64)\n",
    "train_base(base, train_loader)\n",
    "\n",
    "# Train\n",
    "for _ in range(epoch):\n",
    "    loss = round(train_base(base, train_loader).item(), 2)\n",
    "    train_acc = round(test_base(base, train_loader), 2)\n",
    "    val_acc = round(test_base(base, val_loader), 2)\n",
    "    \n",
    "    print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; test_acc: {val_acc}')\n",
    "\n",
    "# Test\n",
    "test = test_base(base, test_loader)\n",
    "print(f'Accuracy: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        G = data_transformation(edge_index, feature_emb)\n",
    "        S = calculate_similarity_matrix(G)\n",
    "        # clustering = AffinityPropagation(affinity='precomputed', random_state=123, max_iter=200).fit(S)\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h\n",
    "\n",
    "    def data_transformation():\n",
    "        print('s')\n",
    "        \n",
    "\n",
    "\n",
    "experiment = Experiment(dataset, 64)\n",
    "# train_base(experiment, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 2590], x=[1168, 7], edge_attr=[2590, 4], y=[64], batch=[1168], ptr=[65])\n",
      "tensor([ 0,  0,  0,  ..., 63, 63, 63])\n",
      "edge_index tensor([[   0,    0,    1,  ..., 1165, 1166, 1167],\n",
      "        [   1,    5,    0,  ..., 1167, 1165, 1165]])\n",
      "batch tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.]])\n",
      "ptr tensor([   0,   17,   30,   43,   62,   73,  101,  117,  137,  149,  166,  183,\n",
      "         203,  225,  238,  257,  279,  290,  307,  320,  338,  356,  373,  396,\n",
      "         423,  440,  453,  476,  493,  516,  539,  561,  585,  608,  621,  638,\n",
      "         652,  669,  684,  699,  712,  729,  742,  761,  778,  790,  813,  835,\n",
      "         852,  872,  888,  914,  940,  959,  978,  992, 1009, 1030, 1055, 1078,\n",
      "        1097, 1114, 1125, 1148, 1168])\n"
     ]
    }
   ],
   "source": [
    "batch1 = None\n",
    "for batch in train_loader:\n",
    "    batch1 = batch\n",
    "    break\n",
    "print(batch1)\n",
    "print(batch1.batch)\n",
    "print(\"edge_index\", batch1.edge_index)\n",
    "print(\"batch\",batch1.edge_attr)\n",
    "print(\"ptr\",batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1167)\n",
      "tensor(1167)\n",
      "tensor([[ 0,  0,  1,  1,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  6,  6,  7,  7,\n",
      "          8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14,\n",
      "         15, 16],\n",
      "        [ 1,  5,  0,  2,  1,  3,  2,  4,  9,  3,  5,  6,  0,  4,  4,  7,  6,  8,\n",
      "          7,  9, 13,  3,  8, 10,  9, 11, 10, 12, 11, 13, 14,  8, 12, 12, 15, 16,\n",
      "         14, 14]])\n",
      "tensor([   0,   17,   30,   43,   62,   73,  101,  117,  137,  149,  166,  183,\n",
      "         203,  225,  238,  257,  279,  290,  307,  320,  338,  356,  373,  396,\n",
      "         423,  440,  453,  476,  493,  516,  539,  561,  585,  608,  621,  638,\n",
      "         652,  669,  684,  699,  712,  729,  742,  761,  778,  790,  813,  835,\n",
      "         852,  872,  888,  914,  940,  959,  978,  992, 1009, 1030, 1055, 1078,\n",
      "        1097, 1114, 1125, 1148, 1168]) ; len: 65\n"
     ]
    }
   ],
   "source": [
    "print(max(batch1.edge_index[0]))\n",
    "print(max(batch1.edge_index[1]))\n",
    "print((dataset[0].edge_index))\n",
    "print((batch1.ptr), '; len:', len(batch1.ptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 0 - 17\n",
      "1. 17 - 30\n",
      "2. 30 - 43\n",
      "3. 43 - 62\n",
      "4. 62 - 73\n",
      "5. 73 - 101\n",
      "6. 101 - 117\n",
      "7. 117 - 137\n",
      "8. 137 - 149\n",
      "9. 149 - 166\n",
      "10. 166 - 183\n",
      "11. 183 - 203\n",
      "12. 203 - 225\n",
      "13. 225 - 238\n",
      "14. 238 - 257\n",
      "15. 257 - 279\n",
      "16. 279 - 290\n",
      "17. 290 - 307\n",
      "18. 307 - 320\n",
      "19. 320 - 338\n",
      "20. 338 - 356\n",
      "21. 356 - 373\n",
      "22. 373 - 396\n",
      "23. 396 - 423\n",
      "24. 423 - 440\n",
      "25. 440 - 453\n",
      "26. 453 - 476\n",
      "27. 476 - 493\n",
      "28. 493 - 516\n",
      "29. 516 - 539\n",
      "30. 539 - 561\n",
      "31. 561 - 585\n",
      "32. 585 - 608\n",
      "33. 608 - 621\n",
      "34. 621 - 638\n",
      "35. 638 - 652\n",
      "36. 652 - 669\n",
      "37. 669 - 684\n",
      "38. 684 - 699\n",
      "39. 699 - 712\n",
      "40. 712 - 729\n",
      "41. 729 - 742\n",
      "42. 742 - 761\n",
      "43. 761 - 778\n",
      "44. 778 - 790\n",
      "45. 790 - 813\n",
      "46. 813 - 835\n",
      "47. 835 - 852\n",
      "48. 852 - 872\n",
      "49. 872 - 888\n",
      "50. 888 - 914\n",
      "51. 914 - 940\n",
      "52. 940 - 959\n",
      "53. 959 - 978\n",
      "54. 978 - 992\n",
      "55. 992 - 1009\n",
      "56. 1009 - 1030\n",
      "57. 1030 - 1055\n",
      "58. 1055 - 1078\n",
      "59. 1078 - 1097\n",
      "60. 1097 - 1114\n",
      "61. 1114 - 1125\n",
      "62. 1125 - 1148\n",
      "63. 1148 - 1168\n"
     ]
    }
   ],
   "source": [
    "graph_bound = {}\n",
    "\n",
    "for i in range(len(batch1.ptr)-1):\n",
    "    graph_bound[i] = [batch1.ptr[i].item(), batch1.ptr[i+1].item()]\n",
    "    print(str(i)+\".\", batch1.ptr[i].item(), \"-\", batch1.ptr[i+1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below --> Subgraph extractor with batch information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Graph 0 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "[0 0 0 0 1 1 1 3 3 3 3 2 3 3 2 2 2]\n",
      "{0, 1, 2, 3}\n",
      "communities {0: [0, 1, 2, 3], 1: [4, 5, 6], 3: [7, 8, 9, 10, 12, 13], 2: [11, 14, 15, 16]}\n",
      "=== Graph 1 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[2 1 1 1 0 0 1 0 2 2 2 2 2]\n",
      "{0, 1, 2}\n",
      "communities {2: [0, 8, 9, 10, 11, 12], 1: [1, 2, 3, 6], 0: [4, 5, 7]}\n",
      "=== Graph 2 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[1 1 0 0 0 0 0 0 1 1 1 1 1]\n",
      "{0, 1}\n",
      "communities {1: [0, 1, 8, 9, 10, 11, 12], 0: [2, 3, 4, 5, 6, 7]}\n",
      "=== Graph 3 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[1 0 1 0 1 1 2 2 3 3 3 2 2 4 4 3 4 4 4]\n",
      "{0, 1, 2, 3, 4}\n",
      "communities {1: [0, 2, 4, 5], 0: [1, 3], 2: [6, 7, 11, 12], 3: [8, 9, 10, 15], 4: [13, 14, 16, 17, 18]}\n",
      "=== Graph 4 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[0 1 1 2 2 0 0 2 2 2 1]\n",
      "{0, 1, 2}\n",
      "communities {0: [0, 5, 6], 1: [1, 2, 10], 2: [3, 4, 7, 8, 9]}\n",
      "=== Graph 5 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "[4 0 0 0 1 1 0 4 4 2 2 4 3 3 3 3 3 3 3 4 2 2 1 1 1 0 0 0]\n",
      "{0, 1, 2, 3, 4}\n",
      "communities {4: [0, 7, 8, 11, 19], 0: [1, 2, 3, 6, 25, 26, 27], 1: [4, 5, 22, 23, 24], 2: [9, 10, 20, 21], 3: [12, 13, 14, 15, 16, 17, 18]}\n",
      "=== Graph 6 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[1 1 1 1 1 0 1 0 0 0 2 2 2 2 2 2]\n",
      "{0, 1, 2}\n",
      "communities {1: [0, 1, 2, 3, 4, 6], 0: [5, 7, 8, 9], 2: [10, 11, 12, 13, 14, 15]}\n",
      "=== Graph 7 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[0 0 0 0 1 1 1 1 1 1 3 3 3 3 2 3 3 2 2 2]\n",
      "{0, 1, 2, 3}\n",
      "communities {0: [0, 1, 2, 3], 1: [4, 5, 6, 7, 8, 9], 3: [10, 11, 12, 13, 15, 16], 2: [14, 17, 18, 19]}\n",
      "=== Graph 8 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "[0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "{0, 1}\n",
      "communities {0: [0, 1, 2, 3, 4], 1: [5, 6, 7, 8, 9, 10, 11]}\n",
      "=== Graph 9 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "[0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "{0, 1}\n",
      "communities {0: [0, 1, 2, 3, 4, 5, 6], 1: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}\n",
      "=== Graph 10 ===\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "masalah disini bro\n"
     ]
    }
   ],
   "source": [
    "# idx_from = 0\n",
    "# idx_to = 0\n",
    "graph_counter = 0\n",
    "graph_bound\n",
    "edge_index = [[],[]]\n",
    "Gs = []\n",
    "\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "print(f'=== Graph {graph_counter} ===')\n",
    "for i, (src, dst) in enumerate(zip(batch1.edge_index[0], batch1.edge_index[1])):\n",
    "    # if (graph_counter < len(batch1.ptr)):\n",
    "    lower_bound = graph_bound[graph_counter][0]\n",
    "    upper_bound = graph_bound[graph_counter][1]\n",
    "    if ((src >= lower_bound and src < upper_bound) or\n",
    "        (dst >= lower_bound and dst < upper_bound)):\n",
    "        # print(i,src.item()-lower_bound, dst.item()-lower_bound)\n",
    "        edge_index[0].append(src - lower_bound)\n",
    "        edge_index[1].append(dst - lower_bound)\n",
    "    else:\n",
    "        # print(edge_index)\n",
    "        embs = []\n",
    "        # make new graph\n",
    "        for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "            if (b == graph_counter):\n",
    "                # print(i, emb)\n",
    "                embs.append(emb)\n",
    "        \n",
    "        G = data_transformation(edge_index, embs)\n",
    "        Gs.append(G)\n",
    "        print(sorted(list(G.nodes)))\n",
    "        # print('pre', precalc_shortest_path_length)\n",
    "        # for node in sorted(list(G.nodes)):\n",
    "        #     print(G.nodes[node])\n",
    "        \n",
    "        \n",
    "        # testt()\n",
    "        if graph_counter == 10:\n",
    "            print('masalah disini bro')\n",
    "            break\n",
    "        \n",
    "        # Calculate S matrix\n",
    "        S = calculate_similarity_matrix(G)\n",
    "        \n",
    "        # AP Clustering\n",
    "        clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "\n",
    "        print(clustering.labels_)\n",
    "        # print(clustering.)\n",
    "        \n",
    "        communities = {}\n",
    "        print(set(clustering.labels_))\n",
    "        # communities init\n",
    "        for lab in clustering.labels_:\n",
    "            communities[lab] = []\n",
    "        \n",
    "        for nd, clust in enumerate(clustering.labels_):\n",
    "            communities[clust].append(nd)\n",
    "        print(\"communities\", communities) \n",
    "            \n",
    "        edge_index = [[],[]]\n",
    "        graph_counter+=1\n",
    "        \n",
    "        print(f'=== Graph {graph_counter} ===')\n",
    "        \n",
    "    if i == len(batch1.edge_index[0]) - 1:\n",
    "        embs = []\n",
    "        # make new graph\n",
    "        for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "            if (b == graph_counter):\n",
    "                # print(i, emb)\n",
    "                embs.append(emb)\n",
    "        \n",
    "        G = data_transformation(edge_index, embs)\n",
    "        Gs.append(G)\n",
    "        \n",
    "        S = calculate_similarity_matrix(G)\n",
    "        # AP Clustering        \n",
    "        clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "        \n",
    "        print(sorted(list(G.nodes)))\n",
    "        print(clustering.labels_)\n",
    "        \n",
    "        # print(edge_index)\n",
    "        print('udh di akhir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3].x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([0., 0., 1., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([1., 0., 0., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'node_features': array([0., 1., 0., 0., 0., 0., 0.], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "for node in Gs[6].nodes:\n",
    "    print(Gs[0].nodes[node])\n",
    "    # Udah bisa tambah nodes per batch, tinggal masukin ke algo utama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  5,  5,  5,  6,  6,  6,\n",
       "          7,  8,  9,  9,  9, 10, 11, 11, 11, 12, 12, 12, 13, 13, 14, 14, 15, 15,\n",
       "         15, 16, 16, 17, 17, 17, 18, 19],\n",
       "        [ 1,  5,  0,  2,  1,  3, 12,  2,  4,  9,  3,  5,  0,  4,  6,  5,  7,  8,\n",
       "          6,  6,  3, 10, 11,  9,  9, 12, 16,  2, 11, 13, 12, 14, 13, 15, 14, 16,\n",
       "         17, 11, 15, 15, 18, 19, 17, 17]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[63].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "1 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "2 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "3 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "4 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "5 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "6 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "7 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "8 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "9 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "10 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "11 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "12 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "13 tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "14 tensor([0., 1., 0., 0., 0., 0., 0.])\n",
      "15 tensor([0., 0., 1., 0., 0., 0., 0.])\n",
      "16 tensor([0., 0., 1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "    if (b == 0):\n",
    "        print(i, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 720], x=[337, 7], edge_attr=[720, 4], y=[22], batch=[337], ptr=[23])\n",
      "=== Graph 0 ===\n",
      "Graph with 14 nodes and 14 edges\n",
      "edge_idx [[tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(8), tensor(9), tensor(9), tensor(9), tensor(10), tensor(11), tensor(12), tensor(12), tensor(13)], [tensor(1), tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(12), tensor(2), tensor(4), tensor(9), tensor(3), tensor(5), tensor(0), tensor(4), tensor(6), tensor(5), tensor(7), tensor(8), tensor(6), tensor(6), tensor(3), tensor(10), tensor(11), tensor(9), tensor(9), tensor(2), tensor(13), tensor(12)]]\n",
      "13\n",
      "14\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "[0 0 2 1 1 0 0 0 0 1 1 1 2 2]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11]]\n",
      "=== Graph 1 ===\n",
      "Graph with 11 nodes and 11 edges\n",
      "edge_idx [[tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(7), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(10)], [tensor(0), tensor(2), tensor(1), tensor(3), tensor(7), tensor(2), tensor(4), tensor(3), tensor(5), tensor(4), tensor(6), tensor(5), tensor(7), tensor(2), tensor(6), tensor(8), tensor(7), tensor(9), tensor(10), tensor(8), tensor(8)]]\n",
      "10\n",
      "11\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[0 0 0 0 0 1 1 1 1 1 1]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24]]\n",
      "=== Graph 2 ===\n",
      "Graph with 19 nodes and 20 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(11), tensor(12), tensor(12), tensor(13), tensor(13), tensor(13), tensor(14), tensor(14), tensor(15), tensor(15), tensor(16), tensor(16), tensor(16), tensor(17), tensor(18)], [tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(2), tensor(4), tensor(6), tensor(3), tensor(5), tensor(0), tensor(4), tensor(3), tensor(7), tensor(6), tensor(8), tensor(7), tensor(9), tensor(10), tensor(8), tensor(8), tensor(11), tensor(15), tensor(10), tensor(12), tensor(11), tensor(13), tensor(12), tensor(14), tensor(16), tensor(13), tensor(15), tensor(10), tensor(14), tensor(13), tensor(17), tensor(18), tensor(16), tensor(16)]]\n",
      "18\n",
      "19\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 2 2 2 2]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43]]\n",
      "=== Graph 3 ===\n",
      "Graph with 11 nodes and 11 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(10)], [tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(2), tensor(4), tensor(8), tensor(3), tensor(5), tensor(6), tensor(0), tensor(4), tensor(4), tensor(7), tensor(6), tensor(3), tensor(9), tensor(10), tensor(8), tensor(8)]]\n",
      "10\n",
      "11\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[1 1 1 1 0 0 0 0 1 1 1]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51]]\n",
      "=== Graph 4 ===\n",
      "Graph with 21 nodes and 23 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(7), tensor(8), tensor(8), tensor(9), tensor(9), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(11), tensor(12), tensor(12), tensor(12), tensor(13), tensor(13), tensor(13), tensor(14), tensor(14), tensor(14), tensor(15), tensor(16), tensor(16), tensor(16), tensor(17), tensor(18), tensor(19), tensor(20), tensor(20)], [tensor(5), tensor(0), tensor(2), tensor(20), tensor(1), tensor(3), tensor(8), tensor(2), tensor(4), tensor(3), tensor(5), tensor(7), tensor(0), tensor(4), tensor(6), tensor(5), tensor(4), tensor(2), tensor(9), tensor(8), tensor(10), tensor(14), tensor(9), tensor(11), tensor(20), tensor(10), tensor(12), tensor(11), tensor(13), tensor(19), tensor(12), tensor(14), tensor(16), tensor(9), tensor(13), tensor(15), tensor(14), tensor(13), tensor(17), tensor(18), tensor(16), tensor(16), tensor(12), tensor(1), tensor(10)]]\n",
      "20\n",
      "21\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[0 2 1 1 1 0 0 1 1 1 2 2 2 4 3 3 4 4 4 2 0]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70]]\n",
      "=== Graph 5 ===\n",
      "Graph with 22 nodes and 25 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(9), tensor(9), tensor(10), tensor(10), tensor(11), tensor(11), tensor(11), tensor(12), tensor(12), tensor(13), tensor(13), tensor(13), tensor(14), tensor(14), tensor(15), tensor(15), tensor(16), tensor(16), tensor(16), tensor(17), tensor(18), tensor(19), tensor(19), tensor(19), tensor(20), tensor(21)], [tensor(9), tensor(0), tensor(2), tensor(1), tensor(3), tensor(7), tensor(2), tensor(4), tensor(3), tensor(5), tensor(19), tensor(4), tensor(6), tensor(5), tensor(7), tensor(15), tensor(2), tensor(6), tensor(8), tensor(7), tensor(9), tensor(13), tensor(0), tensor(8), tensor(10), tensor(9), tensor(11), tensor(10), tensor(12), tensor(16), tensor(11), tensor(13), tensor(8), tensor(12), tensor(14), tensor(13), tensor(15), tensor(6), tensor(14), tensor(11), tensor(17), tensor(18), tensor(16), tensor(16), tensor(4), tensor(20), tensor(21), tensor(19), tensor(19)]]\n",
      "21\n",
      "22\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[2 3 3 0 0 3 3 3 2 2 2 1 1 2 2 3 2 1 1 3 0 0]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88]]\n",
      "=== Graph 6 ===\n",
      "Graph with 11 nodes and 11 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(8), tensor(9), tensor(10)], [tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(10), tensor(2), tensor(4), tensor(9), tensor(3), tensor(5), tensor(0), tensor(4), tensor(6), tensor(5), tensor(7), tensor(8), tensor(6), tensor(6), tensor(3), tensor(2)]]\n",
      "10\n",
      "11\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[2 0 0 1 1 1 2 2 2 1 0]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87, 106, 104, 99, 100, 107, 101, 102], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88, 104, 105, 100, 108, 101, 102, 103]]\n",
      "=== Graph 7 ===\n",
      "Graph with 20 nodes and 23 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(9), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(11), tensor(11), tensor(12), tensor(12), tensor(12), tensor(13), tensor(13), tensor(14), tensor(14), tensor(15), tensor(15), tensor(16), tensor(17), tensor(17), tensor(17), tensor(18), tensor(19)], [tensor(13), tensor(0), tensor(2), tensor(1), tensor(3), tensor(11), tensor(2), tensor(4), tensor(3), tensor(5), tensor(4), tensor(6), tensor(10), tensor(5), tensor(7), tensor(17), tensor(6), tensor(8), tensor(7), tensor(9), tensor(16), tensor(8), tensor(10), tensor(15), tensor(5), tensor(9), tensor(11), tensor(2), tensor(10), tensor(12), tensor(11), tensor(13), tensor(14), tensor(0), tensor(12), tensor(12), tensor(15), tensor(9), tensor(14), tensor(8), tensor(6), tensor(18), tensor(19), tensor(17), tensor(17)]]\n",
      "19\n",
      "20\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[2 2 0 2 0 0 1 1 1 4 4 2 3 2 3 4 4 1 1 1]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87, 106, 104, 99, 100, 107, 101, 102, 109, 109, 113, 115, 115, 116, 126, 126, 118, 118, 121], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88, 104, 105, 100, 108, 101, 102, 103, 122, 110, 114, 116, 126, 117, 127, 128, 119, 124, 123]]\n",
      "=== Graph 8 ===\n",
      "Graph with 22 nodes and 25 edges\n",
      "edge_idx [[tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(5), tensor(5), tensor(6), tensor(6), tensor(6), tensor(7), tensor(7), tensor(8), tensor(8), tensor(9), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(11), tensor(11), tensor(12), tensor(12), tensor(12), tensor(13), tensor(13), tensor(14), tensor(14), tensor(14), tensor(15), tensor(15), tensor(15), tensor(16), tensor(16), tensor(16), tensor(17), tensor(18), tensor(19), tensor(19), tensor(19), tensor(20), tensor(21)], [tensor(5), tensor(0), tensor(2), tensor(1), tensor(3), tensor(2), tensor(4), tensor(12), tensor(3), tensor(5), tensor(6), tensor(0), tensor(4), tensor(4), tensor(7), tensor(11), tensor(6), tensor(8), tensor(7), tensor(9), tensor(8), tensor(10), tensor(9), tensor(11), tensor(15), tensor(6), tensor(10), tensor(12), tensor(3), tensor(11), tensor(13), tensor(12), tensor(14), tensor(13), tensor(15), tensor(19), tensor(10), tensor(14), tensor(16), tensor(15), tensor(17), tensor(18), tensor(16), tensor(16), tensor(14), tensor(20), tensor(21), tensor(19), tensor(19)]]\n",
      "21\n",
      "22\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[0 0 0 0 1 0 1 1 2 2 2 1 0 3 3 2 2 2 2 3 3 3]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87, 106, 104, 99, 100, 107, 101, 102, 109, 109, 113, 115, 115, 116, 126, 126, 118, 118, 121, 129, 129, 130, 131, 132, 140, 133, 135, 137, 138, 139, 144, 145, 145, 142, 143, 148, 148], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88, 104, 105, 100, 108, 101, 102, 103, 122, 110, 114, 116, 126, 117, 127, 128, 119, 124, 123, 134, 130, 131, 132, 141, 135, 135, 136, 138, 139, 144, 145, 146, 147, 143, 148, 149, 150]]\n",
      "=== Graph 9 ===\n",
      "Graph with 13 nodes and 14 edges\n",
      "edge_idx [[tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(5), tensor(5), tensor(5), tensor(6), tensor(6), tensor(7), tensor(7), tensor(8), tensor(8), tensor(8), tensor(9), tensor(9), tensor(10), tensor(10), tensor(10), tensor(11), tensor(12)], [tensor(0), tensor(2), tensor(9), tensor(1), tensor(3), tensor(2), tensor(4), tensor(8), tensor(3), tensor(5), tensor(4), tensor(6), tensor(10), tensor(5), tensor(7), tensor(6), tensor(8), tensor(3), tensor(7), tensor(9), tensor(1), tensor(8), tensor(5), tensor(11), tensor(12), tensor(10), tensor(10)]]\n",
      "12\n",
      "13\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[2 2 0 0 0 1 1 1 2 2 0 0 0]\n",
      "[[0, 0, 5, 6, 6, 2, 12, 3, 3, 9, 9, 14, 15, 16, 17, 21, 21, 19, 22, 22, 25, 25, 26, 27, 28, 29, 33, 33, 31, 35, 35, 36, 37, 38, 38, 39, 41, 41, 44, 45, 46, 47, 52, 52, 48, 48, 50, 55, 60, 65, 66, 67, 57, 57, 58, 59, 63, 71, 71, 71, 69, 76, 84, 84, 85, 89, 77, 78, 81, 82, 82, 79, 87, 106, 104, 99, 100, 107, 101, 102, 109, 109, 113, 115, 115, 116, 126, 126, 118, 118, 121, 129, 129, 130, 131, 132, 140, 133, 135, 137, 138, 139, 144, 145, 145, 142, 143, 148, 148, 151, 152, 159, 153, 154, 161, 161, 156, 157], [1, 5, 6, 7, 8, 12, 13, 4, 9, 10, 11, 15, 16, 17, 18, 20, 22, 20, 23, 24, 30, 26, 27, 28, 29, 30, 32, 34, 32, 36, 40, 37, 38, 39, 41, 40, 42, 43, 45, 46, 47, 52, 53, 54, 49, 50, 51, 60, 61, 66, 67, 74, 58, 63, 59, 62, 64, 68, 72, 73, 70, 85, 85, 89, 86, 90, 78, 83, 82, 83, 91, 80, 88, 104, 105, 100, 108, 101, 102, 103, 122, 110, 114, 116, 126, 117, 127, 128, 119, 124, 123, 134, 130, 131, 132, 141, 135, 135, 136, 138, 139, 144, 145, 146, 147, 143, 148, 149, 150, 152, 160, 160, 154, 155, 162, 163, 157, 158]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# idx_from = 0\n",
    "# idx_to = 0\n",
    "graph_counter = 0\n",
    "graph_bound\n",
    "edge_index = [[],[]]\n",
    "subgraph_edge_index = [[],[]]\n",
    "Gs = []\n",
    "sub_created = False\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "batches = []\n",
    "for b in train_loader:\n",
    "    batches.append(b)\n",
    "batch1 = batches[2]\n",
    "print(batch1)\n",
    "\n",
    "# return 0\n",
    "graph_bound = {}\n",
    "\n",
    "for i in range(len(batch1.ptr)-1):\n",
    "    graph_bound[i] = [batch1.ptr[i].item(), batch1.ptr[i+1].item()]\n",
    "    # print(str(i)+\".\", batch1.ptr[i].item(), \"-\", batch1.ptr[i+1].item())\n",
    "\n",
    "for i, (src, dst) in enumerate(zip(batch1.edge_index[0], batch1.edge_index[1])):\n",
    "    # if (graph_counter < len(batch1.ptr)):\n",
    "    lower_bound = graph_bound[graph_counter][0]\n",
    "    upper_bound = graph_bound[graph_counter][1]\n",
    "    if ((src >= lower_bound and src < upper_bound) or\n",
    "        (dst >= lower_bound and dst < upper_bound)):\n",
    "        # print(i,src.item()-lower_bound, dst.item()-lower_bound)\n",
    "        edge_index[0].append(src - lower_bound)\n",
    "        edge_index[1].append(dst - lower_bound)\n",
    "    else:\n",
    "        sub_created = True\n",
    "        # continue\n",
    "        \n",
    "        \n",
    "        # # print(edge_index)\n",
    "        # embs = []\n",
    "        # # make new graph\n",
    "        # for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "        #     if (b == graph_counter):\n",
    "        #         # print(i, emb)\n",
    "        #         embs.append(emb)\n",
    "        \n",
    "        # G = data_transformation(edge_index, embs)\n",
    "        # Gs.append(G)\n",
    "        # print(sorted(list(G.nodes)))\n",
    "        # # print('pre', precalc_shortest_path_length)\n",
    "        # # for node in sorted(list(G.nodes)):\n",
    "        # #     print(G.nodes[node])\n",
    "        \n",
    "        \n",
    "        # # testt()\n",
    "        # if graph_counter == 10:\n",
    "        #     print('masalah disini bro')\n",
    "        #     break\n",
    "        \n",
    "        # # Calculate S matrix\n",
    "        # S = calculate_similarity_matrix(G)\n",
    "        \n",
    "        # # AP Clustering\n",
    "        # clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "\n",
    "        # print(clustering.labels_)\n",
    "        # # print(clustering.)\n",
    "        \n",
    "        # communities = {}\n",
    "        # print(\"cluster labels:\", set(clustering.labels_))\n",
    "        # # communities init\n",
    "        # for lab in clustering.labels_:\n",
    "        #     communities[lab] = []\n",
    "        \n",
    "        # for nd, clust in enumerate(clustering.labels_):\n",
    "        #     communities[clust].append(nd)\n",
    "        # print(\"communities\", communities) \n",
    "            \n",
    "        # edge_index = [[],[]]\n",
    "        # graph_counter+=1\n",
    "        \n",
    "        # # make subgraph edge_index\n",
    "        # for c in communities:\n",
    "        #     w = G.subgraph(communities[c])\n",
    "        #     # print(\"edges subgraph\", w.edges)\n",
    "        #     for sub in w.edges:\n",
    "        #         # print(sub[0], sub[1])\n",
    "        #         subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "        #         subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                \n",
    "        # print(subgraph_edge_index)\n",
    "        # print(f'=== Graph {graph_counter} ===') \n",
    "        \n",
    "    if (i == len(batch1.edge_index[0]) - 1) or sub_created:\n",
    "        print(f'=== Graph {graph_counter} ===')\n",
    "        \n",
    "        sub_created = False\n",
    "        \n",
    "        embs = []\n",
    "        # make new graph\n",
    "        for i, (b, emb) in enumerate(zip(batch1.batch, batch1.x)):\n",
    "            if (b == graph_counter):\n",
    "                # print(i, emb)\n",
    "                embs.append(emb)\n",
    "        \n",
    "        G = data_transformation(edge_index, embs)\n",
    "        print(G)\n",
    "        print(\"edge_idx\", edge_index)\n",
    "        print(max(G.nodes))\n",
    "        print(len(G.nodes))\n",
    "        \n",
    "        Gs.append(G)\n",
    "        \n",
    "        S = calculate_similarity_matrix(G)\n",
    "        # AP Clustering        \n",
    "        clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "        \n",
    "        print(sorted(list(G.nodes)))\n",
    "        print(clustering.labels_)\n",
    "        \n",
    "        # Modif disini nanti\n",
    "        #########\n",
    "        communities = {}\n",
    "        # print(\"cluster labels:\", set(clustering.labels_))\n",
    "        # communities init\n",
    "        for lab in clustering.labels_:\n",
    "            communities[lab] = []\n",
    "        \n",
    "        for nd, clust in enumerate(clustering.labels_):\n",
    "            communities[clust].append(nd)\n",
    "        # print(\"communities\", communities) \n",
    "            \n",
    "        edge_index = [[],[]]\n",
    "        graph_counter+=1\n",
    "        \n",
    "        # make subgraph edge_index\n",
    "        for c in communities:\n",
    "            w = G.subgraph(communities[c])\n",
    "            # print(\"edges subgraph\", w.edges)\n",
    "            for sub in w.edges:\n",
    "                # print(sub[0], sub[1])\n",
    "                subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "                subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                \n",
    "        print(subgraph_edge_index)\n",
    "        if (graph_counter == 10):\n",
    "            break\n",
    "        # print(f'=== Graph {graph_counter} ===')\n",
    "        # print(edge_index)\n",
    "        # print('udh di akhir')\n",
    "\n",
    "embeddings_used = []\n",
    "nodes_used = set(np.array(subgraph_edge_index).flatten())\n",
    "feat = batch1.x\n",
    "# for i, f in enumerate(feat):\n",
    "#     if(i in nodes_used):\n",
    "#         embeddings_used.append(feat[i].detach().numpy())\n",
    "        \n",
    "# print(len(embeddings_used))\n",
    "# z = torch.zeros(len(embeddings_used), len(embeddings_used[0]))\n",
    "# z = torch.tensor(embeddings_used)\n",
    "\n",
    "# print(torch.tensor(subgraph_edge_index).size())\n",
    "# nodes_used = set(np.array(subgraph_edge_index).flatten())\n",
    "# feat = batch1.x\n",
    "# for i, f in enumerate(feat):\n",
    "#     if(i in nodes_used):\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "\n",
    "\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # embeddings for subgraph\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # self.conv6 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels*2, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, ptr):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv3(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        # G = data_transformation(edge_index, feature_emb)\n",
    "        # S = calculate_similarity_matrix(G)\n",
    "        # clustering = AffinityPropagation(affinity='precomputed', random_state=123, max_iter=200).fit(S)\n",
    "        subgraph_edge_index, _ = self.subgraph_generator(feature_emb, edge_index, batch, ptr)\n",
    "        subgraph_embedding = self.conv4(embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        subgraph_embedding = self.conv5(subgraph_embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        # subgraph_embedding = self.conv1(x, subgraph_edge_index)\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        # subgraph_embedding = self.conv2(subgraph_embedding, subgraph_edge_index)\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        # subgraph_embedding = self.conv3(subgraph_embedding, subgraph_edge_index)\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        # print(subgraph_edge_index)\n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        # self.subgraph_pooling(\"\",\"\",\"\")\n",
    "        subgraph_embedding = global_max_pool(subgraph_embedding, batch)\n",
    "        \n",
    "        \n",
    "        h = torch.cat((embedding, subgraph_embedding), 1)\n",
    "        \n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin(h)\n",
    "        h = h.relu()\n",
    "        x = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h\n",
    "    \n",
    "    def subgraph_pooling(self, embeddings, batch, ptr):\n",
    "        print('subgraph pooling')\n",
    "\n",
    "    def subgraph_generator(self, embeddings, batch_edge_index, batch, ptr):\n",
    "        '''\n",
    "        Return subgraph_edge_index (edge_index of created subgraph)\n",
    "        '''\n",
    "        # print('processing subgraph_generator...')\n",
    "        graph_counter = 0\n",
    "        edge_index = [[],[]]\n",
    "        subgraph_edge_index = [[],[]]\n",
    "        # Gs = []\n",
    "        sub_created = False\n",
    "        graph_bound = {}\n",
    "\n",
    "        for i in range(len(ptr)-1):\n",
    "            graph_bound[i] = [ptr[i].item(), ptr[i+1].item()]\n",
    "        \n",
    "        for i, (src, dst) in enumerate(zip(batch_edge_index[0], batch_edge_index[1])):\n",
    "            lower_bound = graph_bound[graph_counter][0]\n",
    "            upper_bound = graph_bound[graph_counter][1]\n",
    "            if ((src >= lower_bound and src < upper_bound) or\n",
    "                (dst >= lower_bound and dst < upper_bound)):\n",
    "                \n",
    "                edge_index[0].append(src - lower_bound)\n",
    "                edge_index[1].append(dst - lower_bound)\n",
    "            else:\n",
    "                sub_created = True\n",
    "                \n",
    "            if (i == len(batch_edge_index[0]) - 1) or sub_created:\n",
    "                # print(f'=== Graph {graph_counter} ===')\n",
    "                \n",
    "                sub_created = False\n",
    "                \n",
    "                embs = []\n",
    "                # make new graph\n",
    "                for i, (b, emb) in enumerate(zip(batch, embeddings)):\n",
    "                    if (b == graph_counter):\n",
    "                        embs.append(emb)\n",
    "                \n",
    "                G = data_transformation(edge_index, embs)\n",
    "                # dont need this at the moment\n",
    "                # Gs.append(G)\n",
    "                \n",
    "                # Calculate similarity matrix\n",
    "                S = calculate_similarity_matrix(G)\n",
    "                \n",
    "                # AP Clustering        \n",
    "                # clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, max_iter=1000).fit(S)\n",
    "                clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, convergence_iter=5, max_iter=100).fit(S)\n",
    "                \n",
    "                # Get community\n",
    "                communities = {}\n",
    "                for lab in clustering.labels_:\n",
    "                    communities[lab] = []\n",
    "                \n",
    "                for nd, clust in enumerate(clustering.labels_):\n",
    "                    communities[clust].append(nd)\n",
    "                \n",
    "                edge_index = [[],[]]\n",
    "                graph_counter+=1\n",
    "                \n",
    "                # Make subgraph edge_index\n",
    "                for c in communities:\n",
    "                    w = G.subgraph(communities[c])\n",
    "                    for sub in w.edges:\n",
    "                        subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "                        subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                        \n",
    "                # INI LUPA WOY\n",
    "                # if (graph_counter == 10):\n",
    "                #     break\n",
    "                \n",
    "                \n",
    "        # print(\"finished subgraph_generator\")\n",
    "        \n",
    "        # embeddings_used = []\n",
    "        # nodes_used = set(np.array(subgraph_edge_index).flatten())\n",
    "        \n",
    "        # for i, f in enumerate(embeddings):\n",
    "        #     if(i in nodes_used):\n",
    "        #         embeddings_used.append(embeddings[i].detach().numpy())\n",
    "                \n",
    "        \n",
    "        # print(\"nodes used\", len(nodes_used))\n",
    "        # print(\"nodes used\", (nodes_used))\n",
    "        # z = torch.tensor(embeddings_used)\n",
    "        # print(z.size())\n",
    "        \n",
    "        # return torch.tensor(subgraph_edge_index), z\n",
    "        # pakai embeddings yang awal\n",
    "        return torch.tensor(subgraph_edge_index), torch.tensor(embeddings)\n",
    "    # (embeddings)\n",
    "    \n",
    "btch = None\n",
    "experiment = Experiment(dataset, 64)\n",
    "bcount = 0\n",
    "for b in train_loader:\n",
    "    # print(\"batch count\",bcount)\n",
    "    bcount+=1\n",
    "    btch = b\n",
    "    # print(btch.ptr)\n",
    "    experiment(btch.x, btch.edge_index, btch.batch, btch.ptr)\n",
    "    break\n",
    "    # break\n",
    "    # experiment(btch.x, btch.edge_index, btch.batch)\n",
    "\n",
    "# print(experiment)\n",
    "# experiment(btch.x, btch.edge_index, btch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expTrain(train_loader, val_loader, test_loader, epoch = 2):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "    experiment = Experiment(dataset, 64)\n",
    "\n",
    "    # Train\n",
    "    print('process training')\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(experiment, train_loader, True).item(), 5)\n",
    "        train_acc = round(test_base(experiment, train_loader, True), 5)\n",
    "        val_acc = round(test_base(experiment, val_loader, True), 5)\n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; test_acc: {val_acc}')\n",
    "\n",
    "    # Test\n",
    "    print('process testing')\n",
    "    test = test_base(experiment, test_loader, True)\n",
    "    print(f'Accuracy: {test}')\n",
    "\n",
    "# expTrain(train_loader, val_loader, test_loader, epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseTrain(train_loader, val_loader, test_loader, epoch = 10):\n",
    "    base = Base(dataset, 64)\n",
    "\n",
    "    # Train\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(base, train_loader).item(), 5)\n",
    "        train_acc = round(test_base(base, train_loader), 5)\n",
    "        val_acc = round(test_base(base, val_loader), 5)\n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; val_acc: {val_acc}; test: {round(test_base(base, test_loader), 2)}')\n",
    "\n",
    "    # Test\n",
    "    test = test_base(base, test_loader)\n",
    "    print(f'Accuracy: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.69; train_acc: 0.33; val_acc: 0.4; test: 0.26\n",
      "epoch 1; loss: 0.71; train_acc: 0.67; val_acc: 0.6; test: 0.74\n",
      "epoch 2; loss: 0.7; train_acc: 0.67; val_acc: 0.6; test: 0.74\n",
      "epoch 3; loss: 0.7; train_acc: 0.67; val_acc: 0.6; test: 0.74\n",
      "epoch 4; loss: 0.7; train_acc: 0.67; val_acc: 0.6; test: 0.74\n",
      "epoch 5; loss: 0.7; train_acc: 0.67; val_acc: 0.6; test: 0.74\n",
      "epoch 6; loss: 0.7; train_acc: 0.67; val_acc: 0.6; test: 0.74\n",
      "epoch 7; loss: 0.7; train_acc: 0.67; val_acc: 0.6; test: 0.74\n",
      "epoch 8; loss: 0.7; train_acc: 0.67; val_acc: 0.6; test: 0.74\n",
      "epoch 9; loss: 0.69; train_acc: 0.67; val_acc: 0.6; test: 0.74\n",
      "Accuracy: 0.7368421052631579\n",
      "Fold 1/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.7; train_acc: 0.34; val_acc: 0.33; test: 0.26\n",
      "epoch 1; loss: 0.97; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 2; loss: 0.87; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 3; loss: 0.81; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 4; loss: 0.81; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 5; loss: 0.85; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 6; loss: 0.84; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 7; loss: 0.84; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 8; loss: 0.85; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 9; loss: 0.84; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "Accuracy: 0.7368421052631579\n",
      "Fold 2/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.7; train_acc: 0.34; val_acc: 0.33; test: 0.26\n",
      "epoch 1; loss: 0.69; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 2; loss: 0.69; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 3; loss: 0.7; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 4; loss: 0.71; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 5; loss: 0.71; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 6; loss: 0.7; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 7; loss: 0.7; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 8; loss: 0.69; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "epoch 9; loss: 0.69; train_acc: 0.66; val_acc: 0.67; test: 0.74\n",
      "Accuracy: 0.7368421052631579\n",
      "Fold 3/10\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "train_dataset\n",
    "test_dataset\n",
    "k = 10\n",
    "\n",
    "splits = KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "k_counter = 0\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "    # print('Fold {}'.format(fold + 1))\n",
    "    # print(f'Fold',fold,'Train_idx',train_idx,'Val_idx',val_idx)\n",
    "    print(f'Fold {fold}/{k}')\n",
    "    if k_counter > 2:\n",
    "        break\n",
    "    \n",
    "    fold_train = []\n",
    "    for key in train_idx:\n",
    "        fold_train.append(dataset[key])\n",
    "\n",
    "    fold_val = [] \n",
    "    for key in val_idx:\n",
    "        fold_val.append(dataset[key])\n",
    "\n",
    "    tr = DataLoader(fold_train, batch_size=batch_size, shuffle=False)\n",
    "    vd = DataLoader(fold_val, batch_size=batch_size, shuffle=False)\n",
    "    ts = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Base model\n",
    "    print(\"=== Base model ===\")\n",
    "    baseTrain(tr, vd, ts, 10)\n",
    "    print(\"=== Experiment model ===\")\n",
    "    expTrain(tr, vd, ts, 10)\n",
    "    \n",
    "    k_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
