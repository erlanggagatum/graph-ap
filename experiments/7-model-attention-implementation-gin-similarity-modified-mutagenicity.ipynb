{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This model using modified similarity (similarity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from preprocessing import data_transformation\n",
    "from similarity import calculate_similarity_matrix\n",
    "\n",
    "from model import GCN\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root='datasets/', name='Mutagenicity')\n",
    "torch.manual_seed(1234)\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split: Train test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```train_dataset```: for training model<br/>\n",
    "```val_dataset```: evaluate model for hyperparameter tunning<br/>\n",
    "```test_dataset```: testing model after complete training<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ts, vl = 0.8, 0.1, 0.1\n",
    "dslen = len(dataset)\n",
    "tri = round(tr*dslen)\n",
    "tsi = round((tr+ts)*dslen)\n",
    "train_dataset = dataset[:tri]\n",
    "test_dataset = dataset[tri:tsi]\n",
    "val_dataset = dataset[tsi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 24], x=[12, 14], edge_attr=[24, 3], y=[1])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 0, 1, 0])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = dataset[0]\n",
    "\n",
    "# Function to reindex nodes\n",
    "def reindex_nodes(data):\n",
    "    unique_nodes = torch.unique(data.edge_index)\n",
    "    old_to_new = {old.item(): new for new, old in enumerate(unique_nodes)}\n",
    "\n",
    "    # Reindex edge_index\n",
    "    for i, edge in enumerate(data.edge_index.t()):\n",
    "        data.edge_index[0][i] = old_to_new[edge[0].item()]\n",
    "        data.edge_index[1][i] = old_to_new[edge[1].item()]\n",
    "\n",
    "    # Reorder x according to new indices\n",
    "    data.x = data.x[unique_nodes]\n",
    "\n",
    "    return data\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    data = reindex_nodes(dataset[i])\n",
    "    dataset[i].edge_index = data.edge_index\n",
    "    dataset[i].x = data.x\n",
    "    dataset[i].edge_attr = data.edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mutagenicity(4337)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 0, 1, 1])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        0])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)\n",
    "test_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)\n",
    "val_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import GINConv\n",
    "# from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        nn1 = Sequential(Linear(dataset.num_node_features, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        nn2 = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv2 = GCNConv(nn2)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        \n",
    "        # classification layer        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv3(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# batch1 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from similarity2 import calculate_similarity_matrix, testt\n",
    "\n",
    "\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels, k = 1):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # save number of subgraphs, default 1\n",
    "        self.k_subgraph = k\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        nn1 = Sequential(Linear(dataset.num_node_features, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        nn2 = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # embeddings for subgraph\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # attention layer\n",
    "        self.query_layer = Linear(hidden_channels,hidden_channels)\n",
    "        self.key_layer = Linear(hidden_channels,hidden_channels)\n",
    "        self.value_layer = Linear(hidden_channels,hidden_channels)\n",
    "        \n",
    "        # classification layer\n",
    "        self.lin = Linear(hidden_channels*2, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, ptr):\n",
    "        # Embed original\n",
    "        # embedding = self.conv1(x, edge_index)\n",
    "        # embedding = embedding.relu()\n",
    "        # embedding = self.conv2(embedding, edge_index)\n",
    "                \n",
    "        embedding = F.relu(self.conv1(x, edge_index))\n",
    "        embedding = self.bn1(embedding)\n",
    "        embedding = F.relu(self.conv2(embedding, edge_index))\n",
    "        embedding = self.bn2(embedding)\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        \n",
    "        subgraph_edge_index, communities, S, batch_communities = self.subgraph_generator(feature_emb, edge_index, batch, ptr)\n",
    "        \n",
    "        \n",
    "        subgraph_embedding = self.conv4(embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        subgraph_embedding = self.conv5(subgraph_embedding, subgraph_edge_index)\n",
    "        \n",
    "        # apply readout layer/pooling for each subgraphs\n",
    "        subgraph_pool_embedding = self.subgraph_pooling(subgraph_embedding, communities, batch, ptr, batch_communities)\n",
    "        # print(len(subgraph_pool_embedding))\n",
    "        # apply selective (top k) attention\n",
    "        topk_subgraph_embedding = self.selectk_subgraph(embedding, subgraph_pool_embedding, self.k_subgraph)\n",
    "        \n",
    "        # readout layer for original embedding\n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "                \n",
    "        combined_embeddings = torch.cat((embedding, topk_subgraph_embedding.view(len(embedding), -1)), 1)\n",
    "        \n",
    "        \n",
    "        # h = F.dropout(combined_embeddings, p=0.3, training=self.training)\n",
    "        h = self.lin(combined_embeddings)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h, S, communities, topk_subgraph_embedding.view(len(embedding), -1)\n",
    "    \n",
    "    # checked\n",
    "    def selectk_subgraph(self, embs, sub_embs, k = 1):\n",
    "        # calculate attention and select top k subgraph\n",
    "        \n",
    "        topk_subgraphs_all = []\n",
    "\n",
    "        for i, (emb, sub_emb) in enumerate(zip(embs, sub_embs)):\n",
    "            sub = torch.tensor(sub_emb)\n",
    "            sub = sub.to(torch.float32)\n",
    "\n",
    "            # transform\n",
    "            query = self.query_layer(emb)\n",
    "            key = self.key_layer(sub)\n",
    "            value = self.value_layer(sub)\n",
    "\n",
    "            # att score\n",
    "            attention_score = torch.matmul(query, key.transpose(0,1))\n",
    "            attention_weight = F.softmax(attention_score, dim=0)\n",
    "            \n",
    "            # select topk\n",
    "            topk_subgraph_embeddings = None\n",
    "            \n",
    "            if (k <= len(sub)):\n",
    "                topk_scores, topk_indices = torch.topk(attention_weight, k)\n",
    "                topk_subgraph_embeddings = sub[topk_indices]\n",
    "            else:\n",
    "                print('too big')\n",
    "                \n",
    "            topk_subgraphs_all.append(topk_subgraph_embeddings)\n",
    "        \n",
    "        return torch.stack(topk_subgraphs_all)\n",
    "    \n",
    "    \n",
    "    def subgraph_generator(self, embeddings, batch_edge_index, batch, ptr):\n",
    "        '''\n",
    "        Return subgraph_edge_index (edge_index of created subgraph)\n",
    "        '''\n",
    "        graph_counter = 0\n",
    "        edge_index = [[],[]]\n",
    "        subgraph_edge_index = [[],[]]\n",
    "        # Gs = []\n",
    "        sub_created = False\n",
    "        graph_bound = {}\n",
    "        all_communities = []\n",
    "        batch_communities = {}\n",
    "        S = []\n",
    "\n",
    "        for i in range(len(ptr)-1):\n",
    "            graph_bound[i] = [ptr[i].item(), ptr[i+1].item()]\n",
    "        \n",
    "        for i, (src, dst) in enumerate(zip(batch_edge_index[0], batch_edge_index[1])):\n",
    "            lower_bound = graph_bound[graph_counter][0]\n",
    "            upper_bound = graph_bound[graph_counter][1]\n",
    "            if ((src >= lower_bound and src < upper_bound) or\n",
    "                (dst >= lower_bound and dst < upper_bound)):\n",
    "                \n",
    "                edge_index[0].append(src - lower_bound)\n",
    "                edge_index[1].append(dst - lower_bound)\n",
    "            else:\n",
    "                sub_created = True\n",
    "                \n",
    "            if (i == len(batch_edge_index[0]) - 1) or sub_created:\n",
    "                sub_created = False\n",
    "                \n",
    "                embs = []\n",
    "                # make new graph\n",
    "                for i, (b, emb) in enumerate(zip(batch, embeddings)):\n",
    "                    if (b == graph_counter):\n",
    "                        embs.append(emb)\n",
    "                \n",
    "                G = data_transformation(edge_index, embs)\n",
    "                # dont need this at the moment\n",
    "                # Gs.append(G)\n",
    "                \n",
    "                # Calculate similarity matrix\n",
    "                S = calculate_similarity_matrix(G)\n",
    "                \n",
    "                # AP Clustering        \n",
    "                clustering = AffinityPropagation(affinity='precomputed', damping=0.8, random_state=42, convergence_iter=15, max_iter=1000)\n",
    "                clustering.fit(S)\n",
    "                \n",
    "                \n",
    "                # Get community\n",
    "                communities = {}\n",
    "                for lab in clustering.labels_:\n",
    "                    communities[lab] = []\n",
    "                    all_communities.append(lab)\n",
    "                for nd, clust in enumerate(clustering.labels_):\n",
    "                    communities[clust].append(nd)\n",
    "                \n",
    "                edge_index = [[],[]]\n",
    "                batch_communities[graph_counter] = communities\n",
    "                \n",
    "                graph_counter+=1\n",
    "                \n",
    "                # Make subgraph edge_index\n",
    "                for c in communities:\n",
    "                    w = G.subgraph(communities[c])\n",
    "                    for sub in w.edges:\n",
    "                        subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "                        subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                \n",
    "        \n",
    "        # print('batch communities', batch_communities)\n",
    "        return torch.tensor(subgraph_edge_index), all_communities, S, batch_communities\n",
    "    \n",
    "        \n",
    "    # check autograd (done)\n",
    "    def subgraph_pooling(self, embeddings, communities, batch, ptr, batch_communities, pool_type = 'mean'):\n",
    "        # batch communities: batch (or graph in this batch) -> communities -> member        \n",
    "        all_emb_pool = []\n",
    "        \n",
    "        # LOOP THROUGH BATCH\n",
    "        for b in batch_communities:\n",
    "            \n",
    "            # initialize array\n",
    "            emb_pool = [None] * len(batch_communities[b])\n",
    "            for comm in batch_communities[b]:\n",
    "                emb_temp = []\n",
    "\n",
    "                for member in batch_communities[b][comm]:\n",
    "                    index_used = member + ptr[b].item()\n",
    "                    emb_temp.append(embeddings[index_used])\n",
    "\n",
    "                # Pooling per sub graph using PyTorch\n",
    "                emb_temp_tensor = torch.stack(emb_temp)\n",
    "                if pool_type == 'mean': # mean pool\n",
    "                    emb_pool[comm] = torch.mean(emb_temp_tensor, dim=0)\n",
    "                elif pool_type == 'add': # add pool\n",
    "                    emb_pool[comm] = torch.sum(emb_temp_tensor, dim=0)\n",
    "                else:\n",
    "                    print('TODO: fill later')\n",
    "                    \n",
    "            all_emb_pool.append(torch.stack(emb_pool))\n",
    "        return all_emb_pool\n",
    "\n",
    "# experiment = Experiment(dataset, 64)\n",
    "# emb, h, S, communities, sub_emb = experiment(batch1.x, batch1.edge_index, batch1.batch, batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base(model, loader, experiment_mode=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        if experiment_mode:\n",
    "            emb, h, S, communities, sub_emb = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(h, data.y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_base(model, loader, experiment_mode=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h, S, communities, sub_emb = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        pred = h.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expTrain(train_loader, val_loader, test_loader, epoch = 10, fold=0):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "    num_hidden_layer = 128\n",
    "    experiment = Experiment(dataset, num_hidden_layer)\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    test_acc_history = []\n",
    "    \n",
    "    # early stop\n",
    "    early_stopping_patience = 20\n",
    "    best_val_score = -float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "    best_state = None\n",
    "    \n",
    "    # Train\n",
    "    print('process training')\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(experiment, train_loader, True).item(), 5)\n",
    "        train_acc = round(test_base(experiment, train_loader, True), 5)\n",
    "        val_acc = round(test_base(experiment, val_loader, True), 5)\n",
    "        test_acc = test_base(experiment, test_loader, True)\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "        test_acc_history.append(test_acc)\n",
    "        \n",
    "        print(f'epoch {_+1}; loss: {loss}; train_acc: {train_acc}; val_acc: {val_acc}; test_acc: {test_acc}')\n",
    "        \n",
    "        torch.save(experiment.state_dict(), \"model-history/GIN-Mutagenicity/\"+str(fold)+\"-e\"+str(_)+\".experiment_best_model-gin_data-mutag.pth\")\n",
    "        \n",
    "        # early stop\n",
    "        if (val_acc > best_val_score):\n",
    "            best_val_score = val_acc\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            print('best found, save model')\n",
    "            # save model\n",
    "            torch.save(experiment.state_dict(), \"model-history/GIN-Mutagenicity/\"+str(fold)+\".experiment_best_model-gin_data-mutag.pth\")\n",
    "            best_state = copy.deepcopy(experiment.state_dict())\n",
    "        # else:\n",
    "        #     epochs_without_improvement += 1\n",
    "        #     if (epochs_without_improvement >= early_stopping_patience):\n",
    "        #         print('early stop triggered')\n",
    "        #         break\n",
    "                \n",
    "            \n",
    "\n",
    "    # Test    \n",
    "    # Create a new instance of the model for testing\n",
    "    best_model = Experiment(dataset, num_hidden_layer)\n",
    "    best_model.load_state_dict(best_state)\n",
    "\n",
    "    # Test\n",
    "    test = test_base(best_model, test_loader, True)\n",
    "    print(f'Accuracy: {test}')\n",
    "\n",
    "    \n",
    "    return [loss_history, train_acc_history, val_acc_history, test_acc_history]\n",
    "\n",
    "# expTrain(train_loader, val_loader, test_loader, epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseTrain(train_loader, val_loader, test_loader, epoch = 10, fold=0):\n",
    "    num_hidden_layer = 128\n",
    "    base = Base(dataset, num_hidden_layer)\n",
    "    early_stopping_patience = 20\n",
    "    best_val_score = -float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "    best_state = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    for _ in range(epoch):\n",
    "        \n",
    "        loss = round(train_base(base, train_loader).item(), 5)\n",
    "        train_acc = round(test_base(base, train_loader), 5)\n",
    "        val_acc = round(test_base(base, val_loader), 5)\n",
    "        \n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; val_acc: {val_acc}; test: {round(test_base(base, test_loader), 2)}')\n",
    "        \n",
    "        torch.save(base.state_dict(), \"model-history/GIN-Mutagenicity/\"+str(fold)+\"-e\"+str(_)+\".base_best_model-gin_data-mutag.pth\")\n",
    "        # best_state = copy.deepcopy(base.state_dict())\n",
    "        \n",
    "        if (val_acc > best_val_score):\n",
    "            best_val_score = val_acc\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            print('best found, save model')\n",
    "            # save model\n",
    "            torch.save(base.state_dict(), \"model-history/GIN-Mutagenicity/\"+str(fold)+\".base_best_model-gin_data-mutag.pth\")\n",
    "            best_state = copy.deepcopy(base.state_dict())\n",
    "        # else:\n",
    "        #     epochs_without_improvement += 1\n",
    "        #     if (epochs_without_improvement >= early_stopping_patience):\n",
    "        #         print('early stop triggered')\n",
    "        #         break\n",
    "                \n",
    "            \n",
    "    # Test\n",
    "    # test = test_base(best, test_loader)\n",
    "    # print(f'Accuracy: {test}')\n",
    "    \n",
    "    # Create a new instance of the model for testing\n",
    "    best_model = Base(dataset, num_hidden_layer)\n",
    "    best_model.load_state_dict(best_state)\n",
    "\n",
    "    # Test\n",
    "    test = test_base(best_model, test_loader)\n",
    "    print(f'Accuracy: {test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import GINConv\n",
    "# from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        nn1 = Sequential(Linear(dataset.num_node_features, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        nn2 = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        \n",
    "        # classification layer        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = F.relu(self.conv1(x, edge_index))\n",
    "        embedding = self.bn1(embedding)\n",
    "        embedding = F.relu(self.conv2(embedding, edge_index))\n",
    "        embedding = self.bn2(embedding)\n",
    "        \n",
    "        embedding = global_add_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0,  ..., 0, 1, 1])\n",
      "3470\n",
      "tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[:round(len(dataset) * 0.8)]\n",
    "test_dataset = dataset[round(len(dataset) * 0.8):]\n",
    "print(train_dataset.y)\n",
    "print(len(train_dataset.y))\n",
    "print(test_dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/10\n",
      "ftrain [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 18 19 20 21]\n",
      "=== Base model vs Experiment ===\n",
      "=== Experiment model ===\n",
      "process training\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "train_dataset\n",
    "test_dataset\n",
    "k = 10\n",
    "batch_size = 128\n",
    "\n",
    "splits = KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "k_counter = 0\n",
    "fold_logs = {}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "    print(f'Fold {fold}/{k}')\n",
    "    \n",
    "    fold_train = []\n",
    "    for key in train_idx:\n",
    "        fold_train.append(train_dataset[key])\n",
    "\n",
    "    fold_val = [] \n",
    "    for key in val_idx:\n",
    "        fold_val.append(train_dataset[key])\n",
    "        \n",
    "    print('ftrain', train_idx[0:20])\n",
    "    tr = DataLoader(fold_train, batch_size=batch_size, shuffle=True)\n",
    "    vd = DataLoader(fold_val, batch_size=batch_size, shuffle=True)\n",
    "    ts = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(\"=== Base model vs Experiment ===\")\n",
    "    # print(\"=== Base model ===\")\n",
    "    # fold_logs[fold] = baseTrain(tr, vd, ts, 50, fold=fold)\n",
    "    print(\"=== Experiment model ===\")\n",
    "    fold_logs[fold] = expTrain(tr, vd, ts, 50, fold=fold)\n",
    "    # break\n",
    "    k_counter += 1\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: None, 1: None, 2: None, 3: None, 4: None, 5: None, 6: None, 7: None, 8: None, 9: None}\n"
     ]
    }
   ],
   "source": [
    "print(fold_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Experiment(dataset=dataset, hidden_channels=128)\n",
    "e.load_state_dict(torch.load(\"model-history/GIN-MUTAG/6.experiment_best_model-gin_data-mutag.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [0, 2, 3, 13, 1, 4, 5, 6, 7, 8, 14, 9, 20, 10, 11, 15, 16, 17, 18, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([*list(set(train_dataset[11].edge_index[0].tolist())), *list(set(train_dataset[11].edge_index[1].tolist()))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes2 = [[0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 7, 7, 7, 7, 8, 8, 9, 9, 10, 11, 11, 13, 14, 15, 16, 17, 18, 19, 20], \n",
    "[2, 3, 13, 0, 4, 5, 0, 6, 0, 7, 8, 14, 1, 6, 9, 1, 20, 2, 4, 10, 3, 11, 15, 16, 3, 17, 4, 18, 6, 7, 19, 0, 3, 7, 7, 8, 9, 11, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([*list(set(nodes2[0])), *list(set(nodes2[1]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {0: {0: 0.0, 1: 0.0, 2: 5.9, 3: 14.3, 4: 5.9, 5: 14.3, 6: 11.7, 7: 14.3, 8: 16.4, 20: 20.2, 9: 11.7, 10: 14.3, 11: 16.4, 21: 20.2, 12: 20.1, 22: 20.2, 23: 20.2, 24: 20.2, 13: 20.1, 25: 20.2, 26: 20.2, 27: 20.2, 14: 23.8, 15: 23.8, 16: 20.2, 17: 20.2, 18: 20.2, 19: 20.2}, 1: {0: 0.0, 1: 0.0, 2: 5.9, 3: 14.3, 4: 5.9, 5: 14.3, 6: 11.7, 7: 14.3, 8: 16.4, 20: 20.2, 9: 11.7, 10: 14.3, 11: 16.4, 21: 20.2, 12: 20.1, 22: 20.2, 23: 20.2, 24: 20.2, 13: 20.1, 25: 20.2, 26: 20.2, 27: 20.2, 14: 23.8, 15: 23.8, 16: 20.2, 17: 20.2, 18: 20.2, 19: 20.2}, 2: {0: 5.9, 1: 5.9, 2: 0.0, 3: 10.2, 4: 0.0, 5: 10.2, 6: 9.1, 7: 10.2, 8: 12.4, 20: 16.7, 9: 9.1, 10: 10.2, 11: 12.4, 21: 16.7, 12: 17.8, 22: 16.7, 23: 16.7, 24: 16.7, 13: 17.8, 25: 16.7, 26: 16.7, 27: 16.7, 14: 23.2, 15: 23.2, 16: 18.2, 17: 18.2, 18: 18.2, 19: 18.2}, 3: {0: 14.3, 1: 14.3, 2: 10.2, 3: 0.0, 4: 10.2, 5: 0.0, 6: 8.8, 7: 0.0, 8: 7.6, 20: 10.2, 9: 8.8, 10: 0.0, 11: 7.6, 21: 10.2, 12: 16.1, 22: 10.2, 23: 10.2, 24: 10.2, 13: 16.1, 25: 10.2, 26: 10.2, 27: 10.2, 14: 22.6, 15: 22.6, 16: 15.7, 17: 15.7, 18: 15.7, 19: 15.7}, 4: {0: 5.9, 1: 5.9, 2: 0.0, 3: 10.2, 4: 0.0, 5: 10.2, 6: 9.1, 7: 10.2, 8: 12.4, 20: 16.7, 9: 9.1, 10: 10.2, 11: 12.4, 21: 16.7, 12: 17.8, 22: 16.7, 23: 16.7, 24: 16.7, 13: 17.8, 25: 16.7, 26: 16.7, 27: 16.7, 14: 23.2, 15: 23.2, 16: 18.2, 17: 18.2, 18: 18.2, 19: 18.2}, 5: {0: 14.3, 1: 14.3, 2: 10.2, 3: 0.0, 4: 10.2, 5: 0.0, 6: 8.8, 7: 0.0, 8: 7.6, 20: 10.2, 9: 8.8, 10: 0.0, 11: 7.6, 21: 10.2, 12: 16.1, 22: 10.2, 23: 10.2, 24: 10.2, 13: 16.1, 25: 10.2, 26: 10.2, 27: 10.2, 14: 22.6, 15: 22.6, 16: 15.7, 17: 15.7, 18: 15.7, 19: 15.7}, 6: {0: 11.7, 1: 11.7, 2: 9.1, 3: 8.8, 4: 9.1, 5: 8.8, 6: 0.0, 7: 8.8, 8: 7.8, 20: 10.6, 9: 0.0, 10: 8.8, 11: 7.8, 21: 10.6, 12: 15.4, 22: 10.6, 23: 10.6, 24: 10.6, 13: 15.4, 25: 10.6, 26: 10.6, 27: 10.6, 14: 21.2, 15: 21.2, 16: 14.4, 17: 14.4, 18: 14.4, 19: 14.4}, 7: {0: 14.3, 1: 14.3, 2: 10.2, 3: 0.0, 4: 10.2, 5: 0.0, 6: 8.8, 7: 0.0, 8: 7.6, 20: 10.2, 9: 8.8, 10: 0.0, 11: 7.6, 21: 10.2, 12: 16.1, 22: 10.2, 23: 10.2, 24: 10.2, 13: 16.1, 25: 10.2, 26: 10.2, 27: 10.2, 14: 22.6, 15: 22.6, 16: 15.7, 17: 15.7, 18: 15.7, 19: 15.7}, 8: {0: 16.4, 1: 16.4, 2: 12.4, 3: 7.6, 4: 12.4, 5: 7.6, 6: 7.8, 7: 7.6, 8: 0.0, 20: 8.9, 9: 7.8, 10: 7.6, 11: 0.0, 21: 8.9, 12: 16.7, 22: 8.9, 23: 8.9, 24: 8.9, 13: 16.7, 25: 8.9, 26: 8.9, 27: 8.9, 14: 22.6, 15: 22.6, 16: 15.2, 17: 15.2, 18: 15.2, 19: 15.2}, 20: {0: 20.2, 1: 20.2, 2: 16.7, 3: 10.2, 4: 16.7, 5: 10.2, 6: 10.6, 7: 10.2, 8: 8.9, 20: 0.0, 9: 10.6, 10: 10.2, 11: 8.9, 21: 0.0, 12: 16.5, 22: 0.0, 23: 0.0, 24: 0.0, 13: 16.5, 25: 0.0, 26: 0.0, 27: 0.0, 14: 22.7, 15: 22.7, 16: 13.9, 17: 13.9, 18: 13.9, 19: 13.9}, 9: {0: 11.7, 1: 11.7, 2: 9.1, 3: 8.8, 4: 9.1, 5: 8.8, 6: 0.0, 7: 8.8, 8: 7.8, 20: 10.6, 9: 0.0, 10: 8.8, 11: 7.8, 21: 10.6, 12: 15.4, 22: 10.6, 23: 10.6, 24: 10.6, 13: 15.4, 25: 10.6, 26: 10.6, 27: 10.6, 14: 21.2, 15: 21.2, 16: 14.4, 17: 14.4, 18: 14.4, 19: 14.4}, 10: {0: 14.3, 1: 14.3, 2: 10.2, 3: 0.0, 4: 10.2, 5: 0.0, 6: 8.8, 7: 0.0, 8: 7.6, 20: 10.2, 9: 8.8, 10: 0.0, 11: 7.6, 21: 10.2, 12: 16.1, 22: 10.2, 23: 10.2, 24: 10.2, 13: 16.1, 25: 10.2, 26: 10.2, 27: 10.2, 14: 22.6, 15: 22.6, 16: 15.7, 17: 15.7, 18: 15.7, 19: 15.7}, 11: {0: 16.4, 1: 16.4, 2: 12.4, 3: 7.6, 4: 12.4, 5: 7.6, 6: 7.8, 7: 7.6, 8: 0.0, 20: 8.9, 9: 7.8, 10: 7.6, 11: 0.0, 21: 8.9, 12: 16.7, 22: 8.9, 23: 8.9, 24: 8.9, 13: 16.7, 25: 8.9, 26: 8.9, 27: 8.9, 14: 22.6, 15: 22.6, 16: 15.2, 17: 15.2, 18: 15.2, 19: 15.2}, 21: {0: 20.2, 1: 20.2, 2: 16.7, 3: 10.2, 4: 16.7, 5: 10.2, 6: 10.6, 7: 10.2, 8: 8.9, 20: 0.0, 9: 10.6, 10: 10.2, 11: 8.9, 21: 0.0, 12: 16.5, 22: 0.0, 23: 0.0, 24: 0.0, 13: 16.5, 25: 0.0, 26: 0.0, 27: 0.0, 14: 22.7, 15: 22.7, 16: 13.9, 17: 13.9, 18: 13.9, 19: 13.9}, 12: {0: 20.1, 1: 20.1, 2: 17.8, 3: 16.1, 4: 17.8, 5: 16.1, 6: 15.4, 7: 16.1, 8: 16.7, 20: 16.5, 9: 15.4, 10: 16.1, 11: 16.7, 21: 16.5, 12: 0.0, 22: 16.5, 23: 16.5, 24: 16.5, 13: 0.0, 25: 16.5, 26: 16.5, 27: 16.5, 14: 22.7, 15: 22.7, 16: 17.2, 17: 17.2, 18: 17.2, 19: 17.2}, 22: {0: 20.2, 1: 20.2, 2: 16.7, 3: 10.2, 4: 16.7, 5: 10.2, 6: 10.6, 7: 10.2, 8: 8.9, 20: 0.0, 9: 10.6, 10: 10.2, 11: 8.9, 21: 0.0, 12: 16.5, 22: 0.0, 23: 0.0, 24: 0.0, 13: 16.5, 25: 0.0, 26: 0.0, 27: 0.0, 14: 22.7, 15: 22.7, 16: 13.9, 17: 13.9, 18: 13.9, 19: 13.9}, 23: {0: 20.2, 1: 20.2, 2: 16.7, 3: 10.2, 4: 16.7, 5: 10.2, 6: 10.6, 7: 10.2, 8: 8.9, 20: 0.0, 9: 10.6, 10: 10.2, 11: 8.9, 21: 0.0, 12: 16.5, 22: 0.0, 23: 0.0, 24: 0.0, 13: 16.5, 25: 0.0, 26: 0.0, 27: 0.0, 14: 22.7, 15: 22.7, 16: 13.9, 17: 13.9, 18: 13.9, 19: 13.9}, 24: {0: 20.2, 1: 20.2, 2: 16.7, 3: 10.2, 4: 16.7, 5: 10.2, 6: 10.6, 7: 10.2, 8: 8.9, 20: 0.0, 9: 10.6, 10: 10.2, 11: 8.9, 21: 0.0, 12: 16.5, 22: 0.0, 23: 0.0, 24: 0.0, 13: 16.5, 25: 0.0, 26: 0.0, 27: 0.0, 14: 22.7, 15: 22.7, 16: 13.9, 17: 13.9, 18: 13.9, 19: 13.9}, 13: {0: 20.1, 1: 20.1, 2: 17.8, 3: 16.1, 4: 17.8, 5: 16.1, 6: 15.4, 7: 16.1, 8: 16.7, 20: 16.5, 9: 15.4, 10: 16.1, 11: 16.7, 21: 16.5, 12: 0.0, 22: 16.5, 23: 16.5, 24: 16.5, 13: 0.0, 25: 16.5, 26: 16.5, 27: 16.5, 14: 22.7, 15: 22.7, 16: 17.2, 17: 17.2, 18: 17.2, 19: 17.2}, 25: {0: 20.2, 1: 20.2, 2: 16.7, 3: 10.2, 4: 16.7, 5: 10.2, 6: 10.6, 7: 10.2, 8: 8.9, 20: 0.0, 9: 10.6, 10: 10.2, 11: 8.9, 21: 0.0, 12: 16.5, 22: 0.0, 23: 0.0, 24: 0.0, 13: 16.5, 25: 0.0, 26: 0.0, 27: 0.0, 14: 22.7, 15: 22.7, 16: 13.9, 17: 13.9, 18: 13.9, 19: 13.9}, 26: {0: 20.2, 1: 20.2, 2: 16.7, 3: 10.2, 4: 16.7, 5: 10.2, 6: 10.6, 7: 10.2, 8: 8.9, 20: 0.0, 9: 10.6, 10: 10.2, 11: 8.9, 21: 0.0, 12: 16.5, 22: 0.0, 23: 0.0, 24: 0.0, 13: 16.5, 25: 0.0, 26: 0.0, 27: 0.0, 14: 22.7, 15: 22.7, 16: 13.9, 17: 13.9, 18: 13.9, 19: 13.9}, 27: {0: 20.2, 1: 20.2, 2: 16.7, 3: 10.2, 4: 16.7, 5: 10.2, 6: 10.6, 7: 10.2, 8: 8.9, 20: 0.0, 9: 10.6, 10: 10.2, 11: 8.9, 21: 0.0, 12: 16.5, 22: 0.0, 23: 0.0, 24: 0.0, 13: 16.5, 25: 0.0, 26: 0.0, 27: 0.0, 14: 22.7, 15: 22.7, 16: 13.9, 17: 13.9, 18: 13.9, 19: 13.9}, 14: {0: 23.8, 1: 23.8, 2: 23.2, 3: 22.6, 4: 23.2, 5: 22.6, 6: 21.2, 7: 22.6, 8: 22.6, 20: 22.7, 9: 21.2, 10: 22.6, 11: 22.6, 21: 22.7, 12: 22.7, 22: 22.7, 23: 22.7, 24: 22.7, 13: 22.7, 25: 22.7, 26: 22.7, 27: 22.7, 14: 0.0, 15: 0.0, 16: 11.3, 17: 11.3, 18: 11.3, 19: 11.3}, 15: {0: 23.8, 1: 23.8, 2: 23.2, 3: 22.6, 4: 23.2, 5: 22.6, 6: 21.2, 7: 22.6, 8: 22.6, 20: 22.7, 9: 21.2, 10: 22.6, 11: 22.6, 21: 22.7, 12: 22.7, 22: 22.7, 23: 22.7, 24: 22.7, 13: 22.7, 25: 22.7, 26: 22.7, 27: 22.7, 14: 0.0, 15: 0.0, 16: 11.3, 17: 11.3, 18: 11.3, 19: 11.3}, 16: {0: 20.2, 1: 20.2, 2: 18.2, 3: 15.7, 4: 18.2, 5: 15.7, 6: 14.4, 7: 15.7, 8: 15.2, 20: 13.9, 9: 14.4, 10: 15.7, 11: 15.2, 21: 13.9, 12: 17.2, 22: 13.9, 23: 13.9, 24: 13.9, 13: 17.2, 25: 13.9, 26: 13.9, 27: 13.9, 14: 11.3, 15: 11.3, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0}, 17: {0: 20.2, 1: 20.2, 2: 18.2, 3: 15.7, 4: 18.2, 5: 15.7, 6: 14.4, 7: 15.7, 8: 15.2, 20: 13.9, 9: 14.4, 10: 15.7, 11: 15.2, 21: 13.9, 12: 17.2, 22: 13.9, 23: 13.9, 24: 13.9, 13: 17.2, 25: 13.9, 26: 13.9, 27: 13.9, 14: 11.3, 15: 11.3, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0}, 18: {0: 20.2, 1: 20.2, 2: 18.2, 3: 15.7, 4: 18.2, 5: 15.7, 6: 14.4, 7: 15.7, 8: 15.2, 20: 13.9, 9: 14.4, 10: 15.7, 11: 15.2, 21: 13.9, 12: 17.2, 22: 13.9, 23: 13.9, 24: 13.9, 13: 17.2, 25: 13.9, 26: 13.9, 27: 13.9, 14: 11.3, 15: 11.3, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0}, 19: {0: 20.2, 1: 20.2, 2: 18.2, 3: 15.7, 4: 18.2, 5: 15.7, 6: 14.4, 7: 15.7, 8: 15.2, 20: 13.9, 9: 14.4, 10: 15.7, 11: 15.2, 21: 13.9, 12: 17.2, 22: 13.9, 23: 13.9, 24: 13.9, 13: 17.2, 25: 13.9, 26: 13.9, 27: 13.9, 14: 11.3, 15: 11.3, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for dt in train_dataset: \n",
    "    if i == 20:\n",
    "        break\n",
    "    i+= 1\n",
    "    edge = list(set([*list(set(dt.edge_index[0].tolist())), *list(set(dt.edge_index[1].tolist()))]))\n",
    "    print(edge)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
