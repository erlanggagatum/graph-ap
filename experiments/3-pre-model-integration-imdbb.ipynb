{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from preprocessing import data_transformation\n",
    "from similarity import calculate_similarity_matrix\n",
    "\n",
    "from model import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root='datasets/', name='IMDB-BINARY')\n",
    "torch.manual_seed(1234)\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 196], y=[1], num_nodes=21, x=[21, 136])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_degree = 0\n",
    "degs = []\n",
    "for data in dataset:\n",
    "    deg = torch_geometric.utils.degree(data.edge_index[1], num_nodes=data.num_nodes)\n",
    "    degs.extend(deg.numpy())\n",
    "    max_degree = max(max_degree, max(deg).item())\n",
    "# assign to one hot degree for each data (OneHotDegree receive maximum degree parameter)\n",
    "dataset.transform = torch_geometric.transforms.OneHotDegree(int(max_degree))\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split: Train test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```train_dataset```: for training model<br/>\n",
    "```val_dataset```: evaluate model for hyperparameter tunning<br/>\n",
    "```test_dataset```: testing model after complete training<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ts, vl = 0.8, 0.1, 0.1\n",
    "dslen = len(dataset)\n",
    "tri = round(tr*dslen)\n",
    "tsi = round((tr+ts)*dslen)\n",
    "train_dataset = dataset[:tri]\n",
    "test_dataset = dataset[tri:tsi]\n",
    "val_dataset = dataset[tsi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)\n",
    "test_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)\n",
    "val_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader\n",
      "tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0])\n",
      "tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1])\n",
      "tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1])\n",
      "tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0])\n",
      "tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1])\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1])\n",
      "tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1])\n",
      "tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0])\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0])\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1])\n",
      "tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1])\n",
      "val loader\n",
      "tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1])\n",
      "test loader\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print('train loader')\n",
    "for data in train_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('val loader')\n",
    "for data in val_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('test loader')\n",
    "for data in test_loader:\n",
    "    print(data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv3(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base(\n",
       "  (conv1): GCNConv(136, 64)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (conv3): GCNConv(64, 64)\n",
       "  (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = Base(dataset, 64)\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6222, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_base(model, loader, experiment_mode=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(h, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss\n",
    "    #     print(h[0])\n",
    "    # print(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_base(model, loader, experiment_mode=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        pred = h.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct/len(loader.dataset)\n",
    "\n",
    "base = Base(dataset, 64)\n",
    "train_base(base, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; loss: 0.56; train_acc: 0.69; test_acc: 0.76\n",
      "epoch 1; loss: 0.46; train_acc: 0.71; test_acc: 0.69\n",
      "epoch 2; loss: 0.53; train_acc: 0.72; test_acc: 0.72\n",
      "epoch 3; loss: 0.55; train_acc: 0.69; test_acc: 0.64\n",
      "epoch 4; loss: 0.55; train_acc: 0.66; test_acc: 0.59\n",
      "epoch 5; loss: 0.49; train_acc: 0.75; test_acc: 0.73\n",
      "epoch 6; loss: 0.58; train_acc: 0.74; test_acc: 0.66\n",
      "epoch 7; loss: 0.5; train_acc: 0.75; test_acc: 0.72\n",
      "epoch 8; loss: 0.52; train_acc: 0.71; test_acc: 0.64\n",
      "epoch 9; loss: 0.48; train_acc: 0.73; test_acc: 0.68\n",
      "epoch 10; loss: 0.5; train_acc: 0.74; test_acc: 0.68\n",
      "epoch 11; loss: 0.53; train_acc: 0.68; test_acc: 0.59\n",
      "epoch 12; loss: 0.47; train_acc: 0.76; test_acc: 0.7\n",
      "epoch 13; loss: 0.53; train_acc: 0.73; test_acc: 0.66\n",
      "epoch 14; loss: 0.52; train_acc: 0.76; test_acc: 0.7\n",
      "epoch 15; loss: 0.46; train_acc: 0.77; test_acc: 0.75\n",
      "epoch 16; loss: 0.44; train_acc: 0.77; test_acc: 0.72\n",
      "epoch 17; loss: 0.47; train_acc: 0.73; test_acc: 0.66\n",
      "epoch 18; loss: 0.48; train_acc: 0.78; test_acc: 0.68\n",
      "epoch 19; loss: 0.49; train_acc: 0.67; test_acc: 0.59\n",
      "epoch 20; loss: 0.49; train_acc: 0.7; test_acc: 0.64\n",
      "epoch 21; loss: 0.5; train_acc: 0.71; test_acc: 0.64\n",
      "epoch 22; loss: 0.5; train_acc: 0.71; test_acc: 0.62\n",
      "epoch 23; loss: 0.49; train_acc: 0.73; test_acc: 0.64\n",
      "epoch 24; loss: 0.51; train_acc: 0.69; test_acc: 0.62\n",
      "epoch 25; loss: 0.45; train_acc: 0.76; test_acc: 0.75\n",
      "epoch 26; loss: 0.52; train_acc: 0.73; test_acc: 0.67\n",
      "epoch 27; loss: 0.4; train_acc: 0.79; test_acc: 0.69\n",
      "epoch 28; loss: 0.56; train_acc: 0.68; test_acc: 0.63\n",
      "epoch 29; loss: 0.45; train_acc: 0.76; test_acc: 0.69\n",
      "epoch 30; loss: 0.49; train_acc: 0.7; test_acc: 0.63\n",
      "epoch 31; loss: 0.5; train_acc: 0.75; test_acc: 0.64\n",
      "epoch 32; loss: 0.49; train_acc: 0.71; test_acc: 0.63\n",
      "epoch 33; loss: 0.61; train_acc: 0.78; test_acc: 0.67\n",
      "epoch 34; loss: 0.51; train_acc: 0.71; test_acc: 0.66\n",
      "epoch 35; loss: 0.56; train_acc: 0.77; test_acc: 0.67\n",
      "epoch 36; loss: 0.54; train_acc: 0.7; test_acc: 0.63\n",
      "epoch 37; loss: 0.59; train_acc: 0.61; test_acc: 0.55\n",
      "epoch 38; loss: 0.46; train_acc: 0.73; test_acc: 0.66\n",
      "epoch 39; loss: 0.56; train_acc: 0.68; test_acc: 0.61\n",
      "epoch 40; loss: 0.61; train_acc: 0.62; test_acc: 0.56\n",
      "epoch 41; loss: 0.72; train_acc: 0.68; test_acc: 0.61\n",
      "epoch 42; loss: 0.63; train_acc: 0.65; test_acc: 0.57\n",
      "epoch 43; loss: 0.73; train_acc: 0.72; test_acc: 0.63\n",
      "epoch 44; loss: 0.57; train_acc: 0.65; test_acc: 0.57\n",
      "epoch 45; loss: 0.53; train_acc: 0.74; test_acc: 0.65\n",
      "epoch 46; loss: 0.47; train_acc: 0.73; test_acc: 0.62\n",
      "epoch 47; loss: 0.56; train_acc: 0.64; test_acc: 0.57\n",
      "epoch 48; loss: 0.64; train_acc: 0.62; test_acc: 0.56\n",
      "epoch 49; loss: 0.46; train_acc: 0.66; test_acc: 0.61\n",
      "epoch 50; loss: 0.5; train_acc: 0.73; test_acc: 0.66\n",
      "epoch 51; loss: 1.29; train_acc: 0.67; test_acc: 0.59\n",
      "epoch 52; loss: 0.65; train_acc: 0.56; test_acc: 0.51\n",
      "epoch 53; loss: 0.64; train_acc: 0.6; test_acc: 0.55\n",
      "epoch 54; loss: 0.65; train_acc: 0.58; test_acc: 0.59\n",
      "epoch 55; loss: 0.66; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 56; loss: 0.65; train_acc: 0.57; test_acc: 0.59\n",
      "epoch 57; loss: 0.66; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 58; loss: 0.65; train_acc: 0.54; test_acc: 0.56\n",
      "epoch 59; loss: 0.65; train_acc: 0.57; test_acc: 0.57\n",
      "epoch 60; loss: 0.68; train_acc: 0.53; test_acc: 0.56\n",
      "epoch 61; loss: 0.66; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 62; loss: 0.65; train_acc: 0.57; test_acc: 0.56\n",
      "epoch 63; loss: 0.66; train_acc: 0.57; test_acc: 0.56\n",
      "epoch 64; loss: 0.66; train_acc: 0.52; test_acc: 0.55\n",
      "epoch 65; loss: 0.65; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 66; loss: 0.66; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 67; loss: 0.65; train_acc: 0.57; test_acc: 0.56\n",
      "epoch 68; loss: 0.67; train_acc: 0.52; test_acc: 0.55\n",
      "epoch 69; loss: 0.65; train_acc: 0.57; test_acc: 0.56\n",
      "epoch 70; loss: 0.65; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 71; loss: 0.66; train_acc: 0.57; test_acc: 0.56\n",
      "epoch 72; loss: 0.65; train_acc: 0.55; test_acc: 0.56\n",
      "epoch 73; loss: 0.65; train_acc: 0.54; test_acc: 0.55\n",
      "epoch 74; loss: 0.65; train_acc: 0.57; test_acc: 0.56\n",
      "epoch 75; loss: 0.65; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 76; loss: 0.65; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 77; loss: 0.66; train_acc: 0.55; test_acc: 0.56\n",
      "epoch 78; loss: 0.65; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 79; loss: 0.65; train_acc: 0.56; test_acc: 0.56\n",
      "epoch 80; loss: 0.7; train_acc: 0.48; test_acc: 0.55\n",
      "epoch 81; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 82; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 83; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 84; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 85; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 86; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 87; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 88; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 89; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 90; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 91; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 92; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 93; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 94; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 95; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 96; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 97; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 98; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "epoch 99; loss: 0.69; train_acc: 0.52; test_acc: 0.45\n",
      "Accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "base = Base(dataset, 64)\n",
    "train_base(base, train_loader)\n",
    "\n",
    "# Train\n",
    "for _ in range(epoch):\n",
    "    loss = round(train_base(base, train_loader).item(), 2)\n",
    "    train_acc = round(test_base(base, train_loader), 2)\n",
    "    val_acc = round(test_base(base, val_loader), 2)\n",
    "    \n",
    "    print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; test_acc: {val_acc}')\n",
    "\n",
    "# Test\n",
    "test = test_base(base, test_loader)\n",
    "print(f'Accuracy: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        G = data_transformation(edge_index, feature_emb)\n",
    "        S = calculate_similarity_matrix(G)\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h\n",
    "\n",
    "    def data_transformation():\n",
    "        print('s')\n",
    "        \n",
    "\n",
    "\n",
    "experiment = Experiment(dataset, 64)\n",
    "# train_base(experiment, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 12108], y=[64], num_nodes=1200, x=[1200, 136], batch=[1200], ptr=[65])\n",
      "tensor([ 0,  0,  0,  ..., 63, 63, 63])\n",
      "edge_index tensor([[   0,    0,    0,  ..., 1199, 1199, 1199],\n",
      "        [   1,    3,    5,  ..., 1194, 1196, 1197]])\n",
      "batch None\n",
      "ptr tensor([   0,   21,   37,   57,   81,   93,  107,  125,  143,  161,  173,  189,\n",
      "         203,  231,  251,  271,  292,  319,  339,  351,  370,  382,  398,  413,\n",
      "         433,  452,  466,  492,  504,  519,  536,  550,  570,  600,  612,  624,\n",
      "         640,  663,  678,  697,  709,  729,  745,  777,  802,  816,  832,  862,\n",
      "         874,  924,  936,  964,  976,  996, 1011, 1032, 1050, 1080, 1101, 1114,\n",
      "        1136, 1151, 1166, 1188, 1200])\n"
     ]
    }
   ],
   "source": [
    "batch1 = None\n",
    "for batch in train_loader:\n",
    "    batch1 = batch\n",
    "    break\n",
    "print(batch1)\n",
    "print(batch1.batch)\n",
    "print(\"edge_index\", batch1.edge_index)\n",
    "print(\"batch\",batch1.edge_attr)\n",
    "print(\"ptr\",batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1199)\n",
      "tensor(1199)\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,\n",
      "          2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,\n",
      "          4,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,\n",
      "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "          8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13,\n",
      "         13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17,\n",
      "         17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19,\n",
      "         19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
      "        [ 1,  3,  5,  6, 10, 13, 14,  0,  3,  5,  6, 10, 13, 14,  7,  8, 10, 11,\n",
      "         12, 15, 16, 18, 20,  0,  1,  5,  6, 10, 13, 14,  7,  8,  9, 10, 15, 19,\n",
      "         20,  0,  1,  3,  6, 10, 13, 14,  0,  1,  3,  5, 10, 13, 14,  2,  4,  8,\n",
      "          9, 10, 11, 12, 15, 16, 17, 18, 19, 20,  2,  4,  7,  9, 10, 11, 12, 15,\n",
      "         16, 17, 18, 19, 20,  4,  7,  8, 10, 15, 19, 20,  0,  1,  2,  3,  4,  5,\n",
      "          6,  7,  8,  9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,  2,  7,  8, 10,\n",
      "         12, 15, 16, 18, 20,  2,  7,  8, 10, 11, 15, 16, 17, 18, 20,  0,  1,  3,\n",
      "          5,  6, 10, 14,  0,  1,  3,  5,  6, 10, 13,  2,  4,  7,  8,  9, 10, 11,\n",
      "         12, 16, 17, 18, 19, 20,  2,  7,  8, 10, 11, 12, 15, 17, 18, 20,  7,  8,\n",
      "         10, 12, 15, 16, 20,  2,  7,  8, 10, 11, 12, 15, 16, 20,  4,  7,  8,  9,\n",
      "         10, 15, 20,  2,  4,  7,  8,  9, 10, 11, 12, 15, 16, 17, 18, 19]])\n",
      "tensor([   0,   21,   37,   57,   81,   93,  107,  125,  143,  161,  173,  189,\n",
      "         203,  231,  251,  271,  292,  319,  339,  351,  370,  382,  398,  413,\n",
      "         433,  452,  466,  492,  504,  519,  536,  550,  570,  600,  612,  624,\n",
      "         640,  663,  678,  697,  709,  729,  745,  777,  802,  816,  832,  862,\n",
      "         874,  924,  936,  964,  976,  996, 1011, 1032, 1050, 1080, 1101, 1114,\n",
      "        1136, 1151, 1166, 1188, 1200]) ; len: 65\n"
     ]
    }
   ],
   "source": [
    "print(max(batch1.edge_index[0]))\n",
    "print(max(batch1.edge_index[1]))\n",
    "print((dataset[0].edge_index))\n",
    "print((batch1.ptr), '; len:', len(batch1.ptr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below --> Subgraph extractor with batch information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "\n",
    "\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # embeddings for subgraph\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # classification layer\n",
    "        self.lin = Linear(hidden_channels*2, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, ptr):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv3(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        \n",
    "        subgraph_edge_index, _ = self.subgraph_generator(feature_emb, edge_index, batch, ptr)\n",
    "        subgraph_embedding = self.conv4(embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        subgraph_embedding = self.conv5(subgraph_embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        # self.subgraph_pooling(\"\",\"\",\"\")\n",
    "        subgraph_embedding = global_max_pool(subgraph_embedding, batch)\n",
    "        \n",
    "        \n",
    "        h = torch.cat((embedding, subgraph_embedding), 1)\n",
    "        \n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin(h)\n",
    "        h = h.relu()\n",
    "        x = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h\n",
    "    \n",
    "    def subgraph_pooling(self, embeddings, batch, ptr):\n",
    "        print('subgraph pooling')\n",
    "\n",
    "    def subgraph_generator(self, embeddings, batch_edge_index, batch, ptr):\n",
    "        '''\n",
    "        Return subgraph_edge_index (edge_index of created subgraph)\n",
    "        '''\n",
    "        graph_counter = 0\n",
    "        edge_index = [[],[]]\n",
    "        subgraph_edge_index = [[],[]]\n",
    "        # Gs = []\n",
    "        sub_created = False\n",
    "        graph_bound = {}\n",
    "\n",
    "        for i in range(len(ptr)-1):\n",
    "            graph_bound[i] = [ptr[i].item(), ptr[i+1].item()]\n",
    "        \n",
    "        for i, (src, dst) in enumerate(zip(batch_edge_index[0], batch_edge_index[1])):\n",
    "            lower_bound = graph_bound[graph_counter][0]\n",
    "            upper_bound = graph_bound[graph_counter][1]\n",
    "            if ((src >= lower_bound and src < upper_bound) or\n",
    "                (dst >= lower_bound and dst < upper_bound)):\n",
    "                \n",
    "                edge_index[0].append(src - lower_bound)\n",
    "                edge_index[1].append(dst - lower_bound)\n",
    "            else:\n",
    "                sub_created = True\n",
    "                \n",
    "            if (i == len(batch_edge_index[0]) - 1) or sub_created:\n",
    "                sub_created = False\n",
    "                \n",
    "                embs = []\n",
    "                # make new graph\n",
    "                for i, (b, emb) in enumerate(zip(batch, embeddings)):\n",
    "                    if (b == graph_counter):\n",
    "                        embs.append(emb)\n",
    "                \n",
    "                G = data_transformation(edge_index, embs)\n",
    "                # dont need this at the moment\n",
    "                # Gs.append(G)\n",
    "                \n",
    "                # Calculate similarity matrix\n",
    "                S = calculate_similarity_matrix(G)\n",
    "                \n",
    "                # AP Clustering        \n",
    "                clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=123, convergence_iter=5, max_iter=100).fit(S)\n",
    "                \n",
    "                # Get community\n",
    "                communities = {}\n",
    "                for lab in clustering.labels_:\n",
    "                    communities[lab] = []\n",
    "                \n",
    "                for nd, clust in enumerate(clustering.labels_):\n",
    "                    communities[clust].append(nd)\n",
    "                \n",
    "                edge_index = [[],[]]\n",
    "                graph_counter+=1\n",
    "                \n",
    "                # Make subgraph edge_index\n",
    "                for c in communities:\n",
    "                    w = G.subgraph(communities[c])\n",
    "                    for sub in w.edges:\n",
    "                        subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "                        subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                        \n",
    "        return torch.tensor(subgraph_edge_index), torch.tensor(embeddings)\n",
    "    # (embeddings)\n",
    "\n",
    "\n",
    "# btch = None\n",
    "# experiment = Experiment(dataset, 64)\n",
    "# bcount = 0\n",
    "# for b in train_loader:\n",
    "#     bcount+=1\n",
    "#     btch = b\n",
    "#     experiment(btch.x, btch.edge_index, btch.batch, btch.ptr)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expTrain(train_loader, val_loader, test_loader, epoch = 2):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "    experiment = Experiment(dataset, 64)\n",
    "\n",
    "    # Train\n",
    "    print('process training')\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(experiment, train_loader, True).item(), 5)\n",
    "        train_acc = round(test_base(experiment, train_loader, True), 5)\n",
    "        val_acc = round(test_base(experiment, val_loader, True), 5)\n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; test_acc: {val_acc}')\n",
    "\n",
    "    # Test\n",
    "    print('process testing')\n",
    "    test = test_base(experiment, test_loader, True)\n",
    "    print(f'Accuracy: {test}')\n",
    "\n",
    "# expTrain(train_loader, val_loader, test_loader, epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseTrain(train_loader, val_loader, test_loader, epoch = 10):\n",
    "    base = Base(dataset, 64)\n",
    "\n",
    "    # Train\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(base, train_loader).item(), 5)\n",
    "        train_acc = round(test_base(base, train_loader), 5)\n",
    "        val_acc = round(test_base(base, val_loader), 5)\n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; val_acc: {val_acc}; test: {round(test_base(base, test_loader), 2)}')\n",
    "\n",
    "    # Test\n",
    "    test = test_base(base, test_loader)\n",
    "    print(f'Accuracy: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:round(len(dataset) * 0.8)]\n",
    "test_dataset = dataset[round(len(dataset) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.69358; train_acc: 0.7125; val_acc: 0.675; test: 0.81\n",
      "epoch 1; loss: 0.51878; train_acc: 0.72083; val_acc: 0.675; test: 0.73\n",
      "epoch 2; loss: 0.57897; train_acc: 0.71944; val_acc: 0.6375; test: 0.73\n",
      "epoch 3; loss: 0.55901; train_acc: 0.72778; val_acc: 0.6875; test: 0.74\n",
      "epoch 4; loss: 0.58857; train_acc: 0.70556; val_acc: 0.575; test: 0.68\n",
      "epoch 5; loss: 0.49834; train_acc: 0.7125; val_acc: 0.6625; test: 0.72\n",
      "epoch 6; loss: 0.55777; train_acc: 0.73472; val_acc: 0.7125; test: 0.77\n",
      "epoch 7; loss: 0.59016; train_acc: 0.74306; val_acc: 0.725; test: 0.76\n",
      "epoch 8; loss: 0.52072; train_acc: 0.74722; val_acc: 0.6625; test: 0.77\n",
      "epoch 9; loss: 0.58987; train_acc: 0.7125; val_acc: 0.75; test: 0.77\n",
      "Accuracy: 0.77\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 0; loss: 0.67796; train_acc: 0.475; test_acc: 0.575\n",
      "epoch 1; loss: 0.66832; train_acc: 0.65417; test_acc: 0.7125\n",
      "epoch 2; loss: 0.7253; train_acc: 0.60972; test_acc: 0.6625\n",
      "epoch 3; loss: 0.65472; train_acc: 0.67639; test_acc: 0.6\n",
      "epoch 4; loss: 0.55065; train_acc: 0.69861; test_acc: 0.625\n",
      "epoch 5; loss: 0.60276; train_acc: 0.675; test_acc: 0.55\n",
      "epoch 6; loss: 0.62852; train_acc: 0.66944; test_acc: 0.6875\n",
      "epoch 7; loss: 0.59652; train_acc: 0.56111; test_acc: 0.6375\n",
      "epoch 8; loss: 0.41183; train_acc: 0.71806; test_acc: 0.65\n",
      "epoch 9; loss: 0.5631; train_acc: 0.71806; test_acc: 0.5875\n",
      "process testing\n",
      "Accuracy: 0.665\n",
      "Fold 1/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.62789; train_acc: 0.70833; val_acc: 0.6625; test: 0.75\n",
      "epoch 1; loss: 0.81079; train_acc: 0.73333; val_acc: 0.675; test: 0.74\n",
      "epoch 2; loss: 0.51333; train_acc: 0.69167; val_acc: 0.7375; test: 0.7\n",
      "epoch 3; loss: 0.79346; train_acc: 0.75; val_acc: 0.7; test: 0.75\n",
      "epoch 4; loss: 0.45477; train_acc: 0.71806; val_acc: 0.7; test: 0.76\n",
      "epoch 5; loss: 0.44062; train_acc: 0.73056; val_acc: 0.675; test: 0.77\n",
      "epoch 6; loss: 0.40441; train_acc: 0.75833; val_acc: 0.7125; test: 0.78\n",
      "epoch 7; loss: 0.39745; train_acc: 0.71389; val_acc: 0.7125; test: 0.68\n",
      "epoch 8; loss: 0.42572; train_acc: 0.59028; val_acc: 0.55; test: 0.61\n",
      "epoch 9; loss: 0.51078; train_acc: 0.74722; val_acc: 0.7; test: 0.74\n",
      "Accuracy: 0.74\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 0; loss: 0.67639; train_acc: 0.51389; test_acc: 0.525\n",
      "epoch 1; loss: 0.9167; train_acc: 0.57639; test_acc: 0.6\n",
      "epoch 2; loss: 0.59037; train_acc: 0.48611; test_acc: 0.475\n",
      "epoch 3; loss: 0.50663; train_acc: 0.59444; test_acc: 0.5625\n",
      "epoch 4; loss: 0.69479; train_acc: 0.575; test_acc: 0.5625\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "train_dataset\n",
    "test_dataset\n",
    "k = 10\n",
    "\n",
    "splits = KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "k_counter = 0\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "    # print('Fold {}'.format(fold + 1))\n",
    "    # print(f'Fold',fold,'Train_idx',train_idx,'Val_idx',val_idx)\n",
    "    print(f'Fold {fold}/{k}')\n",
    "    #if k_counter > 2:\n",
    "    #    break\n",
    "    \n",
    "    fold_train = []\n",
    "    for key in train_idx:\n",
    "        fold_train.append(train_dataset[key])\n",
    "\n",
    "    fold_val = [] \n",
    "    for key in val_idx:\n",
    "        fold_val.append(train_dataset[key])\n",
    "\n",
    "    tr = DataLoader(fold_train, batch_size=batch_size, shuffle=True)\n",
    "    vd = DataLoader(fold_val, batch_size=batch_size, shuffle=True)\n",
    "    ts = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Base model\n",
    "    print(\"=== Base model ===\")\n",
    "    baseTrain(tr, vd, ts, 10)\n",
    "    print(\"=== Experiment model ===\")\n",
    "    expTrain(tr, vd, ts, 10)\n",
    "    \n",
    "    k_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
