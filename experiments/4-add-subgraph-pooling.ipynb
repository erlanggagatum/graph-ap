{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from preprocessing import data_transformation\n",
    "from similarity import calculate_similarity_matrix\n",
    "\n",
    "from model import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root='datasets/', name='MUTAG')\n",
    "torch.manual_seed(1234)\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split: Train test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```train_dataset```: for training model<br/>\n",
    "```val_dataset```: evaluate model for hyperparameter tunning<br/>\n",
    "```test_dataset```: testing model after complete training<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ts, vl = 0.8, 0.1, 0.1\n",
    "dslen = len(dataset)\n",
    "tri = round(tr*dslen)\n",
    "tsi = round((tr+ts)*dslen)\n",
    "train_dataset = dataset[:tri]\n",
    "test_dataset = dataset[tri:tsi]\n",
    "val_dataset = dataset[tsi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)\n",
    "test_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)\n",
    "val_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 92], x=[40, 7], edge_attr=[92, 4], y=[2], batch=[40], ptr=[3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "edge_index tensor([[ 0,  0,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  5,  5,  5,  6,  6,  6,\n",
      "          7,  8,  9,  9, 10, 10, 10, 11, 11, 11, 12, 12, 13, 13, 14, 14, 14, 15,\n",
      "         15, 16, 17, 17, 18, 18, 19, 19, 20, 20, 20, 21, 21, 21, 22, 22, 23, 23,\n",
      "         23, 24, 24, 25, 25, 26, 26, 27, 27, 27, 28, 28, 28, 29, 29, 29, 30, 30,\n",
      "         30, 31, 31, 31, 32, 32, 33, 33, 34, 34, 34, 35, 35, 36, 36, 37, 37, 37,\n",
      "         38, 39],\n",
      "        [ 1,  5,  0,  2,  1,  3, 11,  2,  4,  9,  3,  5,  0,  4,  6,  5,  7,  8,\n",
      "          6,  6,  3, 10,  9, 11, 15,  2, 10, 12, 11, 13, 12, 14, 13, 15, 16, 10,\n",
      "         14, 14, 18, 22, 17, 19, 18, 20, 19, 21, 30, 20, 22, 23, 17, 21, 21, 24,\n",
      "         28, 23, 25, 24, 26, 25, 27, 26, 28, 36, 23, 27, 29, 28, 30, 34, 20, 29,\n",
      "         31, 30, 32, 37, 31, 33, 32, 34, 29, 33, 35, 34, 36, 27, 35, 31, 38, 39,\n",
      "         37, 37]])\n",
      "batch tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.]])\n",
      "ptr tensor([ 0, 17, 40])\n"
     ]
    }
   ],
   "source": [
    "batch1 = None\n",
    "for batch in train_loader:\n",
    "    batch1 = batch\n",
    "    break\n",
    "print(batch1)\n",
    "print(batch1.batch)\n",
    "print(\"edge_index\", batch1.edge_index)\n",
    "print(\"batch\",batch1.edge_attr)\n",
    "print(\"ptr\",batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([0, 0])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([0, 0])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([0, 0])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([0, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([0, 0])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "tensor([0, 0])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([0, 0])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "val loader\n",
      "tensor([0, 0])\n",
      "tensor([0, 1])\n",
      "tensor([1, 0])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 1])\n",
      "tensor([0, 0])\n",
      "tensor([1])\n",
      "test loader\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([1, 0])\n",
      "tensor([1, 1])\n",
      "tensor([0, 1])\n",
      "tensor([0, 1])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "print('train loader')\n",
    "for data in train_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('val loader')\n",
    "for data in val_loader:\n",
    "    print(data.y)\n",
    "    \n",
    "print('test loader')\n",
    "for data in test_loader:\n",
    "    print(data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # classification layer\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv3(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base(\n",
       "  (conv1): GCNConv(7, 64)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (conv3): GCNConv(64, 64)\n",
       "  (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = Base(dataset, 64)\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4234, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_base(model, loader, experiment_mode=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(h, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss\n",
    "    #     print(h[0])\n",
    "    # print(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_base(model, loader, experiment_mode=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        pred = h.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct/len(loader.dataset)\n",
    "\n",
    "base = Base(dataset, 64)\n",
    "train_base(base, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 100\n",
    "\n",
    "# base = Base(dataset, 64)\n",
    "# train_base(base, train_loader)\n",
    "\n",
    "# # Train\n",
    "# for _ in range(epoch):\n",
    "#     loss = round(train_base(base, train_loader).item(), 2)\n",
    "#     train_acc = round(test_base(base, train_loader), 2)\n",
    "#     val_acc = round(test_base(base, val_loader), 2)\n",
    "    \n",
    "#     print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; test_acc: {val_acc}')\n",
    "\n",
    "# # Test\n",
    "# test = test_base(base, test_loader)\n",
    "# print(f'Accuracy: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 92], x=[40, 7], edge_attr=[92, 4], y=[2], batch=[40], ptr=[3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "edge_index tensor([[ 0,  0,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  5,  5,  5,  6,  6,  6,\n",
      "          7,  8,  9,  9, 10, 10, 10, 11, 11, 11, 12, 12, 13, 13, 14, 14, 14, 15,\n",
      "         15, 16, 17, 17, 18, 18, 19, 19, 20, 20, 20, 21, 21, 21, 22, 22, 23, 23,\n",
      "         23, 24, 24, 25, 25, 26, 26, 27, 27, 27, 28, 28, 28, 29, 29, 29, 30, 30,\n",
      "         30, 31, 31, 31, 32, 32, 33, 33, 34, 34, 34, 35, 35, 36, 36, 37, 37, 37,\n",
      "         38, 39],\n",
      "        [ 1,  5,  0,  2,  1,  3, 11,  2,  4,  9,  3,  5,  0,  4,  6,  5,  7,  8,\n",
      "          6,  6,  3, 10,  9, 11, 15,  2, 10, 12, 11, 13, 12, 14, 13, 15, 16, 10,\n",
      "         14, 14, 18, 22, 17, 19, 18, 20, 19, 21, 30, 20, 22, 23, 17, 21, 21, 24,\n",
      "         28, 23, 25, 24, 26, 25, 27, 26, 28, 36, 23, 27, 29, 28, 30, 34, 20, 29,\n",
      "         31, 30, 32, 37, 31, 33, 32, 34, 29, 33, 35, 34, 36, 27, 35, 31, 38, 39,\n",
      "         37, 37]])\n",
      "batch tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.]])\n",
      "ptr tensor([ 0, 17, 40])\n"
     ]
    }
   ],
   "source": [
    "batch1 = None\n",
    "for batch in train_loader:\n",
    "    batch1 = batch\n",
    "    break\n",
    "print(batch1)\n",
    "print(batch1.batch)\n",
    "print(\"edge_index\", batch1.edge_index)\n",
    "print(\"batch\",batch1.edge_attr)\n",
    "print(\"ptr\",batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39)\n",
      "tensor(39)\n",
      "tensor([[ 0,  0,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  5,  5,  5,  6,  6,  6,\n",
      "          7,  8,  9,  9, 10, 10, 10, 11, 11, 11, 12, 12, 13, 13, 14, 14, 14, 15,\n",
      "         15, 16],\n",
      "        [ 1,  5,  0,  2,  1,  3, 11,  2,  4,  9,  3,  5,  0,  4,  6,  5,  7,  8,\n",
      "          6,  6,  3, 10,  9, 11, 15,  2, 10, 12, 11, 13, 12, 14, 13, 15, 16, 10,\n",
      "         14, 14]])\n",
      "tensor([ 0, 17, 40]) ; len: 3\n"
     ]
    }
   ],
   "source": [
    "print(max(batch1.edge_index[0]))\n",
    "print(max(batch1.edge_index[1]))\n",
    "print((dataset[0].edge_index))\n",
    "print((batch1.ptr), '; len:', len(batch1.ptr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below --> Subgraph extractor with batch information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "batch1 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering label [0 0 0 0 0 0 1 0 0 0 1 2 2 2 2 1 2]\n",
      "{0: [0, 1, 2, 3, 4, 5, 7, 8, 9], 1: [6, 10, 15], 2: [11, 12, 13, 14, 16]}\n",
      "all com [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 2, 2, 1, 2]\n",
      "clustering label [0 1 1 3 0 0 2 2 2 2 2 3 3 1 3 3 3 3 3 2 1 3 3]\n",
      "{0: [0, 4, 5], 1: [1, 2, 13, 20], 3: [3, 11, 12, 14, 15, 16, 17, 18, 21, 22], 2: [6, 7, 8, 9, 10, 19]}\n",
      "all com [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 2, 2, 1, 2, 0, 1, 1, 3, 0, 0, 2, 2, 2, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 2, 1, 3, 3]\n",
      "clustering label [0 0 0 0 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
      "{0: [0, 1, 2, 3], 1: [4, 5, 6], 2: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}\n",
      "all com [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 2, 2, 1, 2, 0, 1, 1, 3, 0, 0, 2, 2, 2, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 2, 1, 3, 3, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "clustering label [0 0 0 0 1 1 1 1 1 2 1 1 0 0 3 1 2 2 3 3 2 1 1]\n",
      "{0: [0, 1, 2, 3, 12, 13], 1: [4, 5, 6, 7, 8, 10, 11, 15, 21, 22], 2: [9, 16, 17, 20], 3: [14, 18, 19]}\n",
      "all com [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 2, 2, 1, 2, 0, 1, 1, 3, 0, 0, 2, 2, 2, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 2, 1, 3, 3, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 3, 1, 2, 2, 3, 3, 2, 1, 1]\n",
      "clustering label [1 0 0 0 1 1 1 2 3 0 3 3 3 1 2 2]\n",
      "{1: [0, 4, 5, 6, 13], 0: [1, 2, 3, 9], 2: [7, 14, 15], 3: [8, 10, 11, 12]}\n",
      "all com [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 2, 2, 1, 2, 0, 1, 1, 3, 0, 0, 2, 2, 2, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 2, 1, 3, 3, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 3, 1, 2, 2, 3, 3, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2, 3, 0, 3, 3, 3, 1, 2, 2]\n",
      "batch communities {0: {0: [0, 1, 2, 3, 4, 5, 7, 8, 9], 1: [6, 10, 15], 2: [11, 12, 13, 14, 16]}, 1: {0: [0, 4, 5], 1: [1, 2, 13, 20], 3: [3, 11, 12, 14, 15, 16, 17, 18, 21, 22], 2: [6, 7, 8, 9, 10, 19]}, 2: {0: [0, 1, 2, 3], 1: [4, 5, 6], 2: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}, 3: {0: [0, 1, 2, 3, 12, 13], 1: [4, 5, 6, 7, 8, 10, 11, 15, 21, 22], 2: [9, 16, 17, 20], 3: [14, 18, 19]}, 4: {1: [0, 4, 5, 6, 13], 0: [1, 2, 3, 9], 2: [7, 14, 15], 3: [8, 10, 11, 12]}}\n",
      "batch loop\n",
      "\n",
      "==== BATCH 0 ====\n",
      "lower bound 0\n",
      "len communities on this batch 3\n",
      "break new community 0\n",
      "break new community 1\n",
      "break new community 2\n",
      "pool subgraph ===  [[-7.45563317e-02 -1.08583162e-01  2.85370114e-02  5.73148852e-02\n",
      "  -5.07947227e-03  3.06421496e-02 -1.02712674e-01  1.20645481e-01\n",
      "  -3.83779280e-02  3.19082189e-02  7.82428078e-02 -5.51000005e-02\n",
      "   5.63788435e-03  1.18052648e-01 -6.45821446e-02  1.68163990e-02\n",
      "   1.23708727e-01  6.69716754e-02  4.18599395e-03 -1.33591101e-01\n",
      "  -3.88509412e-02  8.34563279e-02  7.59509487e-02  1.73310599e-01\n",
      "  -3.23809536e-03  1.17532526e-01  2.78548676e-03 -5.51677808e-02\n",
      "   1.12183822e-01 -2.26858016e-01 -4.86638288e-03 -3.52514241e-02\n",
      "   2.24720421e-02  3.65363046e-04  1.74384742e-02  3.42574097e-02\n",
      "  -1.53983744e-01  1.86991067e-02 -1.42017666e-01  7.56818021e-02\n",
      "   3.66131278e-02  1.25089102e-01  8.17746143e-02  1.07636506e-01\n",
      "  -7.92890632e-04  6.02027993e-02  1.64327261e-02  1.05313026e-01\n",
      "  -4.94573853e-02  2.24725574e-01 -1.19963778e-01 -5.45233186e-02\n",
      "   6.43392863e-02  1.22072768e-01  7.17721006e-02  5.83449846e-02\n",
      "   7.52166501e-02 -5.55247450e-02 -1.29407075e-02 -9.70010981e-02\n",
      "   2.93612530e-02 -5.02632476e-02 -3.46229370e-03  2.56266380e-01]\n",
      " [-7.25424668e-02 -1.17377929e-01  3.08031924e-02  6.48037319e-02\n",
      "  -6.05557176e-03  3.39286340e-02 -1.13217242e-01  1.20057564e-01\n",
      "  -3.85804276e-02  3.53969932e-02  8.92906785e-02 -6.00560357e-02\n",
      "   1.26397858e-04  1.33226956e-01 -6.79197982e-02  1.67117842e-02\n",
      "   1.40287489e-01  7.08908575e-02 -1.06547152e-03 -1.41199493e-01\n",
      "  -4.44284411e-02  9.08891199e-02  8.53685861e-02  1.84389176e-01\n",
      "  -6.88944407e-03  1.30473574e-01  2.14963748e-03 -5.35984070e-02\n",
      "   1.16235350e-01 -2.41418280e-01 -7.56810466e-03 -3.86008651e-02\n",
      "   2.99554719e-02  1.16412194e-03  1.67542833e-02  3.64822615e-02\n",
      "  -1.66631388e-01  1.65975317e-02 -1.55303071e-01  7.70958227e-02\n",
      "   3.83416073e-02  1.37901654e-01  8.43870031e-02  1.16553351e-01\n",
      "  -3.36212215e-03  6.48254367e-02  1.62991242e-02  1.16847664e-01\n",
      "  -5.57901412e-02  2.52764404e-01 -1.34451916e-01 -6.14511160e-02\n",
      "   7.56044984e-02  1.29333484e-01  8.07061171e-02  6.41484894e-02\n",
      "   8.55033994e-02 -6.08180501e-02 -1.16294430e-02 -1.05443728e-01\n",
      "   3.23351715e-02 -6.12043577e-02 -5.41891033e-03  2.74856572e-01]\n",
      " [-8.98994364e-02 -1.28153260e-01  1.91843715e-02  5.41176651e-02\n",
      "  -2.58179540e-03  2.88590256e-02 -1.10499360e-01  1.36333321e-01\n",
      "  -5.16270146e-02  1.19583161e-02  6.59656808e-02 -6.74092039e-02\n",
      "   2.69917501e-02  1.25158092e-01 -7.60258630e-02  4.25959588e-03\n",
      "   1.23503178e-01  8.68897840e-02  5.14223436e-03 -1.58654514e-01\n",
      "  -5.68885267e-02  8.46192375e-02  7.35151246e-02  1.88998348e-01\n",
      "  -1.59159072e-02  1.25404501e-01  1.33125498e-02 -4.98141931e-02\n",
      "   1.44997604e-01 -2.54628134e-01 -2.05246838e-02 -3.33079200e-02\n",
      "   1.64167561e-02  1.46256714e-03  2.33829122e-02  3.11316811e-02\n",
      "  -1.80046692e-01  2.91348573e-02 -1.39745352e-01  8.14014308e-02\n",
      "   3.54018148e-02  1.32327047e-01  7.59846084e-02  1.47814944e-01\n",
      "  -8.63054981e-03  5.99928413e-02  1.44099286e-02  1.24261303e-01\n",
      "  -5.30728161e-02  2.45498908e-01 -1.24702699e-01 -5.70738368e-02\n",
      "   7.48712450e-02  1.45179699e-01  8.83052200e-02  3.19480976e-02\n",
      "   8.07433277e-02 -7.47500226e-02 -4.04816411e-02 -1.10058045e-01\n",
      "   2.63869192e-02 -3.97762284e-02 -2.05184769e-02  2.98322940e-01]]\n",
      "Pool size  (3, 64)\n",
      "\n",
      "==== BATCH 1 ====\n",
      "lower bound 17\n",
      "len communities on this batch 4\n",
      "break new community 0\n",
      "break new community 1\n",
      "break new community 3\n",
      "break new community 2\n",
      "pool subgraph ===  [[-0.12144645 -0.15307883  0.02319898  0.07499442 -0.01164866  0.0328696\n",
      "  -0.13569657  0.1812034  -0.06392801  0.01948284  0.09337158 -0.08753499\n",
      "   0.03858336  0.14447287 -0.08023492  0.01473026  0.15844384  0.08890502\n",
      "   0.0197903  -0.19063146 -0.06305069  0.11163729  0.0820257   0.23767805\n",
      "  -0.00922363  0.1643831   0.01677768 -0.0729273   0.17790319 -0.32092146\n",
      "  -0.01171769 -0.03615575  0.01405685  0.00316906  0.03133016  0.04214125\n",
      "  -0.22115064  0.03666062 -0.17314935  0.10313381  0.04579801  0.16188014\n",
      "   0.1121131   0.16794979 -0.00658807  0.07831855  0.02428093  0.13659222\n",
      "  -0.06019838  0.29590405 -0.13706381 -0.05983061  0.09000779  0.17051143\n",
      "   0.10428912  0.05316568  0.08443372 -0.08009291 -0.0344092  -0.1260954\n",
      "   0.03564219 -0.0427734  -0.01304923  0.36143678]\n",
      " [-0.07478761 -0.11300908  0.02799896  0.06156468 -0.00606807  0.03092885\n",
      "  -0.1076082   0.12144163 -0.03820506  0.03160288  0.08352658 -0.05910543\n",
      "   0.00498984  0.12392363 -0.06498208  0.01751267  0.13180969  0.06691728\n",
      "   0.0031194  -0.13716359 -0.04194513  0.08830398  0.07886571  0.18020546\n",
      "  -0.00499732  0.1260018   0.00253249 -0.05509962  0.11616865 -0.23502565\n",
      "  -0.00616457 -0.03538097  0.02520239  0.00124826  0.01788523  0.03468265\n",
      "  -0.1618245   0.017855   -0.14822338  0.07633639  0.03686114  0.13124739\n",
      "   0.08413655  0.11257778 -0.0018566   0.06380007  0.01584005  0.1104454\n",
      "  -0.0508999   0.23832884 -0.12391591 -0.05614237  0.07006116  0.12492912\n",
      "   0.07612391  0.05804534  0.07785358 -0.05911463 -0.0136253  -0.10018854\n",
      "   0.03121344 -0.05308251 -0.00532575  0.26553788]\n",
      " [-0.10536694 -0.1328112   0.02012744  0.06506518 -0.01010639  0.02851768\n",
      "  -0.11773036  0.15721209 -0.05546397  0.01690331  0.08100919 -0.07594535\n",
      "   0.03347492  0.12534465 -0.06961183  0.01277997  0.13746591  0.07713401\n",
      "   0.01717007 -0.1653919  -0.0547028   0.09685653  0.07116549  0.20620951\n",
      "  -0.00800241  0.14261878  0.01455632 -0.06327173  0.15434883 -0.27843147\n",
      "  -0.01016627 -0.03136872  0.01219572  0.00274949  0.02718206  0.03656176\n",
      "  -0.19187031  0.03180675 -0.1502244   0.08947891  0.03973438  0.14044721\n",
      "   0.09726933  0.14571325 -0.00571579  0.06794921  0.02106612  0.11850742\n",
      "  -0.05222814  0.25672633 -0.11891655 -0.05190907  0.07809076  0.14793572\n",
      "   0.09048125  0.04612653  0.07325468 -0.06948862 -0.02985342 -0.10940039\n",
      "   0.03092318 -0.03711021 -0.01132152  0.3135826 ]\n",
      " [-0.07529056 -0.10820412  0.02954316  0.06004617 -0.00489263  0.03211519\n",
      "  -0.10444152  0.11983059 -0.04245402  0.03422525  0.07667367 -0.05409403\n",
      "   0.00375444  0.11935922 -0.06706411  0.01264718  0.12599914  0.07074341\n",
      "   0.00235131 -0.13331052 -0.03883804  0.08339804  0.07586008  0.17310885\n",
      "  -0.00487573  0.11897427  0.00362211 -0.05227326  0.11776986 -0.23126653\n",
      "  -0.00524959 -0.03436635  0.02080553  0.00201521  0.01928308  0.03543396\n",
      "  -0.15470013  0.02069807 -0.13977725  0.077718    0.03582571  0.12625722\n",
      "   0.08238837  0.11055264 -0.000771    0.06114331  0.01745582  0.10444894\n",
      "  -0.05012827  0.22761292 -0.12044835 -0.05793098  0.065583    0.12184495\n",
      "   0.07550387  0.06051153  0.0762182  -0.05382964 -0.01468671 -0.09843456\n",
      "   0.02985408 -0.04958635 -0.00643891  0.25877117]]\n",
      "Pool size  (4, 64)\n",
      "\n",
      "==== BATCH 2 ====\n",
      "lower bound 40\n",
      "len communities on this batch 3\n",
      "break new community 0\n",
      "break new community 1\n",
      "break new community 2\n",
      "pool subgraph ===  [[-0.10075533 -0.12699844  0.01924652  0.06221746 -0.00966405  0.02726953\n",
      "  -0.11257765  0.15033141 -0.05303646  0.01616351  0.07746366 -0.07262146\n",
      "   0.03200984  0.1198587  -0.06656512  0.01222065  0.13144942  0.07375807\n",
      "   0.0164186  -0.15815319 -0.05230862  0.09261738  0.06805078  0.19718431\n",
      "  -0.00765217  0.1363768   0.01391925 -0.06050254  0.14759343 -0.26624537\n",
      "  -0.0097213  -0.02999583  0.01166196  0.00262914  0.02599238  0.03496155\n",
      "  -0.18347272  0.03041467 -0.14364952  0.08556269  0.03799532  0.13430026\n",
      "   0.09301216  0.1393358  -0.00546563  0.06497525  0.02014413  0.11332068\n",
      "  -0.04994224  0.24549019 -0.11371194 -0.04963714  0.07467296  0.141461\n",
      "   0.08652115  0.04410772  0.07004854 -0.06644731 -0.02854683 -0.10461225\n",
      "   0.02956976 -0.035486   -0.01082598  0.29985803]\n",
      " [-0.10996245 -0.13860371  0.02100528  0.06790294 -0.01054716  0.02976144\n",
      "  -0.12286508  0.16406882 -0.05788301  0.01764053  0.08454237 -0.07925767\n",
      "   0.03493491  0.1308115  -0.07264791  0.01333737  0.14346142  0.08049819\n",
      "   0.01791895 -0.17260538 -0.05708865  0.10108085  0.07426935  0.21520324\n",
      "  -0.00835145  0.14883903  0.01519118 -0.06603131  0.16108066 -0.29057516\n",
      "  -0.01060967 -0.03273686  0.01272764  0.0028694   0.02836758  0.03815639\n",
      "  -0.20023864  0.033194   -0.15677636  0.09338149  0.04146737  0.14657276\n",
      "   0.10151168  0.15206846 -0.00596511  0.07091275  0.0219849   0.12367606\n",
      "  -0.05450604  0.26792336 -0.12410306 -0.05417303  0.08149665  0.1543879\n",
      "   0.09442758  0.04813834  0.07644965 -0.07251931 -0.03115548 -0.11417184\n",
      "   0.03227186 -0.03872876 -0.01181528  0.32725937]\n",
      " [-0.07930576 -0.11497914  0.02820391  0.06392096 -0.00440765  0.03198102\n",
      "  -0.11031344  0.12593274 -0.04591544  0.03133508  0.07849711 -0.05837317\n",
      "   0.00684081  0.1222322  -0.072336    0.01473127  0.13458309  0.07329399\n",
      "   0.00378241 -0.14147495 -0.04304416  0.09215255  0.07816776  0.18574205\n",
      "  -0.00691601  0.1278495   0.00335859 -0.05676414  0.130799   -0.24673222\n",
      "  -0.00629508 -0.03324921  0.01983656  0.00376363  0.02270384  0.03549595\n",
      "  -0.16816804  0.02065557 -0.1482101   0.08356941  0.03602049  0.13273509\n",
      "   0.08932717  0.11833143  0.00091777  0.0677267   0.01512465  0.11062265\n",
      "  -0.04996543  0.23982674 -0.12265789 -0.06064217  0.06960489  0.12702222\n",
      "   0.08145715  0.05580778  0.07658243 -0.06066876 -0.02072472 -0.10390984\n",
      "   0.03200495 -0.04722512 -0.01022892  0.27432915]]\n",
      "Pool size  (3, 64)\n",
      "\n",
      "==== BATCH 3 ====\n",
      "lower bound 57\n",
      "len communities on this batch 4\n",
      "break new community 0\n",
      "break new community 1\n",
      "break new community 2\n",
      "break new community 3\n",
      "pool subgraph ===  [[-0.1151654  -0.14516181  0.02199917  0.07111583 -0.01104623  0.03116964\n",
      "  -0.12867854  0.17183181 -0.06062175  0.0184752   0.08854252 -0.08300779\n",
      "   0.03658788  0.13700091 -0.0760853   0.01396841  0.15024938  0.08430698\n",
      "   0.01876676 -0.18077227 -0.05978981  0.10586359  0.07778342  0.22538568\n",
      "  -0.00874659  0.15588142  0.01590996 -0.06915558  0.16870231 -0.30432385\n",
      "  -0.01111165 -0.03428581  0.01332983  0.00300519  0.02970983  0.03996177\n",
      "  -0.20971302  0.03476458 -0.16419431  0.09779988  0.04342941  0.15350791\n",
      "   0.10631476  0.15926366 -0.00624733  0.07426805  0.02302513  0.12952787\n",
      "  -0.05708502  0.28060026 -0.12997504 -0.05673629  0.08535272  0.16169282\n",
      "   0.09889544  0.05041602  0.08006692 -0.07595062 -0.0326296  -0.11957394\n",
      "   0.03379885 -0.04056122 -0.01237436  0.34274382]\n",
      " [-0.07778822 -0.1101656   0.02531034  0.05696518 -0.0071211   0.0298418\n",
      "  -0.102074    0.12297589 -0.04074887  0.02723526  0.0774537  -0.05853335\n",
      "   0.01179387  0.11691902 -0.06078548  0.01334209  0.12286889  0.06572919\n",
      "   0.00580095 -0.13458084 -0.04267946  0.08176781  0.07113184  0.17086788\n",
      "  -0.00462249  0.11830395  0.0069116  -0.05214972  0.11415539 -0.22835467\n",
      "  -0.00677508 -0.03404611  0.0198489   0.00096426  0.01786599  0.03414238\n",
      "  -0.15559481  0.02115796 -0.13677643  0.07321801  0.03632514  0.1239773\n",
      "   0.07991696  0.11232103 -0.0045821   0.05673849  0.01837038  0.10392062\n",
      "  -0.04979619  0.22500526 -0.11555265 -0.05119148  0.06713326  0.12300126\n",
      "   0.07456888  0.05619866  0.0730671  -0.05466188 -0.01357318 -0.09498261\n",
      "   0.02809633 -0.04836388 -0.00386139  0.2586469 ]\n",
      " [-0.07330108 -0.11113535  0.02771501  0.06064675 -0.00592551  0.03052651\n",
      "  -0.10594726  0.11922365 -0.03742257  0.03136441  0.08238368 -0.05803398\n",
      "   0.00451757  0.12215524 -0.06399999  0.01733237  0.12987029  0.06582906\n",
      "   0.00287716 -0.13483019 -0.04117337  0.0869375   0.07786169  0.17729623\n",
      "  -0.00488442  0.12398969  0.00232713 -0.05420696  0.11399105 -0.23109748\n",
      "  -0.00602113 -0.03493841  0.02503034  0.00120946  0.01750174  0.03416683\n",
      "  -0.15911756  0.01740626 -0.14610397  0.075074    0.03630054  0.12926592\n",
      "   0.08276426  0.11052203 -0.00177598  0.06284143  0.01554283  0.10877347\n",
      "  -0.05016305  0.23470687 -0.12223821 -0.05541003  0.06895944  0.12284201\n",
      "   0.07484737  0.05739457  0.07682009 -0.05813427 -0.01320412 -0.0986451\n",
      "   0.03077718 -0.05255894 -0.00516605  0.26111382]\n",
      " [-0.10967193 -0.13823751  0.02094978  0.06772354 -0.01051929  0.02968282\n",
      "  -0.12254047  0.16363535 -0.05773007  0.01759392  0.084319   -0.07904828\n",
      "   0.03484262  0.13046589 -0.07245597  0.01330213  0.14308239  0.08028549\n",
      "   0.0178716  -0.17214936 -0.05693781  0.1008138   0.07407312  0.21463466\n",
      "  -0.00832938  0.1484458   0.01515105 -0.06585685  0.16065508 -0.28980745\n",
      "  -0.01058165 -0.03265037  0.01269401  0.00286181  0.02829263  0.03805558\n",
      "  -0.19970961  0.03310631 -0.15636215  0.09313477  0.04135781  0.14618552\n",
      "   0.10124348  0.15166669 -0.00594935  0.0707254   0.02192682  0.12334931\n",
      "  -0.05436203  0.26721551 -0.12377518 -0.0540299   0.08128133  0.15398\n",
      "   0.09417809  0.04801115  0.07624767 -0.07232771 -0.03107316 -0.11387019\n",
      "   0.03218659 -0.03862644 -0.01178407  0.32639473]]\n",
      "Pool size  (4, 64)\n",
      "\n",
      "==== BATCH 4 ====\n",
      "lower bound 80\n",
      "len communities on this batch 4\n",
      "break new community 1\n",
      "break new community 0\n",
      "break new community 2\n",
      "break new community 3\n",
      "pool subgraph ===  [[-1.03541199e-01 -1.31017487e-01  2.18766308e-02  6.50958149e-02\n",
      "  -8.73395952e-03  2.89426078e-02 -1.17569888e-01  1.55100070e-01\n",
      "  -5.46871852e-02  1.98602513e-02  8.00542366e-02 -7.34102549e-02\n",
      "   2.97776302e-02  1.24780942e-01 -7.24126734e-02  1.39080225e-02\n",
      "   1.36980060e-01  7.81489480e-02  1.61084128e-02 -1.63685903e-01\n",
      "  -5.21263750e-02  9.72818732e-02  7.35011343e-02  2.06657980e-01\n",
      "  -7.72304647e-03  1.41320610e-01  1.21642738e-02 -6.45118933e-02\n",
      "   1.53867781e-01 -2.76965376e-01 -8.91738245e-03 -3.16414903e-02\n",
      "   1.30014699e-02  2.76962237e-03  2.75602238e-02  3.65398824e-02\n",
      "  -1.89893916e-01  3.06619676e-02 -1.51819602e-01  9.13413167e-02\n",
      "   3.95121826e-02  1.40167793e-01  9.81431603e-02  1.42472962e-01\n",
      "  -2.99664040e-03  6.98908539e-02  1.98032623e-02  1.18236667e-01\n",
      "  -5.14077591e-02  2.54679140e-01 -1.20366719e-01 -5.42680435e-02\n",
      "   7.56099122e-02  1.46283928e-01  8.87190402e-02  4.69041932e-02\n",
      "   7.38296472e-02 -6.90932469e-02 -3.04509047e-02 -1.10265171e-01\n",
      "   3.12536252e-02 -3.70424762e-02 -1.18890069e-02  3.11029054e-01]\n",
      " [-9.69768200e-02 -1.36988337e-01  2.87497178e-02  7.20264643e-02\n",
      "  -8.85532312e-03  3.46486289e-02 -1.27215207e-01  1.52041436e-01\n",
      "  -5.05249307e-02  3.03623829e-02  9.49356586e-02 -7.43944451e-02\n",
      "   1.67898282e-02  1.42772536e-01 -7.55433962e-02  1.73569970e-02\n",
      "   1.53628421e-01  8.03393207e-02  8.58902372e-03 -1.67451077e-01\n",
      "  -5.35576303e-02  1.03966256e-01  8.68437499e-02  2.15186046e-01\n",
      "  -7.37503659e-03  1.50692794e-01  8.20737761e-03 -6.51246201e-02\n",
      "   1.47238412e-01 -2.85523085e-01 -8.94020861e-03 -3.89565207e-02\n",
      "   2.37810990e-02  2.28514764e-03  2.39158064e-02  4.05217621e-02\n",
      "  -1.96636344e-01  2.58359838e-02 -1.69619992e-01  9.17486988e-02\n",
      "   4.32013098e-02  1.53742346e-01  1.00781039e-01  1.41853407e-01\n",
      "  -4.43724710e-03  7.37978205e-02  2.03852791e-02  1.29167388e-01\n",
      "  -5.90835340e-02  2.80464712e-01 -1.39922333e-01 -6.26617573e-02\n",
      "   8.40391248e-02  1.51578848e-01  9.33611497e-02  6.24136113e-02\n",
      "   8.74992371e-02 -7.10194610e-02 -2.12905839e-02 -1.17849514e-01\n",
      "   3.54184832e-02 -5.52409761e-02 -8.26205667e-03  3.22280040e-01]\n",
      " [-9.68718032e-03 -5.37976523e-02  4.06869277e-02  4.13927262e-02\n",
      "   1.27642381e-03  3.51968569e-02 -6.67191794e-02  3.74034162e-02\n",
      "  -9.04942490e-03  5.71512046e-02  6.72292237e-02 -1.30009238e-02\n",
      "  -4.64261221e-02  9.87969687e-02 -4.39521136e-02  1.10800965e-02\n",
      "   9.12023410e-02  4.35051943e-02 -2.48727308e-02 -5.92025345e-02\n",
      "  -1.13608086e-02  4.59809812e-02  7.29320943e-02  8.56997420e-02\n",
      "   2.48413704e-03  6.11232445e-02 -1.24054463e-02 -2.25070473e-02\n",
      "   2.30536349e-02 -1.11540613e-01  1.81033484e-03 -3.94968595e-02\n",
      "   3.75011570e-02 -1.51327144e-03 -2.81788036e-03  2.95553046e-02\n",
      "  -6.79182385e-02 -3.44531673e-03 -1.03737754e-01  3.71976861e-02\n",
      "   2.68465212e-02  8.59740227e-02  3.93415131e-02  3.31411637e-02\n",
      "  -1.61986488e-04  3.27859583e-02  1.20002972e-02  6.79670945e-02\n",
      "  -4.45052062e-02  1.51912133e-01 -1.10090914e-01 -5.53433796e-02\n",
      "   4.11355793e-02  6.19800985e-02  3.94967857e-02  8.40405474e-02\n",
      "   7.49756123e-02 -1.75411815e-02  2.45160585e-02 -6.24989793e-02\n",
      "   2.29061525e-02 -7.57101501e-02  1.06929963e-02  1.26874139e-01]\n",
      " [-4.27819579e-03 -6.99499585e-02  4.95071132e-02  5.61829051e-02\n",
      "   2.60087784e-03  3.93504845e-02 -8.69284309e-02  3.94619191e-02\n",
      "   4.90750885e-04  6.90134028e-02  9.54372166e-02 -2.22381328e-02\n",
      "  -6.38169247e-02  1.29341802e-01 -5.31750815e-02  2.66190120e-02\n",
      "   1.24912633e-01  4.57556844e-02 -3.30618394e-02 -7.45840017e-02\n",
      "  -1.53322143e-02  6.89145206e-02  1.00703154e-01  1.22216938e-01\n",
      "   2.62332990e-03  9.08356756e-02 -2.42277115e-02 -3.47152520e-02\n",
      "   2.71862957e-02 -1.39600115e-01  3.09241150e-03 -4.77193724e-02\n",
      "   5.83543559e-02 -1.86816382e-03 -5.85076725e-03  3.18396818e-02\n",
      "  -9.44846012e-02 -1.27544664e-02 -1.46723159e-01  4.50149281e-02\n",
      "   3.13909263e-02  1.15260916e-01  5.49808200e-02  3.80856595e-02\n",
      "   3.83816211e-03  5.37238326e-02  6.50706922e-03  9.49034188e-02\n",
      "  -5.19883912e-02  2.07037102e-01 -1.42675551e-01 -6.83652414e-02\n",
      "   5.66323651e-02  7.48466402e-02  4.66824435e-02  9.36770951e-02\n",
      "   9.38878525e-02 -3.51252118e-02  2.85773734e-02 -8.11912566e-02\n",
      "   3.24905575e-02 -9.58848372e-02  1.06983993e-02  1.61382869e-01]]\n",
      "Pool size  (4, 64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "\n",
    "\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # embeddings for subgraph\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # classification layer\n",
    "        self.lin = Linear(hidden_channels*2, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, ptr):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        # embedding = embedding.relu()\n",
    "        # embedding = self.conv3(embedding, edge_index)\n",
    "        # embedding = embedding.relu()\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        \n",
    "        subgraph_edge_index, communities, S, batch_communities = self.subgraph_generator(feature_emb, edge_index, batch, ptr)\n",
    "        subgraph_embedding = self.conv4(embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        subgraph_embedding = self.conv5(subgraph_embedding, subgraph_edge_index)\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        self.subgraph_pooling(subgraph_embedding, communities, batch, ptr, batch_communities)\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        subgraph_embedding = global_max_pool(subgraph_embedding, batch)\n",
    "        \n",
    "        \n",
    "        h = torch.cat((embedding, subgraph_embedding), 1)\n",
    "        \n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin(h)\n",
    "        h = h.relu()\n",
    "        x = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h, S, communities\n",
    "    \n",
    "    def subgraph_generator(self, embeddings, batch_edge_index, batch, ptr):\n",
    "        '''\n",
    "        Return subgraph_edge_index (edge_index of created subgraph)\n",
    "        '''\n",
    "        graph_counter = 0\n",
    "        edge_index = [[],[]]\n",
    "        subgraph_edge_index = [[],[]]\n",
    "        # Gs = []\n",
    "        sub_created = False\n",
    "        graph_bound = {}\n",
    "        all_communities = []\n",
    "        batch_communities = {}\n",
    "        S = []\n",
    "\n",
    "        for i in range(len(ptr)-1):\n",
    "            graph_bound[i] = [ptr[i].item(), ptr[i+1].item()]\n",
    "        \n",
    "        for i, (src, dst) in enumerate(zip(batch_edge_index[0], batch_edge_index[1])):\n",
    "            lower_bound = graph_bound[graph_counter][0]\n",
    "            upper_bound = graph_bound[graph_counter][1]\n",
    "            if ((src >= lower_bound and src < upper_bound) or\n",
    "                (dst >= lower_bound and dst < upper_bound)):\n",
    "                \n",
    "                edge_index[0].append(src - lower_bound)\n",
    "                edge_index[1].append(dst - lower_bound)\n",
    "            else:\n",
    "                sub_created = True\n",
    "                \n",
    "            if (i == len(batch_edge_index[0]) - 1) or sub_created:\n",
    "                sub_created = False\n",
    "                \n",
    "                embs = []\n",
    "                # make new graph\n",
    "                for i, (b, emb) in enumerate(zip(batch, embeddings)):\n",
    "                    if (b == graph_counter):\n",
    "                        embs.append(emb)\n",
    "                # print('emb x', embs)\n",
    "                G = data_transformation(edge_index, embs)\n",
    "                # dont need this at the moment\n",
    "                # Gs.append(G)\n",
    "                \n",
    "                # Calculate similarity matrix\n",
    "                S = calculate_similarity_matrix(G)\n",
    "                \n",
    "                # print('S matrix', S)\n",
    "                # AP Clustering        \n",
    "                clustering = AffinityPropagation(affinity='precomputed', damping=0.9, random_state=0, convergence_iter=15, max_iter=1000)\n",
    "                clustering.fit(S)\n",
    "                \n",
    "                print('clustering label', clustering.labels_)\n",
    "                \n",
    "                # Get community\n",
    "                communities = {}\n",
    "                for lab in clustering.labels_:\n",
    "                    communities[lab] = []\n",
    "                    all_communities.append(lab)\n",
    "                for nd, clust in enumerate(clustering.labels_):\n",
    "                    communities[clust].append(nd)\n",
    "                print(communities)\n",
    "                \n",
    "                edge_index = [[],[]]\n",
    "                batch_communities[graph_counter] = communities\n",
    "                \n",
    "                graph_counter+=1\n",
    "                \n",
    "                # Make subgraph edge_index\n",
    "                for c in communities:\n",
    "                    w = G.subgraph(communities[c])\n",
    "                    for sub in w.edges:\n",
    "                        subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "                        subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                \n",
    "                print('all com', all_communities)\n",
    "                # break # sementara aja\n",
    "        \n",
    "        # print('batch communities', batch_communities)\n",
    "        return torch.tensor(subgraph_edge_index), all_communities, S, batch_communities\n",
    "    \n",
    "        \n",
    "    def subgraph_pooling(self, embeddings, communities, batch, ptr, batch_communities):\n",
    "        # batch communities: batch -> communities -> member\n",
    "        pool_type = 'mean'\n",
    "        curr_batch = 0\n",
    "        emb_temp = None\n",
    "        emb_pool = []\n",
    "        print('batch communities', batch_communities)\n",
    "        \n",
    "        print('batch loop')\n",
    "        print('')\n",
    "        \n",
    "        # LOOP THROUGH BATCH\n",
    "        for b in batch_communities:\n",
    "            print(f'==== BATCH {b} ====')\n",
    "            print('lower bound', ptr[b].item())\n",
    "            print('len communities on this batch', len(batch_communities[b]))\n",
    "            \n",
    "            # initialize array\n",
    "            emb_temp = [[] for _ in range(len(batch_communities[b]))]\n",
    "            emb_pool = [[] for _ in range(len(batch_communities[b]))]\n",
    "            for comm in batch_communities[b]:\n",
    "                for member in batch_communities[b][comm]:\n",
    "                    # emb_temp[comm].append(member + ptr[b].item())\n",
    "                    index_used = member + ptr[b].item()\n",
    "                    emb_temp[comm].append(embeddings[index_used].detach().tolist())\n",
    "                    # print('embtemp-log', emb_temp)\n",
    "                    # print(comm, \"-\",member)\n",
    "                print('break new community', comm)\n",
    "\n",
    "                # Pooling per sub graph                \n",
    "                if pool_type == 'mean': # mean pool\n",
    "                    emb_pool[comm] = np.array(emb_temp[comm]).mean(axis=0)\n",
    "                elif pool_type == 'add': # add pool\n",
    "                    emb_pool[comm] = np.array(emb_temp[comm]).sum(axis=0)\n",
    "                else:\n",
    "                    print('TODO: fill later')\n",
    "                \n",
    "            print(\"pool subgraph === \", np.array(emb_pool))\n",
    "            print(\"Pool size \", np.array(emb_pool).shape)\n",
    "            print()\n",
    "            # emb_temp = np.array(emb_temp)\n",
    "            # print(\"emb_temp\", emb_temp)\n",
    "            # print(\"len emb_temp\", len(emb_temp))\n",
    "            # print(\"emb_temp average\")\n",
    "            # print(\"size emb_temp\", len(emb_temp))\n",
    "        # print(len(emb_temp))\n",
    "                    \n",
    "        # curr_batch+=1\n",
    "        # # print('num iteration', curr_batch)\n",
    "        # print(\"communities\", communities)\n",
    "        # print(\"batch\", batch)\n",
    "        # print(\"ptr\", ptr)\n",
    "        # print('calling subgraph pooling')\n",
    "        \n",
    "\n",
    "experiment = Experiment(dataset, 64)\n",
    "emb, h, S, communities = experiment(batch1.x, batch1.edge_index, batch1.batch, batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean [1.33333333 2.33333333 2.33333333]\n",
      "sum [4 7 7]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1,2,3],[1,3,2],[2,2,2]])\n",
    "\n",
    "print('mean', np.mean(test, axis=0))\n",
    "print('sum', np.sum(test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [2], []]\n"
     ]
    }
   ],
   "source": [
    "emb_temp = [[] for _ in range(3)]\n",
    "emb_temp[1].append(2)\n",
    "print(emb_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.size()\n",
    "# len(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(communities) == \n",
    "batch1.batch.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 3, 0, 1, 1, 2, 1, 2, 2, 3, 3, 3, 4, 3, 3, 3, 3, 4, 4,\n",
       "       4], dtype=int64)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering = AffinityPropagation(affinity='precomputed', damping=0.7, random_state=42, convergence_iter=15, max_iter=3000)\n",
    "clustering.fit(S)\n",
    "clustering.labels_\n",
    "# clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MUTAG(188)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expTrain(train_loader, val_loader, test_loader, epoch = 2):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "    experiment = Experiment(dataset, 64)\n",
    "\n",
    "    # Train\n",
    "    print('process training')\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(experiment, train_loader, True).item(), 5)\n",
    "        train_acc = round(test_base(experiment, train_loader, True), 5)\n",
    "        val_acc = round(test_base(experiment, val_loader, True), 5)\n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; test_acc: {val_acc}')\n",
    "\n",
    "    # Test\n",
    "    print('process testing')\n",
    "    test = test_base(experiment, test_loader, True)\n",
    "    print(f'Accuracy: {test}')\n",
    "\n",
    "# expTrain(train_loader, val_loader, test_loader, epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseTrain(train_loader, val_loader, test_loader, epoch = 10):\n",
    "    base = Base(dataset, 64)\n",
    "\n",
    "    # Train\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(base, train_loader).item(), 5)\n",
    "        train_acc = round(test_base(base, train_loader), 5)\n",
    "        val_acc = round(test_base(base, val_loader), 5)\n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; val_acc: {val_acc}; test: {round(test_base(base, test_loader), 2)}')\n",
    "\n",
    "    # Test\n",
    "    test = test_base(base, test_loader)\n",
    "    print(f'Accuracy: {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:round(len(dataset) * 0.8)]\n",
    "test_dataset = dataset[round(len(dataset) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/10\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.69358; train_acc: 0.7125; val_acc: 0.675; test: 0.81\n",
      "epoch 1; loss: 0.51878; train_acc: 0.72083; val_acc: 0.675; test: 0.73\n",
      "epoch 2; loss: 0.57897; train_acc: 0.71944; val_acc: 0.6375; test: 0.73\n",
      "epoch 3; loss: 0.55901; train_acc: 0.72778; val_acc: 0.6875; test: 0.74\n",
      "epoch 4; loss: 0.58857; train_acc: 0.70556; val_acc: 0.575; test: 0.68\n",
      "epoch 5; loss: 0.49834; train_acc: 0.7125; val_acc: 0.6625; test: 0.72\n",
      "epoch 6; loss: 0.55777; train_acc: 0.73472; val_acc: 0.7125; test: 0.77\n",
      "epoch 7; loss: 0.59016; train_acc: 0.74306; val_acc: 0.725; test: 0.76\n",
      "epoch 8; loss: 0.52072; train_acc: 0.74722; val_acc: 0.6625; test: 0.77\n",
      "epoch 9; loss: 0.58987; train_acc: 0.7125; val_acc: 0.75; test: 0.77\n",
      "Accuracy: 0.77\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 0; loss: 0.67796; train_acc: 0.475; test_acc: 0.575\n",
      "epoch 1; loss: 0.66832; train_acc: 0.65417; test_acc: 0.7125\n",
      "epoch 2; loss: 0.7253; train_acc: 0.60972; test_acc: 0.6625\n",
      "epoch 3; loss: 0.65472; train_acc: 0.67639; test_acc: 0.6\n",
      "epoch 4; loss: 0.55065; train_acc: 0.69861; test_acc: 0.625\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "train_dataset\n",
    "test_dataset\n",
    "k = 10\n",
    "\n",
    "splits = KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "k_counter = 0\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "    # print('Fold {}'.format(fold + 1))\n",
    "    # print(f'Fold',fold,'Train_idx',train_idx,'Val_idx',val_idx)\n",
    "    print(f'Fold {fold}/{k}')\n",
    "    #if k_counter > 2:\n",
    "    #    break\n",
    "    \n",
    "    fold_train = []\n",
    "    for key in train_idx:\n",
    "        fold_train.append(train_dataset[key])\n",
    "\n",
    "    fold_val = [] \n",
    "    for key in val_idx:\n",
    "        fold_val.append(train_dataset[key])\n",
    "\n",
    "    tr = DataLoader(fold_train, batch_size=batch_size, shuffle=True)\n",
    "    vd = DataLoader(fold_val, batch_size=batch_size, shuffle=True)\n",
    "    ts = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Base model\n",
    "    print(\"=== Base model ===\")\n",
    "    baseTrain(tr, vd, ts, 10)\n",
    "    print(\"=== Experiment model ===\")\n",
    "    expTrain(tr, vd, ts, 10)\n",
    "    \n",
    "    k_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
