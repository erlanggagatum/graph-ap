{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from preprocessing import data_transformation\n",
    "from similarity import calculate_similarity_matrix\n",
    "\n",
    "from model import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root='datasets/', name='MUTAG')\n",
    "torch.manual_seed(1234)\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split: Train test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```train_dataset```: for training model<br/>\n",
    "```val_dataset```: evaluate model for hyperparameter tunning<br/>\n",
    "```test_dataset```: testing model after complete training<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ts, vl = 0.8, 0.1, 0.1\n",
    "dslen = len(dataset)\n",
    "tri = round(tr*dslen)\n",
    "tsi = round((tr+ts)*dslen)\n",
    "train_dataset = dataset[:tri]\n",
    "test_dataset = dataset[tri:tsi]\n",
    "val_dataset = dataset[tsi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)\n",
    "test_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)\n",
    "val_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# batch1 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from similarity import calculate_similarity_matrix, testt\n",
    "\n",
    "\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels, k = 1):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # save number of subgraphs, default 1\n",
    "        self.k_subgraph = k\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # embeddings for subgraph\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # attention layer\n",
    "        self.query_layer = Linear(hidden_channels,hidden_channels)\n",
    "        self.key_layer = Linear(hidden_channels,hidden_channels)\n",
    "        self.value_layer = Linear(hidden_channels,hidden_channels)\n",
    "        \n",
    "        # classification layer\n",
    "        self.lin = Linear(hidden_channels*2, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, ptr):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        \n",
    "        subgraph_edge_index, communities, S, batch_communities = self.subgraph_generator(feature_emb, edge_index, batch, ptr)\n",
    "        subgraph_embedding = self.conv4(embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        subgraph_embedding = self.conv5(subgraph_embedding, subgraph_edge_index)\n",
    "        \n",
    "        # apply readout layer/pooling for each subgraphs\n",
    "        subgraph_pool_embedding = self.subgraph_pooling(subgraph_embedding, communities, batch, ptr, batch_communities)\n",
    "        # print(len(subgraph_pool_embedding))\n",
    "        # apply selective (top k) attention\n",
    "        topk_subgraph_embedding = self.selectk_subgraph(embedding, subgraph_pool_embedding, self.k_subgraph)\n",
    "        \n",
    "        # readout layer for original embedding\n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "                \n",
    "        combined_embeddings = torch.cat((embedding, topk_subgraph_embedding.view(len(embedding), -1)), 1)\n",
    "        \n",
    "        \n",
    "        h = F.dropout(combined_embeddings, p=0.3, training=self.training)\n",
    "        h = self.lin(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h, S, communities, topk_subgraph_embedding.view(len(embedding), -1)\n",
    "    \n",
    "    # checked\n",
    "    def selectk_subgraph(self, embs, sub_embs, k = 1):\n",
    "        # calculate attention and select top k subgraph\n",
    "        \n",
    "        topk_subgraphs_all = []\n",
    "\n",
    "        for i, (emb, sub_emb) in enumerate(zip(embs, sub_embs)):\n",
    "            sub = torch.tensor(sub_emb)\n",
    "            sub = sub.to(torch.float32)\n",
    "\n",
    "            # transform\n",
    "            query = self.query_layer(emb)\n",
    "            key = self.key_layer(sub)\n",
    "            value = self.value_layer(sub)\n",
    "\n",
    "            # att score\n",
    "            attention_score = torch.matmul(query, key.transpose(0,1))\n",
    "            attention_weight = F.softmax(attention_score, dim=0)\n",
    "            \n",
    "            # select topk\n",
    "            topk_subgraph_embeddings = None\n",
    "            \n",
    "            if (k <= len(sub)):\n",
    "                topk_scores, topk_indices = torch.topk(attention_weight, k)\n",
    "                topk_subgraph_embeddings = sub[topk_indices]\n",
    "            else:\n",
    "                print('too big')\n",
    "                \n",
    "            topk_subgraphs_all.append(topk_subgraph_embeddings)\n",
    "        \n",
    "        return torch.stack(topk_subgraphs_all)\n",
    "    \n",
    "    \n",
    "    def subgraph_generator(self, embeddings, batch_edge_index, batch, ptr):\n",
    "        '''\n",
    "        Return subgraph_edge_index (edge_index of created subgraph)\n",
    "        '''\n",
    "        graph_counter = 0\n",
    "        edge_index = [[],[]]\n",
    "        subgraph_edge_index = [[],[]]\n",
    "        # Gs = []\n",
    "        sub_created = False\n",
    "        graph_bound = {}\n",
    "        all_communities = []\n",
    "        batch_communities = {}\n",
    "        S = []\n",
    "\n",
    "        for i in range(len(ptr)-1):\n",
    "            graph_bound[i] = [ptr[i].item(), ptr[i+1].item()]\n",
    "        \n",
    "        for i, (src, dst) in enumerate(zip(batch_edge_index[0], batch_edge_index[1])):\n",
    "            lower_bound = graph_bound[graph_counter][0]\n",
    "            upper_bound = graph_bound[graph_counter][1]\n",
    "            if ((src >= lower_bound and src < upper_bound) or\n",
    "                (dst >= lower_bound and dst < upper_bound)):\n",
    "                \n",
    "                edge_index[0].append(src - lower_bound)\n",
    "                edge_index[1].append(dst - lower_bound)\n",
    "            else:\n",
    "                sub_created = True\n",
    "                \n",
    "            if (i == len(batch_edge_index[0]) - 1) or sub_created:\n",
    "                sub_created = False\n",
    "                \n",
    "                embs = []\n",
    "                # make new graph\n",
    "                for i, (b, emb) in enumerate(zip(batch, embeddings)):\n",
    "                    if (b == graph_counter):\n",
    "                        embs.append(emb)\n",
    "                \n",
    "                G = data_transformation(edge_index, embs)\n",
    "                # dont need this at the moment\n",
    "                # Gs.append(G)\n",
    "                \n",
    "                # Calculate similarity matrix\n",
    "                S = calculate_similarity_matrix(G)\n",
    "                \n",
    "                # AP Clustering        \n",
    "                clustering = AffinityPropagation(affinity='precomputed', damping=0.8, random_state=42, convergence_iter=15, max_iter=1000)\n",
    "                clustering.fit(S)\n",
    "                \n",
    "                \n",
    "                # Get community\n",
    "                communities = {}\n",
    "                for lab in clustering.labels_:\n",
    "                    communities[lab] = []\n",
    "                    all_communities.append(lab)\n",
    "                for nd, clust in enumerate(clustering.labels_):\n",
    "                    communities[clust].append(nd)\n",
    "                \n",
    "                edge_index = [[],[]]\n",
    "                batch_communities[graph_counter] = communities\n",
    "                \n",
    "                graph_counter+=1\n",
    "                \n",
    "                # Make subgraph edge_index\n",
    "                for c in communities:\n",
    "                    w = G.subgraph(communities[c])\n",
    "                    for sub in w.edges:\n",
    "                        subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "                        subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                \n",
    "        \n",
    "        # print('batch communities', batch_communities)\n",
    "        return torch.tensor(subgraph_edge_index), all_communities, S, batch_communities\n",
    "    \n",
    "        \n",
    "    # check autograd (done)\n",
    "    def subgraph_pooling(self, embeddings, communities, batch, ptr, batch_communities, pool_type = 'mean'):\n",
    "        # batch communities: batch (or graph in this batch) -> communities -> member        \n",
    "        all_emb_pool = []\n",
    "        \n",
    "        # LOOP THROUGH BATCH\n",
    "        for b in batch_communities:\n",
    "            \n",
    "            # initialize array\n",
    "            emb_pool = [None] * len(batch_communities[b])\n",
    "            for comm in batch_communities[b]:\n",
    "                emb_temp = []\n",
    "\n",
    "                for member in batch_communities[b][comm]:\n",
    "                    index_used = member + ptr[b].item()\n",
    "                    emb_temp.append(embeddings[index_used])\n",
    "\n",
    "                # Pooling per sub graph using PyTorch\n",
    "                emb_temp_tensor = torch.stack(emb_temp)\n",
    "                if pool_type == 'mean': # mean pool\n",
    "                    emb_pool[comm] = torch.mean(emb_temp_tensor, dim=0)\n",
    "                elif pool_type == 'add': # add pool\n",
    "                    emb_pool[comm] = torch.sum(emb_temp_tensor, dim=0)\n",
    "                else:\n",
    "                    print('TODO: fill later')\n",
    "                    \n",
    "            all_emb_pool.append(torch.stack(emb_pool))\n",
    "        return all_emb_pool\n",
    "\n",
    "# experiment = Experiment(dataset, 64)\n",
    "# emb, h, S, communities, sub_emb = experiment(batch1.x, batch1.edge_index, batch1.batch, batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base(model, loader, experiment_mode=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        if experiment_mode:\n",
    "            emb, h, S, communities, sub_emb = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(h, data.y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_base(model, loader, experiment_mode=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h, S, communities, sub_emb = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        pred = h.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expTrain(train_loader, val_loader, test_loader, epoch = 5):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "    experiment = Experiment(dataset, 64)\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    test_acc_history = []\n",
    "    \n",
    "    # Train\n",
    "    print('process training')\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(experiment, train_loader, True).item(), 5)\n",
    "        train_acc = round(test_base(experiment, train_loader, True), 5)\n",
    "        val_acc = round(test_base(experiment, val_loader, True), 5)\n",
    "        test_acc = test_base(experiment, test_loader, True)\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "        test_acc_history.append(test_acc)\n",
    "        \n",
    "        print(f'epoch {_+1}; loss: {loss}; train_acc: {train_acc}; val_acc: {val_acc}')\n",
    "\n",
    "    # Test\n",
    "    print('process testing')\n",
    "    # test = test_base(experiment, test_loader, True)\n",
    "    print(f'Accuracy: {test_acc}')\n",
    "    \n",
    "    return [loss_history, train_acc_history, val_acc_history, test_acc_history]\n",
    "\n",
    "# expTrain(train_loader, val_loader, test_loader, epoch = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[:round(len(dataset) * 0.9)]\n",
    "test_dataset = dataset[round(len(dataset) * 0.9):]\n",
    "print(train_dataset.y)\n",
    "print(test_dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/10\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.23742; train_acc: 0.32895; val_acc: 0.29412\n",
      "epoch 2; loss: 0.22217; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 3; loss: 0.21057; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 4; loss: 0.20538; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 5; loss: 0.19197; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 6; loss: 0.21372; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 7; loss: 0.2149; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 8; loss: 0.21882; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 9; loss: 0.20542; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 10; loss: 0.19194; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 11; loss: 0.28858; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 12; loss: 0.17726; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 13; loss: 0.19053; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 14; loss: 0.20298; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 15; loss: 0.19102; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 16; loss: 0.23815; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 17; loss: 0.21878; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 18; loss: 0.18464; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 19; loss: 0.22176; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 20; loss: 0.25426; train_acc: 0.67763; val_acc: 0.70588\n",
      "epoch 21; loss: 0.21947; train_acc: 0.67763; val_acc: 0.70588\n",
      "epoch 22; loss: 0.20977; train_acc: 0.69079; val_acc: 0.70588\n",
      "epoch 23; loss: 0.18366; train_acc: 0.69079; val_acc: 0.70588\n",
      "epoch 24; loss: 0.23159; train_acc: 0.69737; val_acc: 0.70588\n",
      "epoch 25; loss: 0.19053; train_acc: 0.69079; val_acc: 0.70588\n",
      "epoch 26; loss: 0.16992; train_acc: 0.69079; val_acc: 0.70588\n",
      "epoch 27; loss: 0.2092; train_acc: 0.66447; val_acc: 0.70588\n",
      "epoch 28; loss: 0.19439; train_acc: 0.71053; val_acc: 0.64706\n",
      "epoch 29; loss: 0.20997; train_acc: 0.72368; val_acc: 0.64706\n",
      "epoch 30; loss: 0.17302; train_acc: 0.71711; val_acc: 0.58824\n",
      "epoch 31; loss: 0.17786; train_acc: 0.74342; val_acc: 0.58824\n",
      "epoch 32; loss: 0.19485; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 33; loss: 0.14876; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 34; loss: 0.21101; train_acc: 0.73684; val_acc: 0.64706\n",
      "epoch 35; loss: 0.22254; train_acc: 0.71711; val_acc: 0.70588\n",
      "epoch 36; loss: 0.14406; train_acc: 0.73026; val_acc: 0.70588\n",
      "epoch 37; loss: 0.2146; train_acc: 0.69737; val_acc: 0.70588\n",
      "epoch 38; loss: 0.16556; train_acc: 0.69737; val_acc: 0.70588\n",
      "epoch 39; loss: 0.16488; train_acc: 0.72368; val_acc: 0.70588\n",
      "epoch 40; loss: 0.16167; train_acc: 0.69079; val_acc: 0.64706\n",
      "epoch 41; loss: 0.18311; train_acc: 0.71711; val_acc: 0.70588\n",
      "epoch 42; loss: 0.20476; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 43; loss: 0.13969; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 44; loss: 0.20012; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 45; loss: 0.14206; train_acc: 0.71711; val_acc: 0.70588\n",
      "epoch 46; loss: 0.16876; train_acc: 0.71053; val_acc: 0.64706\n",
      "epoch 47; loss: 0.16358; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 48; loss: 0.1504; train_acc: 0.72368; val_acc: 0.58824\n",
      "epoch 49; loss: 0.15682; train_acc: 0.73026; val_acc: 0.70588\n",
      "epoch 50; loss: 0.2406; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 51; loss: 0.19594; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 52; loss: 0.16918; train_acc: 0.70395; val_acc: 0.70588\n",
      "epoch 53; loss: 0.18585; train_acc: 0.69737; val_acc: 0.70588\n",
      "epoch 54; loss: 0.1412; train_acc: 0.71711; val_acc: 0.76471\n",
      "epoch 55; loss: 0.14061; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 56; loss: 0.18111; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 57; loss: 0.18501; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 58; loss: 0.161; train_acc: 0.71711; val_acc: 0.76471\n",
      "epoch 59; loss: 0.18838; train_acc: 0.70395; val_acc: 0.70588\n",
      "epoch 60; loss: 0.27342; train_acc: 0.75658; val_acc: 0.70588\n",
      "epoch 61; loss: 0.17372; train_acc: 0.72368; val_acc: 0.58824\n",
      "epoch 62; loss: 0.17366; train_acc: 0.73026; val_acc: 0.76471\n",
      "epoch 63; loss: 0.22111; train_acc: 0.73684; val_acc: 0.58824\n",
      "epoch 64; loss: 0.20436; train_acc: 0.73684; val_acc: 0.64706\n",
      "epoch 65; loss: 0.28375; train_acc: 0.71711; val_acc: 0.76471\n",
      "epoch 66; loss: 0.14469; train_acc: 0.71711; val_acc: 0.64706\n",
      "epoch 67; loss: 0.31194; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 68; loss: 0.15661; train_acc: 0.71711; val_acc: 0.70588\n",
      "epoch 69; loss: 0.21395; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 70; loss: 0.10812; train_acc: 0.75658; val_acc: 0.70588\n",
      "epoch 71; loss: 0.24281; train_acc: 0.72368; val_acc: 0.58824\n",
      "epoch 72; loss: 0.15505; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 73; loss: 0.13432; train_acc: 0.71711; val_acc: 0.58824\n",
      "epoch 74; loss: 0.15172; train_acc: 0.73684; val_acc: 0.76471\n",
      "epoch 75; loss: 0.11206; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 76; loss: 0.21236; train_acc: 0.73684; val_acc: 0.76471\n",
      "epoch 77; loss: 0.18976; train_acc: 0.73026; val_acc: 0.70588\n",
      "epoch 78; loss: 0.17899; train_acc: 0.72368; val_acc: 0.76471\n",
      "epoch 79; loss: 0.141; train_acc: 0.71711; val_acc: 0.64706\n",
      "epoch 80; loss: 0.14102; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 81; loss: 0.13897; train_acc: 0.73026; val_acc: 0.70588\n",
      "epoch 82; loss: 0.24609; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 83; loss: 0.15117; train_acc: 0.70395; val_acc: 0.64706\n",
      "epoch 84; loss: 0.18098; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 85; loss: 0.13424; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 86; loss: 0.17089; train_acc: 0.75; val_acc: 0.70588\n",
      "epoch 87; loss: 0.17294; train_acc: 0.73684; val_acc: 0.64706\n",
      "epoch 88; loss: 0.25337; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 89; loss: 0.15164; train_acc: 0.73684; val_acc: 0.64706\n",
      "epoch 90; loss: 0.23343; train_acc: 0.73026; val_acc: 0.76471\n",
      "epoch 91; loss: 0.16783; train_acc: 0.75; val_acc: 0.70588\n",
      "epoch 92; loss: 0.19688; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 93; loss: 0.1547; train_acc: 0.73684; val_acc: 0.64706\n",
      "epoch 94; loss: 0.12267; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 95; loss: 0.13706; train_acc: 0.75; val_acc: 0.70588\n",
      "epoch 96; loss: 0.17042; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 97; loss: 0.16882; train_acc: 0.75658; val_acc: 0.64706\n",
      "epoch 98; loss: 0.15495; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 99; loss: 0.22814; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 100; loss: 0.17862; train_acc: 0.73684; val_acc: 0.64706\n",
      "process testing\n",
      "Accuracy: 0.7894736842105263\n",
      "Fold 1/10\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.23028; train_acc: 0.31579; val_acc: 0.41176\n",
      "epoch 2; loss: 0.22959; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 3; loss: 0.21042; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 4; loss: 0.20339; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 5; loss: 0.18601; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 6; loss: 0.20865; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 7; loss: 0.18652; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 8; loss: 0.19826; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 9; loss: 0.18219; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 10; loss: 0.18315; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 11; loss: 0.23162; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 12; loss: 0.23684; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 13; loss: 0.20085; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 14; loss: 0.17175; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 15; loss: 0.18398; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 16; loss: 0.18511; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 17; loss: 0.22524; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 18; loss: 0.16463; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 19; loss: 0.23414; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 20; loss: 0.2093; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 21; loss: 0.16385; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 22; loss: 0.17256; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 23; loss: 0.20394; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 24; loss: 0.17257; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 25; loss: 0.2781; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 26; loss: 0.20705; train_acc: 0.70395; val_acc: 0.76471\n",
      "epoch 27; loss: 0.2444; train_acc: 0.69079; val_acc: 0.58824\n",
      "epoch 28; loss: 0.19126; train_acc: 0.70395; val_acc: 0.58824\n",
      "epoch 29; loss: 0.19623; train_acc: 0.69079; val_acc: 0.70588\n",
      "epoch 30; loss: 0.23284; train_acc: 0.67763; val_acc: 0.58824\n",
      "epoch 31; loss: 0.22173; train_acc: 0.71711; val_acc: 0.64706\n",
      "epoch 32; loss: 0.15169; train_acc: 0.69079; val_acc: 0.64706\n",
      "epoch 33; loss: 0.17316; train_acc: 0.70395; val_acc: 0.64706\n",
      "epoch 34; loss: 0.18652; train_acc: 0.72368; val_acc: 0.64706\n",
      "epoch 35; loss: 0.14364; train_acc: 0.69737; val_acc: 0.58824\n",
      "epoch 36; loss: 0.20455; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 37; loss: 0.17743; train_acc: 0.75658; val_acc: 0.82353\n",
      "epoch 38; loss: 0.13892; train_acc: 0.76316; val_acc: 0.70588\n",
      "epoch 39; loss: 0.19401; train_acc: 0.76316; val_acc: 0.70588\n",
      "epoch 40; loss: 0.20553; train_acc: 0.73026; val_acc: 0.70588\n",
      "epoch 41; loss: 0.22003; train_acc: 0.75658; val_acc: 0.58824\n",
      "epoch 42; loss: 0.16342; train_acc: 0.76316; val_acc: 0.70588\n",
      "epoch 43; loss: 0.15233; train_acc: 0.76974; val_acc: 0.70588\n",
      "epoch 44; loss: 0.19826; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 45; loss: 0.22062; train_acc: 0.73684; val_acc: 0.64706\n",
      "epoch 46; loss: 0.16421; train_acc: 0.75658; val_acc: 0.58824\n",
      "epoch 47; loss: 0.25294; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 48; loss: 0.21701; train_acc: 0.75; val_acc: 0.70588\n",
      "epoch 49; loss: 0.13713; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 50; loss: 0.1412; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 51; loss: 0.20368; train_acc: 0.71711; val_acc: 0.76471\n",
      "epoch 52; loss: 0.17418; train_acc: 0.76316; val_acc: 0.64706\n",
      "epoch 53; loss: 0.18549; train_acc: 0.71053; val_acc: 0.76471\n",
      "epoch 54; loss: 0.21348; train_acc: 0.75658; val_acc: 0.64706\n",
      "epoch 55; loss: 0.20289; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 56; loss: 0.17166; train_acc: 0.72368; val_acc: 0.76471\n",
      "epoch 57; loss: 0.21217; train_acc: 0.75658; val_acc: 0.64706\n",
      "epoch 58; loss: 0.15952; train_acc: 0.71711; val_acc: 0.76471\n",
      "epoch 59; loss: 0.24384; train_acc: 0.76974; val_acc: 0.64706\n",
      "epoch 60; loss: 0.1944; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 61; loss: 0.24215; train_acc: 0.76316; val_acc: 0.70588\n",
      "epoch 62; loss: 0.19283; train_acc: 0.75; val_acc: 0.82353\n",
      "epoch 63; loss: 0.22728; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 64; loss: 0.15644; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 65; loss: 0.19409; train_acc: 0.76316; val_acc: 0.70588\n",
      "epoch 66; loss: 0.17699; train_acc: 0.76974; val_acc: 0.76471\n",
      "epoch 67; loss: 0.19463; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 68; loss: 0.18206; train_acc: 0.76974; val_acc: 0.76471\n",
      "epoch 69; loss: 0.15561; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 70; loss: 0.16704; train_acc: 0.76974; val_acc: 0.76471\n",
      "epoch 71; loss: 0.15589; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 72; loss: 0.20284; train_acc: 0.76316; val_acc: 0.76471\n",
      "epoch 73; loss: 0.15356; train_acc: 0.76974; val_acc: 0.76471\n",
      "epoch 74; loss: 0.15596; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 75; loss: 0.10838; train_acc: 0.76974; val_acc: 0.76471\n",
      "epoch 76; loss: 0.19929; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 77; loss: 0.23487; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 78; loss: 0.14982; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 79; loss: 0.19578; train_acc: 0.76316; val_acc: 0.76471\n",
      "epoch 80; loss: 0.14519; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 81; loss: 0.17201; train_acc: 0.76316; val_acc: 0.76471\n",
      "epoch 82; loss: 0.18039; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 83; loss: 0.17516; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 84; loss: 0.13753; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 85; loss: 0.16658; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 86; loss: 0.14107; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 87; loss: 0.18642; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 88; loss: 0.15931; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 89; loss: 0.16591; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 90; loss: 0.17178; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 91; loss: 0.14111; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 92; loss: 0.13216; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 93; loss: 0.13574; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 94; loss: 0.20991; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 95; loss: 0.22634; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 96; loss: 0.13932; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 97; loss: 0.19169; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 98; loss: 0.12652; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 99; loss: 0.11595; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 100; loss: 0.18333; train_acc: 0.73684; val_acc: 0.76471\n",
      "process testing\n",
      "Accuracy: 0.7894736842105263\n",
      "Fold 2/10\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.23424; train_acc: 0.32895; val_acc: 0.29412\n",
      "epoch 2; loss: 0.22856; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 3; loss: 0.21124; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 4; loss: 0.18201; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 5; loss: 0.19121; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 6; loss: 0.21389; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 7; loss: 0.23919; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 8; loss: 0.2463; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 9; loss: 0.19489; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 10; loss: 0.2397; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 11; loss: 0.20068; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 12; loss: 0.19191; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 13; loss: 0.20037; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 14; loss: 0.17484; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 15; loss: 0.17195; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 16; loss: 0.18773; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 17; loss: 0.18888; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 18; loss: 0.16071; train_acc: 0.67105; val_acc: 0.76471\n",
      "epoch 19; loss: 0.20726; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 20; loss: 0.24796; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 21; loss: 0.23105; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 22; loss: 0.1925; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 23; loss: 0.2093; train_acc: 0.68421; val_acc: 0.70588\n",
      "epoch 24; loss: 0.20559; train_acc: 0.67763; val_acc: 0.70588\n",
      "epoch 25; loss: 0.17231; train_acc: 0.67105; val_acc: 0.76471\n",
      "epoch 26; loss: 0.19605; train_acc: 0.68421; val_acc: 0.82353\n",
      "epoch 27; loss: 0.17592; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 28; loss: 0.23136; train_acc: 0.67105; val_acc: 0.70588\n",
      "epoch 29; loss: 0.18887; train_acc: 0.70395; val_acc: 0.70588\n",
      "epoch 30; loss: 0.1946; train_acc: 0.69079; val_acc: 0.76471\n",
      "epoch 31; loss: 0.202; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 32; loss: 0.17741; train_acc: 0.69737; val_acc: 0.70588\n",
      "epoch 33; loss: 0.15939; train_acc: 0.72368; val_acc: 0.64706\n",
      "epoch 34; loss: 0.28187; train_acc: 0.69079; val_acc: 0.70588\n",
      "epoch 35; loss: 0.1977; train_acc: 0.71053; val_acc: 0.64706\n",
      "epoch 36; loss: 0.24138; train_acc: 0.73026; val_acc: 0.76471\n",
      "epoch 37; loss: 0.16711; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 38; loss: 0.16943; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 39; loss: 0.17274; train_acc: 0.72368; val_acc: 0.58824\n",
      "epoch 40; loss: 0.17478; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 41; loss: 0.1702; train_acc: 0.71053; val_acc: 0.64706\n",
      "epoch 42; loss: 0.25938; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 43; loss: 0.17557; train_acc: 0.71711; val_acc: 0.64706\n",
      "epoch 44; loss: 0.1365; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 45; loss: 0.18986; train_acc: 0.71711; val_acc: 0.64706\n",
      "epoch 46; loss: 0.17611; train_acc: 0.72368; val_acc: 0.64706\n",
      "epoch 47; loss: 0.19049; train_acc: 0.70395; val_acc: 0.70588\n",
      "epoch 48; loss: 0.13819; train_acc: 0.72368; val_acc: 0.70588\n",
      "epoch 49; loss: 0.19576; train_acc: 0.76316; val_acc: 0.82353\n",
      "epoch 50; loss: 0.26326; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 51; loss: 0.20598; train_acc: 0.70395; val_acc: 0.64706\n",
      "epoch 52; loss: 0.21677; train_acc: 0.72368; val_acc: 0.64706\n",
      "epoch 53; loss: 0.13868; train_acc: 0.71711; val_acc: 0.64706\n",
      "epoch 54; loss: 0.19993; train_acc: 0.71711; val_acc: 0.64706\n",
      "epoch 55; loss: 0.18244; train_acc: 0.71053; val_acc: 0.64706\n",
      "epoch 56; loss: 0.19877; train_acc: 0.72368; val_acc: 0.64706\n",
      "epoch 57; loss: 0.13929; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 58; loss: 0.1294; train_acc: 0.71053; val_acc: 0.64706\n",
      "epoch 59; loss: 0.17997; train_acc: 0.72368; val_acc: 0.64706\n",
      "epoch 60; loss: 0.167; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 61; loss: 0.1654; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 62; loss: 0.23476; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 63; loss: 0.19761; train_acc: 0.73684; val_acc: 0.64706\n",
      "epoch 64; loss: 0.14612; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 65; loss: 0.16665; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 66; loss: 0.18067; train_acc: 0.73684; val_acc: 0.64706\n",
      "epoch 67; loss: 0.15194; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 68; loss: 0.11354; train_acc: 0.75658; val_acc: 0.64706\n",
      "epoch 69; loss: 0.20473; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 70; loss: 0.1446; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 71; loss: 0.17757; train_acc: 0.71711; val_acc: 0.64706\n",
      "epoch 72; loss: 0.13499; train_acc: 0.75; val_acc: 0.64706\n",
      "epoch 73; loss: 0.12848; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 74; loss: 0.16569; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 75; loss: 0.14512; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 76; loss: 0.199; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 77; loss: 0.21681; train_acc: 0.72368; val_acc: 0.64706\n",
      "epoch 78; loss: 0.16552; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 79; loss: 0.17594; train_acc: 0.73684; val_acc: 0.64706\n",
      "epoch 80; loss: 0.15695; train_acc: 0.75; val_acc: 0.64706\n",
      "epoch 81; loss: 0.12272; train_acc: 0.76974; val_acc: 0.64706\n",
      "epoch 82; loss: 0.21858; train_acc: 0.72368; val_acc: 0.64706\n",
      "epoch 83; loss: 0.16442; train_acc: 0.76316; val_acc: 0.64706\n",
      "epoch 84; loss: 0.15294; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 85; loss: 0.13782; train_acc: 0.75658; val_acc: 0.64706\n",
      "epoch 86; loss: 0.13835; train_acc: 0.74342; val_acc: 0.64706\n",
      "epoch 87; loss: 0.1829; train_acc: 0.75; val_acc: 0.64706\n",
      "epoch 88; loss: 0.17395; train_acc: 0.76316; val_acc: 0.64706\n",
      "epoch 89; loss: 0.26135; train_acc: 0.75658; val_acc: 0.64706\n",
      "epoch 90; loss: 0.16802; train_acc: 0.75; val_acc: 0.64706\n",
      "epoch 91; loss: 0.13819; train_acc: 0.76316; val_acc: 0.64706\n",
      "epoch 92; loss: 0.17504; train_acc: 0.76316; val_acc: 0.64706\n",
      "epoch 93; loss: 0.11345; train_acc: 0.76974; val_acc: 0.64706\n",
      "epoch 94; loss: 0.15975; train_acc: 0.75658; val_acc: 0.70588\n",
      "epoch 95; loss: 0.16422; train_acc: 0.76316; val_acc: 0.64706\n",
      "epoch 96; loss: 0.1654; train_acc: 0.76316; val_acc: 0.64706\n",
      "epoch 97; loss: 0.18455; train_acc: 0.76316; val_acc: 0.64706\n",
      "epoch 98; loss: 0.12697; train_acc: 0.76316; val_acc: 0.64706\n",
      "epoch 99; loss: 0.11525; train_acc: 0.75; val_acc: 0.64706\n",
      "epoch 100; loss: 0.21264; train_acc: 0.76316; val_acc: 0.64706\n",
      "process testing\n",
      "Accuracy: 0.7894736842105263\n",
      "Fold 3/10\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.23929; train_acc: 0.31579; val_acc: 0.41176\n",
      "epoch 2; loss: 0.22541; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 3; loss: 0.21742; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 4; loss: 0.20705; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 5; loss: 0.21123; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 6; loss: 0.20987; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 7; loss: 0.19128; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 8; loss: 0.2252; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 9; loss: 0.17833; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 10; loss: 0.19309; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 11; loss: 0.20321; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 12; loss: 0.19665; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 13; loss: 0.18016; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 14; loss: 0.17647; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 15; loss: 0.15689; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 16; loss: 0.20049; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 17; loss: 0.17561; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 18; loss: 0.18483; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 19; loss: 0.19403; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 20; loss: 0.15693; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 21; loss: 0.22087; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 22; loss: 0.17239; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 23; loss: 0.1873; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 24; loss: 0.20246; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 25; loss: 0.16625; train_acc: 0.68421; val_acc: 0.58824\n",
      "epoch 26; loss: 0.22169; train_acc: 0.69079; val_acc: 0.58824\n",
      "epoch 27; loss: 0.22424; train_acc: 0.67763; val_acc: 0.70588\n",
      "epoch 28; loss: 0.21392; train_acc: 0.69079; val_acc: 0.64706\n",
      "epoch 29; loss: 0.18437; train_acc: 0.69079; val_acc: 0.58824\n",
      "epoch 30; loss: 0.18709; train_acc: 0.69079; val_acc: 0.70588\n",
      "epoch 31; loss: 0.20363; train_acc: 0.69737; val_acc: 0.70588\n",
      "epoch 32; loss: 0.25603; train_acc: 0.71053; val_acc: 0.64706\n",
      "epoch 33; loss: 0.20828; train_acc: 0.71053; val_acc: 0.58824\n",
      "epoch 34; loss: 0.15825; train_acc: 0.70395; val_acc: 0.70588\n",
      "epoch 35; loss: 0.18732; train_acc: 0.73026; val_acc: 0.58824\n",
      "epoch 36; loss: 0.19391; train_acc: 0.73026; val_acc: 0.76471\n",
      "epoch 37; loss: 0.18073; train_acc: 0.73026; val_acc: 0.76471\n",
      "epoch 38; loss: 0.18738; train_acc: 0.71711; val_acc: 0.64706\n",
      "epoch 39; loss: 0.18894; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 40; loss: 0.17883; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 41; loss: 0.19372; train_acc: 0.75658; val_acc: 0.70588\n",
      "epoch 42; loss: 0.19232; train_acc: 0.75658; val_acc: 0.64706\n",
      "epoch 43; loss: 0.17274; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 44; loss: 0.21053; train_acc: 0.73026; val_acc: 0.76471\n",
      "epoch 45; loss: 0.2028; train_acc: 0.75; val_acc: 0.64706\n",
      "epoch 46; loss: 0.21084; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 47; loss: 0.19042; train_acc: 0.75; val_acc: 0.70588\n",
      "epoch 48; loss: 0.17328; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 49; loss: 0.14206; train_acc: 0.76316; val_acc: 0.76471\n",
      "epoch 50; loss: 0.16264; train_acc: 0.76316; val_acc: 0.76471\n",
      "epoch 51; loss: 0.19492; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 52; loss: 0.15375; train_acc: 0.76316; val_acc: 0.76471\n",
      "epoch 53; loss: 0.13993; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 54; loss: 0.19937; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 55; loss: 0.1733; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 56; loss: 0.16527; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 57; loss: 0.1585; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 58; loss: 0.17577; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 59; loss: 0.15198; train_acc: 0.73684; val_acc: 0.76471\n",
      "epoch 60; loss: 0.21677; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 61; loss: 0.19279; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 62; loss: 0.15049; train_acc: 0.73684; val_acc: 0.76471\n",
      "epoch 63; loss: 0.17402; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 64; loss: 0.18052; train_acc: 0.73684; val_acc: 0.76471\n",
      "epoch 65; loss: 0.18738; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 66; loss: 0.18105; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 67; loss: 0.23676; train_acc: 0.75; val_acc: 0.76471\n",
      "epoch 68; loss: 0.19589; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 69; loss: 0.15941; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 70; loss: 0.17322; train_acc: 0.75658; val_acc: 0.76471\n",
      "epoch 71; loss: 0.15031; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 72; loss: 0.18172; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 73; loss: 0.14928; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 74; loss: 0.13467; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 75; loss: 0.18212; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 76; loss: 0.22679; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 77; loss: 0.16958; train_acc: 0.73684; val_acc: 0.76471\n",
      "epoch 78; loss: 0.13896; train_acc: 0.75; val_acc: 0.70588\n",
      "epoch 79; loss: 0.15346; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 80; loss: 0.16324; train_acc: 0.73026; val_acc: 0.70588\n",
      "epoch 81; loss: 0.23726; train_acc: 0.73684; val_acc: 0.76471\n",
      "epoch 82; loss: 0.17453; train_acc: 0.75; val_acc: 0.70588\n",
      "epoch 83; loss: 0.15083; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 84; loss: 0.15526; train_acc: 0.77632; val_acc: 0.70588\n",
      "epoch 85; loss: 0.14354; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 86; loss: 0.19668; train_acc: 0.78289; val_acc: 0.70588\n",
      "epoch 87; loss: 0.1884; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 88; loss: 0.15599; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 89; loss: 0.16458; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 90; loss: 0.17773; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 91; loss: 0.17942; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 92; loss: 0.13672; train_acc: 0.72368; val_acc: 0.70588\n",
      "epoch 93; loss: 0.13544; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 94; loss: 0.15133; train_acc: 0.75; val_acc: 0.70588\n",
      "epoch 95; loss: 0.15885; train_acc: 0.74342; val_acc: 0.70588\n",
      "epoch 96; loss: 0.1661; train_acc: 0.76316; val_acc: 0.70588\n",
      "epoch 97; loss: 0.17951; train_acc: 0.73684; val_acc: 0.70588\n",
      "epoch 98; loss: 0.14016; train_acc: 0.75658; val_acc: 0.70588\n",
      "epoch 99; loss: 0.15006; train_acc: 0.75658; val_acc: 0.70588\n",
      "epoch 100; loss: 0.22584; train_acc: 0.75; val_acc: 0.70588\n",
      "process testing\n",
      "Accuracy: 0.7894736842105263\n",
      "Fold 4/10\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.23359; train_acc: 0.33553; val_acc: 0.23529\n",
      "epoch 2; loss: 0.22202; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 3; loss: 0.20776; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 4; loss: 0.2055; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 5; loss: 0.17586; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 6; loss: 0.22641; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 7; loss: 0.19498; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 8; loss: 0.22443; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 9; loss: 0.18739; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 10; loss: 0.19796; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 11; loss: 0.21562; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 12; loss: 0.20671; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 13; loss: 0.14571; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 14; loss: 0.19855; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 15; loss: 0.14864; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 16; loss: 0.20835; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 17; loss: 0.22585; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 18; loss: 0.16621; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 19; loss: 0.1845; train_acc: 0.67105; val_acc: 0.76471\n",
      "epoch 20; loss: 0.20239; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 21; loss: 0.18472; train_acc: 0.67105; val_acc: 0.76471\n",
      "epoch 22; loss: 0.16952; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 23; loss: 0.24137; train_acc: 0.67763; val_acc: 0.76471\n",
      "epoch 24; loss: 0.21476; train_acc: 0.66447; val_acc: 0.76471\n",
      "epoch 25; loss: 0.22891; train_acc: 0.67105; val_acc: 0.76471\n",
      "epoch 26; loss: 0.20127; train_acc: 0.71053; val_acc: 0.82353\n",
      "epoch 27; loss: 0.19467; train_acc: 0.69079; val_acc: 0.82353\n",
      "epoch 28; loss: 0.18661; train_acc: 0.70395; val_acc: 0.76471\n",
      "epoch 29; loss: 0.17758; train_acc: 0.72368; val_acc: 0.70588\n",
      "epoch 30; loss: 0.14787; train_acc: 0.71053; val_acc: 0.82353\n",
      "epoch 31; loss: 0.17766; train_acc: 0.71711; val_acc: 0.76471\n",
      "epoch 32; loss: 0.19746; train_acc: 0.73026; val_acc: 0.82353\n",
      "epoch 33; loss: 0.18833; train_acc: 0.68421; val_acc: 0.76471\n",
      "epoch 34; loss: 0.25296; train_acc: 0.71053; val_acc: 0.82353\n",
      "epoch 35; loss: 0.21463; train_acc: 0.72368; val_acc: 0.70588\n",
      "epoch 36; loss: 0.17535; train_acc: 0.70395; val_acc: 0.82353\n",
      "epoch 37; loss: 0.20922; train_acc: 0.71053; val_acc: 0.76471\n",
      "epoch 38; loss: 0.16101; train_acc: 0.71711; val_acc: 0.82353\n",
      "epoch 39; loss: 0.2016; train_acc: 0.69079; val_acc: 0.76471\n",
      "epoch 40; loss: 0.16719; train_acc: 0.73684; val_acc: 0.82353\n",
      "epoch 41; loss: 0.17134; train_acc: 0.73026; val_acc: 0.76471\n",
      "epoch 42; loss: 0.21081; train_acc: 0.71711; val_acc: 0.82353\n",
      "epoch 43; loss: 0.18098; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 44; loss: 0.19447; train_acc: 0.73026; val_acc: 0.76471\n",
      "epoch 45; loss: 0.19927; train_acc: 0.71711; val_acc: 0.70588\n",
      "epoch 46; loss: 0.1614; train_acc: 0.71711; val_acc: 0.76471\n",
      "epoch 47; loss: 0.22155; train_acc: 0.71711; val_acc: 0.70588\n",
      "epoch 48; loss: 0.16362; train_acc: 0.71053; val_acc: 0.70588\n",
      "epoch 49; loss: 0.17272; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 50; loss: 0.22396; train_acc: 0.72368; val_acc: 0.70588\n",
      "epoch 51; loss: 0.17803; train_acc: 0.70395; val_acc: 0.64706\n",
      "epoch 52; loss: 0.17207; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 53; loss: 0.15349; train_acc: 0.70395; val_acc: 0.70588\n",
      "epoch 54; loss: 0.19392; train_acc: 0.73026; val_acc: 0.64706\n",
      "epoch 55; loss: 0.17364; train_acc: 0.73026; val_acc: 0.76471\n",
      "epoch 56; loss: 0.15821; train_acc: 0.74342; val_acc: 0.76471\n",
      "epoch 57; loss: 0.16844; train_acc: 0.73026; val_acc: 0.70588\n",
      "epoch 58; loss: 0.18867; train_acc: 0.76974; val_acc: 0.82353\n",
      "epoch 59; loss: 0.15439; train_acc: 0.73026; val_acc: 0.70588\n",
      "epoch 60; loss: 0.12255; train_acc: 0.72368; val_acc: 0.82353\n",
      "epoch 61; loss: 0.19583; train_acc: 0.72368; val_acc: 0.70588\n",
      "epoch 62; loss: 0.16837; train_acc: 0.73026; val_acc: 0.82353\n",
      "epoch 63; loss: 0.19402; train_acc: 0.74342; val_acc: 0.82353\n",
      "epoch 64; loss: 0.15376; train_acc: 0.73684; val_acc: 0.82353\n",
      "epoch 65; loss: 0.23952; train_acc: 0.73684; val_acc: 0.82353\n",
      "epoch 66; loss: 0.15806; train_acc: 0.73026; val_acc: 0.82353\n",
      "epoch 67; loss: 0.24415; train_acc: 0.72368; val_acc: 0.76471\n",
      "epoch 68; loss: 0.1912; train_acc: 0.75; val_acc: 0.82353\n",
      "epoch 69; loss: 0.14372; train_acc: 0.72368; val_acc: 0.82353\n",
      "epoch 70; loss: 0.18958; train_acc: 0.73684; val_acc: 0.82353\n",
      "epoch 71; loss: 0.15978; train_acc: 0.74342; val_acc: 0.82353\n",
      "epoch 72; loss: 0.18672; train_acc: 0.73026; val_acc: 0.76471\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "train_dataset\n",
    "test_dataset\n",
    "k = 10\n",
    "\n",
    "splits = KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "k_counter = 0\n",
    "fold_logs = {}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "    \n",
    "    print(f'Fold {fold}/{k}')\n",
    "    fold_train = []\n",
    "    for key in train_idx:\n",
    "        fold_train.append(train_dataset[key])\n",
    "\n",
    "    fold_val = [] \n",
    "    for key in val_idx:\n",
    "        fold_val.append(train_dataset[key])\n",
    "\n",
    "    tr = DataLoader(fold_train, batch_size=batch_size, shuffle=True)\n",
    "    vd = DataLoader(fold_val, batch_size=batch_size, shuffle=True)\n",
    "    ts = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(\"=== Experiment model ===\")\n",
    "    fold_logs[fold] = expTrain(tr, vd, ts, 100)\n",
    "    \n",
    "    k_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fold_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
