{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This model using modified similarity (similarity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from preprocessing import data_transformation\n",
    "from similarity import calculate_similarity_matrix\n",
    "\n",
    "from model import GCN\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root='datasets/', name='MUTAG')\n",
    "torch.manual_seed(1234)\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split: Train test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```train_dataset```: for training model<br/>\n",
    "```val_dataset```: evaluate model for hyperparameter tunning<br/>\n",
    "```test_dataset```: testing model after complete training<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, ts, vl = 0.8, 0.1, 0.1\n",
    "dslen = len(dataset)\n",
    "tri = round(tr*dslen)\n",
    "tsi = round((tr+ts)*dslen)\n",
    "train_dataset = dataset[:tri]\n",
    "test_dataset = dataset[tri:tsi]\n",
    "val_dataset = dataset[tsi:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)\n",
    "test_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)\n",
    "val_dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import GINConv\n",
    "# from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        nn1 = Sequential(Linear(dataset.num_node_features, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        nn2 = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv2 = GCNConv(nn2)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        \n",
    "        # classification layer        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = self.conv1(x, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv2(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        embedding = self.conv3(embedding, edge_index)\n",
    "        embedding = embedding.relu()\n",
    "        # subgraph_embedding = subgraph_embedding.relu()\n",
    "        \n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 128\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# batch1 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from similarity2 import calculate_similarity_matrix, testt\n",
    "\n",
    "\n",
    "# AP Clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Experiment(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels, k = 1):\n",
    "        super(Experiment, self).__init__()\n",
    "        \n",
    "        # save number of subgraphs, default 1\n",
    "        self.k_subgraph = k\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        nn1 = Sequential(Linear(dataset.num_node_features, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        nn2 = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # embeddings for subgraph\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # attention layer\n",
    "        self.query_layer = Linear(hidden_channels,hidden_channels)\n",
    "        self.key_layer = Linear(hidden_channels,hidden_channels)\n",
    "        self.value_layer = Linear(hidden_channels,hidden_channels)\n",
    "        \n",
    "        # classification layer\n",
    "        self.lin = Linear(hidden_channels*2, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, ptr):\n",
    "        # Embed original\n",
    "        # embedding = self.conv1(x, edge_index)\n",
    "        # embedding = embedding.relu()\n",
    "        # embedding = self.conv2(embedding, edge_index)\n",
    "                \n",
    "        embedding = F.relu(self.conv1(x, edge_index))\n",
    "        embedding = self.bn1(embedding)\n",
    "        embedding = F.relu(self.conv2(embedding, edge_index))\n",
    "        embedding = self.bn2(embedding)\n",
    "        \n",
    "        # generate subgraph based on embeddings\n",
    "        feature_emb = embedding.detach()\n",
    "        \n",
    "        subgraph_edge_index, communities, S, batch_communities = self.subgraph_generator(feature_emb, edge_index, batch, ptr)\n",
    "        \n",
    "        \n",
    "        subgraph_embedding = self.conv4(embedding, subgraph_edge_index)\n",
    "        subgraph_embedding = subgraph_embedding.relu()\n",
    "        subgraph_embedding = self.conv5(subgraph_embedding, subgraph_edge_index)\n",
    "        \n",
    "        # apply readout layer/pooling for each subgraphs\n",
    "        subgraph_pool_embedding = self.subgraph_pooling(subgraph_embedding, communities, batch, ptr, batch_communities)\n",
    "        # print(len(subgraph_pool_embedding))\n",
    "        # apply selective (top k) attention\n",
    "        topk_subgraph_embedding = self.selectk_subgraph(embedding, subgraph_pool_embedding, self.k_subgraph)\n",
    "        \n",
    "        # readout layer for original embedding\n",
    "        embedding = global_mean_pool(embedding, batch)\n",
    "                \n",
    "        combined_embeddings = torch.cat((embedding, topk_subgraph_embedding.view(len(embedding), -1)), 1)\n",
    "        \n",
    "        \n",
    "        # h = F.dropout(combined_embeddings, p=0.3, training=self.training)\n",
    "        h = self.lin(combined_embeddings)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h, S, communities, topk_subgraph_embedding.view(len(embedding), -1)\n",
    "    \n",
    "    # checked\n",
    "    def selectk_subgraph(self, embs, sub_embs, k = 1):\n",
    "        # calculate attention and select top k subgraph\n",
    "        \n",
    "        topk_subgraphs_all = []\n",
    "\n",
    "        for i, (emb, sub_emb) in enumerate(zip(embs, sub_embs)):\n",
    "            sub = torch.tensor(sub_emb)\n",
    "            sub = sub.to(torch.float32)\n",
    "\n",
    "            # transform\n",
    "            query = self.query_layer(emb)\n",
    "            key = self.key_layer(sub)\n",
    "            value = self.value_layer(sub)\n",
    "\n",
    "            # att score\n",
    "            attention_score = torch.matmul(query, key.transpose(0,1))\n",
    "            attention_weight = F.softmax(attention_score, dim=0)\n",
    "            \n",
    "            # select topk\n",
    "            topk_subgraph_embeddings = None\n",
    "            \n",
    "            if (k <= len(sub)):\n",
    "                topk_scores, topk_indices = torch.topk(attention_weight, k)\n",
    "                topk_subgraph_embeddings = sub[topk_indices]\n",
    "            else:\n",
    "                print('too big')\n",
    "                \n",
    "            topk_subgraphs_all.append(topk_subgraph_embeddings)\n",
    "        \n",
    "        return torch.stack(topk_subgraphs_all)\n",
    "    \n",
    "    \n",
    "    def subgraph_generator(self, embeddings, batch_edge_index, batch, ptr):\n",
    "        '''\n",
    "        Return subgraph_edge_index (edge_index of created subgraph)\n",
    "        '''\n",
    "        graph_counter = 0\n",
    "        edge_index = [[],[]]\n",
    "        subgraph_edge_index = [[],[]]\n",
    "        # Gs = []\n",
    "        sub_created = False\n",
    "        graph_bound = {}\n",
    "        all_communities = []\n",
    "        batch_communities = {}\n",
    "        S = []\n",
    "\n",
    "        for i in range(len(ptr)-1):\n",
    "            graph_bound[i] = [ptr[i].item(), ptr[i+1].item()]\n",
    "        \n",
    "        for i, (src, dst) in enumerate(zip(batch_edge_index[0], batch_edge_index[1])):\n",
    "            lower_bound = graph_bound[graph_counter][0]\n",
    "            upper_bound = graph_bound[graph_counter][1]\n",
    "            if ((src >= lower_bound and src < upper_bound) or\n",
    "                (dst >= lower_bound and dst < upper_bound)):\n",
    "                \n",
    "                edge_index[0].append(src - lower_bound)\n",
    "                edge_index[1].append(dst - lower_bound)\n",
    "            else:\n",
    "                sub_created = True\n",
    "                \n",
    "            if (i == len(batch_edge_index[0]) - 1) or sub_created:\n",
    "                sub_created = False\n",
    "                \n",
    "                embs = []\n",
    "                # make new graph\n",
    "                for i, (b, emb) in enumerate(zip(batch, embeddings)):\n",
    "                    if (b == graph_counter):\n",
    "                        embs.append(emb)\n",
    "                \n",
    "                G = data_transformation(edge_index, embs)\n",
    "                # dont need this at the moment\n",
    "                # Gs.append(G)\n",
    "                \n",
    "                # Calculate similarity matrix\n",
    "                S = calculate_similarity_matrix(G)\n",
    "                \n",
    "                # AP Clustering        \n",
    "                clustering = AffinityPropagation(affinity='precomputed', damping=0.8, random_state=42, convergence_iter=15, max_iter=1000)\n",
    "                clustering.fit(S)\n",
    "                \n",
    "                \n",
    "                # Get community\n",
    "                communities = {}\n",
    "                for lab in clustering.labels_:\n",
    "                    communities[lab] = []\n",
    "                    all_communities.append(lab)\n",
    "                for nd, clust in enumerate(clustering.labels_):\n",
    "                    communities[clust].append(nd)\n",
    "                \n",
    "                edge_index = [[],[]]\n",
    "                batch_communities[graph_counter] = communities\n",
    "                \n",
    "                graph_counter+=1\n",
    "                \n",
    "                # Make subgraph edge_index\n",
    "                for c in communities:\n",
    "                    w = G.subgraph(communities[c])\n",
    "                    for sub in w.edges:\n",
    "                        subgraph_edge_index[0].append(sub[0] + lower_bound)\n",
    "                        subgraph_edge_index[1].append(sub[1] + lower_bound)\n",
    "                \n",
    "        \n",
    "        # print('batch communities', batch_communities)\n",
    "        return torch.tensor(subgraph_edge_index), all_communities, S, batch_communities\n",
    "    \n",
    "        \n",
    "    # check autograd (done)\n",
    "    def subgraph_pooling(self, embeddings, communities, batch, ptr, batch_communities, pool_type = 'mean'):\n",
    "        # batch communities: batch (or graph in this batch) -> communities -> member        \n",
    "        all_emb_pool = []\n",
    "        \n",
    "        # LOOP THROUGH BATCH\n",
    "        for b in batch_communities:\n",
    "            \n",
    "            # initialize array\n",
    "            emb_pool = [None] * len(batch_communities[b])\n",
    "            for comm in batch_communities[b]:\n",
    "                emb_temp = []\n",
    "\n",
    "                for member in batch_communities[b][comm]:\n",
    "                    index_used = member + ptr[b].item()\n",
    "                    emb_temp.append(embeddings[index_used])\n",
    "\n",
    "                # Pooling per sub graph using PyTorch\n",
    "                emb_temp_tensor = torch.stack(emb_temp)\n",
    "                if pool_type == 'mean': # mean pool\n",
    "                    emb_pool[comm] = torch.mean(emb_temp_tensor, dim=0)\n",
    "                elif pool_type == 'add': # add pool\n",
    "                    emb_pool[comm] = torch.sum(emb_temp_tensor, dim=0)\n",
    "                else:\n",
    "                    print('TODO: fill later')\n",
    "                    \n",
    "            all_emb_pool.append(torch.stack(emb_pool))\n",
    "        return all_emb_pool\n",
    "\n",
    "# experiment = Experiment(dataset, 64)\n",
    "# emb, h, S, communities, sub_emb = experiment(batch1.x, batch1.edge_index, batch1.batch, batch1.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base(model, loader, experiment_mode=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        if experiment_mode:\n",
    "            emb, h, S, communities, sub_emb = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(h, data.y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_base(model, loader, experiment_mode=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        if experiment_mode:\n",
    "            emb, h, S, communities, sub_emb = model(data.x, data.edge_index, data.batch, data.ptr)\n",
    "        else:\n",
    "            emb, h = model(data.x, data.edge_index, data.batch)\n",
    "        pred = h.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expTrain(train_loader, val_loader, test_loader, epoch = 10, fold=0):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "    num_hidden_layer = 128\n",
    "    experiment = Experiment(dataset, num_hidden_layer)\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    test_acc_history = []\n",
    "    \n",
    "    # early stop\n",
    "    early_stopping_patience = 20\n",
    "    best_val_score = -float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "    best_state = None\n",
    "    \n",
    "    # Train\n",
    "    print('process training')\n",
    "    for _ in range(epoch):\n",
    "        loss = round(train_base(experiment, train_loader, True).item(), 5)\n",
    "        train_acc = round(test_base(experiment, train_loader, True), 5)\n",
    "        val_acc = round(test_base(experiment, val_loader, True), 5)\n",
    "        test_acc = test_base(experiment, test_loader, True)\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "        test_acc_history.append(test_acc)\n",
    "        \n",
    "        print(f'epoch {_+1}; loss: {loss}; train_acc: {train_acc}; val_acc: {val_acc}; test_acc: {test_acc}')\n",
    "        \n",
    "        # early stop\n",
    "        if (val_acc > best_val_score):\n",
    "            best_val_score = val_acc\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            print('best found, save model')\n",
    "            # save model\n",
    "            torch.save(experiment.state_dict(), \"model-history/\"+str(fold)+\".experiment_best_model-gin_data-mutag.pth\")\n",
    "            best_state = copy.deepcopy(experiment.state_dict())\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if (epochs_without_improvement >= early_stopping_patience):\n",
    "                print('early stop triggered')\n",
    "                break\n",
    "                \n",
    "            \n",
    "\n",
    "    # Test    \n",
    "    # Create a new instance of the model for testing\n",
    "    best_model = Experiment(dataset, num_hidden_layer)\n",
    "    best_model.load_state_dict(best_state)\n",
    "\n",
    "    # Test\n",
    "    test = test_base(best_model, test_loader, True)\n",
    "    print(f'Accuracy: {test}')\n",
    "\n",
    "    \n",
    "    return [loss_history, train_acc_history, val_acc_history, test_acc_history]\n",
    "\n",
    "# expTrain(train_loader, val_loader, test_loader, epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseTrain(train_loader, val_loader, test_loader, epoch = 10, fold=0):\n",
    "    num_hidden_layer = 128\n",
    "    base = Base(dataset, num_hidden_layer)\n",
    "    early_stopping_patience = 20\n",
    "    best_val_score = -float(\"inf\")\n",
    "    epochs_without_improvement = 0\n",
    "    best_state = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    for _ in range(epoch):\n",
    "        \n",
    "        loss = round(train_base(base, train_loader).item(), 5)\n",
    "        train_acc = round(test_base(base, train_loader), 5)\n",
    "        val_acc = round(test_base(base, val_loader), 5)\n",
    "        \n",
    "        \n",
    "        print(f'epoch {_}; loss: {loss}; train_acc: {train_acc}; val_acc: {val_acc}; test: {round(test_base(base, test_loader), 2)}')\n",
    "\n",
    "        if (val_acc > best_val_score):\n",
    "            best_val_score = val_acc\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            print('best found, save model')\n",
    "            # save model\n",
    "            torch.save(base.state_dict(), \"model-history/\"+str(fold)+\".base_best_model-gin_data-mutag.pth\")\n",
    "            best_state = copy.deepcopy(base.state_dict())\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if (epochs_without_improvement >= early_stopping_patience):\n",
    "                print('early stop triggered')\n",
    "                break\n",
    "                \n",
    "            \n",
    "    # Test\n",
    "    # test = test_base(best, test_loader)\n",
    "    # print(f'Accuracy: {test}')\n",
    "    \n",
    "    # Create a new instance of the model for testing\n",
    "    best_model = Base(dataset, num_hidden_layer)\n",
    "    best_model.load_state_dict(best_state)\n",
    "\n",
    "    # Test\n",
    "    test = test_base(best_model, test_loader)\n",
    "    print(f'Accuracy: {test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import GINConv\n",
    "# from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(torch.nn.Module):\n",
    "    # merging type: o --> complement only, s --> substraction, c --> concatenation\n",
    "    def __init__(self, dataset, hidden_channels):\n",
    "        super(Base, self).__init__()\n",
    "        \n",
    "        # weight seed\n",
    "        torch.manual_seed(42)\n",
    "        nn1 = Sequential(Linear(dataset.num_node_features, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        nn2 = Sequential(Linear(hidden_channels, hidden_channels), ReLU(), Linear(hidden_channels,hidden_channels))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        \n",
    "        # classification layer        \n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed original\n",
    "        embedding = F.relu(self.conv1(x, edge_index))\n",
    "        embedding = self.bn1(embedding)\n",
    "        embedding = F.relu(self.conv2(embedding, edge_index))\n",
    "        embedding = self.bn2(embedding)\n",
    "        \n",
    "        embedding = global_add_pool(embedding, batch)\n",
    "        h = self.lin(embedding)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.3, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return embedding, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1])\n",
      "150\n",
      "tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[:round(len(dataset) * 0.8)]\n",
    "test_dataset = dataset[round(len(dataset) * 0.8):]\n",
    "print(train_dataset.y)\n",
    "print(len(train_dataset.y))\n",
    "print(test_dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.23846; train_acc: 0.68889; val_acc: 0.6; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.04173; train_acc: 0.7037; val_acc: 0.6; test: 0.61\n",
      "epoch 2; loss: 0.22985; train_acc: 0.71852; val_acc: 0.73333; test: 0.92\n",
      "best found, save model\n",
      "epoch 3; loss: 0.23669; train_acc: 0.31111; val_acc: 0.4; test: 0.39\n",
      "epoch 4; loss: 0.32219; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 5; loss: 0.26925; train_acc: 0.31111; val_acc: 0.4; test: 0.39\n",
      "epoch 6; loss: 0.1798; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 7; loss: 0.43792; train_acc: 0.31111; val_acc: 0.4; test: 0.39\n",
      "epoch 8; loss: 0.56106; train_acc: 0.31111; val_acc: 0.4; test: 0.39\n",
      "epoch 9; loss: 0.12165; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 10; loss: 0.21387; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 11; loss: 0.13289; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 12; loss: 0.25455; train_acc: 0.31111; val_acc: 0.4; test: 0.39\n",
      "epoch 13; loss: 0.06274; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 14; loss: 0.35824; train_acc: 0.4; val_acc: 0.66667; test: 0.45\n",
      "epoch 15; loss: 0.19334; train_acc: 0.65185; val_acc: 0.8; test: 0.87\n",
      "best found, save model\n",
      "epoch 16; loss: 0.27337; train_acc: 0.68148; val_acc: 0.8; test: 0.89\n",
      "epoch 17; loss: 1.96676; train_acc: 0.48889; val_acc: 0.73333; test: 0.66\n",
      "epoch 18; loss: 0.28502; train_acc: 0.71111; val_acc: 0.8; test: 0.92\n",
      "epoch 19; loss: 0.05487; train_acc: 0.54074; val_acc: 0.73333; test: 0.71\n",
      "epoch 20; loss: 0.06637; train_acc: 0.73333; val_acc: 0.8; test: 0.89\n",
      "epoch 21; loss: 0.19519; train_acc: 0.62222; val_acc: 0.73333; test: 0.79\n",
      "epoch 22; loss: 0.46075; train_acc: 0.54815; val_acc: 0.73333; test: 0.74\n",
      "epoch 23; loss: 0.07587; train_acc: 0.77778; val_acc: 0.8; test: 0.92\n",
      "epoch 24; loss: 0.19374; train_acc: 0.74074; val_acc: 0.86667; test: 0.89\n",
      "best found, save model\n",
      "epoch 25; loss: 0.32523; train_acc: 0.78519; val_acc: 0.8; test: 0.89\n",
      "epoch 26; loss: 0.22616; train_acc: 0.81481; val_acc: 0.8; test: 0.92\n",
      "epoch 27; loss: 0.08238; train_acc: 0.35556; val_acc: 0.4; test: 0.37\n",
      "epoch 28; loss: 0.13233; train_acc: 0.82222; val_acc: 0.8; test: 0.84\n",
      "epoch 29; loss: 0.19487; train_acc: 0.83704; val_acc: 0.86667; test: 0.87\n",
      "epoch 30; loss: 0.3107; train_acc: 0.79259; val_acc: 0.8; test: 0.84\n",
      "epoch 31; loss: 0.01558; train_acc: 0.88889; val_acc: 0.86667; test: 0.89\n",
      "epoch 32; loss: 0.13517; train_acc: 0.80741; val_acc: 0.8; test: 0.82\n",
      "epoch 33; loss: 0.32357; train_acc: 0.83704; val_acc: 0.8; test: 0.79\n",
      "epoch 34; loss: 0.07405; train_acc: 0.82963; val_acc: 0.8; test: 0.84\n",
      "epoch 35; loss: 0.15295; train_acc: 0.88148; val_acc: 1.0; test: 0.82\n",
      "best found, save model\n",
      "epoch 36; loss: 0.04786; train_acc: 0.79259; val_acc: 0.8; test: 0.82\n",
      "epoch 37; loss: 0.23348; train_acc: 0.88889; val_acc: 0.86667; test: 0.92\n",
      "epoch 38; loss: 0.14319; train_acc: 0.82222; val_acc: 0.8; test: 0.76\n",
      "epoch 39; loss: 0.19654; train_acc: 0.86667; val_acc: 0.8; test: 0.84\n",
      "epoch 40; loss: 0.06477; train_acc: 0.83704; val_acc: 0.86667; test: 0.79\n",
      "epoch 41; loss: 0.01279; train_acc: 0.83704; val_acc: 0.8; test: 0.82\n",
      "epoch 42; loss: 0.00977; train_acc: 0.87407; val_acc: 0.93333; test: 0.82\n",
      "epoch 43; loss: 0.15058; train_acc: 0.9037; val_acc: 1.0; test: 0.84\n",
      "epoch 44; loss: 0.06211; train_acc: 0.82222; val_acc: 0.8; test: 0.76\n",
      "epoch 45; loss: 0.2446; train_acc: 0.85185; val_acc: 0.86667; test: 0.82\n",
      "epoch 46; loss: 0.07029; train_acc: 0.83704; val_acc: 0.8; test: 0.79\n",
      "epoch 47; loss: 0.62392; train_acc: 0.87407; val_acc: 0.86667; test: 0.89\n",
      "epoch 48; loss: 0.26186; train_acc: 0.82222; val_acc: 0.8; test: 0.76\n",
      "epoch 49; loss: 0.11437; train_acc: 0.71852; val_acc: 0.73333; test: 0.58\n",
      "Accuracy: 0.8157894736842105\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.2843; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.30028; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.26972; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.27534; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.21437; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 6; loss: 0.1865; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 7; loss: 0.17634; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.631578947368421\n",
      "epoch 8; loss: 0.17124; train_acc: 0.81481; val_acc: 0.8; test_acc: 0.7894736842105263\n",
      "best found, save model\n",
      "epoch 9; loss: 0.12937; train_acc: 0.76296; val_acc: 0.8; test_acc: 0.8421052631578947\n",
      "epoch 10; loss: 0.22987; train_acc: 0.80741; val_acc: 0.8; test_acc: 0.868421052631579\n",
      "epoch 11; loss: 0.12276; train_acc: 0.77778; val_acc: 0.8; test_acc: 0.8421052631578947\n",
      "epoch 12; loss: 0.30257; train_acc: 0.51852; val_acc: 0.73333; test_acc: 0.6842105263157895\n",
      "epoch 13; loss: 0.1258; train_acc: 0.76296; val_acc: 0.8; test_acc: 0.8421052631578947\n",
      "epoch 14; loss: 0.09772; train_acc: 0.62963; val_acc: 0.73333; test_acc: 0.7631578947368421\n",
      "epoch 15; loss: 0.26374; train_acc: 0.73333; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 16; loss: 0.19947; train_acc: 0.82963; val_acc: 0.86667; test_acc: 0.868421052631579\n",
      "best found, save model\n",
      "epoch 17; loss: 0.07236; train_acc: 0.83704; val_acc: 0.93333; test_acc: 0.8421052631578947\n",
      "best found, save model\n",
      "epoch 18; loss: 0.13975; train_acc: 0.74815; val_acc: 0.8; test_acc: 0.7894736842105263\n",
      "epoch 19; loss: 0.06855; train_acc: 0.75556; val_acc: 0.73333; test_acc: 0.7631578947368421\n",
      "epoch 20; loss: 0.17024; train_acc: 0.34815; val_acc: 0.4; test_acc: 0.39473684210526316\n",
      "epoch 21; loss: 0.04502; train_acc: 0.75556; val_acc: 0.8; test_acc: 0.7894736842105263\n",
      "epoch 22; loss: 0.11075; train_acc: 0.55556; val_acc: 0.4; test_acc: 0.34210526315789475\n",
      "epoch 23; loss: 0.01342; train_acc: 0.71852; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 24; loss: 0.06938; train_acc: 0.83704; val_acc: 0.86667; test_acc: 0.7105263157894737\n",
      "epoch 25; loss: 0.125; train_acc: 0.72593; val_acc: 0.66667; test_acc: 0.6578947368421053\n",
      "epoch 26; loss: 0.04675; train_acc: 0.54074; val_acc: 0.53333; test_acc: 0.42105263157894735\n",
      "epoch 27; loss: 0.0917; train_acc: 0.74074; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 28; loss: 0.17207; train_acc: 0.40741; val_acc: 0.4; test_acc: 0.39473684210526316\n",
      "epoch 29; loss: 0.06567; train_acc: 0.73333; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 30; loss: 0.21189; train_acc: 0.58519; val_acc: 0.6; test_acc: 0.5263157894736842\n",
      "epoch 31; loss: 0.71391; train_acc: 0.8; val_acc: 0.86667; test_acc: 0.8421052631578947\n",
      "epoch 32; loss: 0.22989; train_acc: 0.87407; val_acc: 0.93333; test_acc: 0.868421052631579\n",
      "epoch 33; loss: 0.25484; train_acc: 0.87407; val_acc: 1.0; test_acc: 0.8421052631578947\n",
      "best found, save model\n",
      "epoch 34; loss: 0.1175; train_acc: 0.83704; val_acc: 0.8; test_acc: 0.8157894736842105\n",
      "epoch 35; loss: 0.27105; train_acc: 0.31111; val_acc: 0.4; test_acc: 0.39473684210526316\n",
      "epoch 36; loss: 0.14822; train_acc: 0.84444; val_acc: 0.86667; test_acc: 0.8421052631578947\n",
      "epoch 37; loss: 0.1836; train_acc: 0.74815; val_acc: 0.73333; test_acc: 0.868421052631579\n",
      "epoch 38; loss: 0.11121; train_acc: 0.78519; val_acc: 0.73333; test_acc: 0.7894736842105263\n",
      "epoch 39; loss: 0.13102; train_acc: 0.42963; val_acc: 0.4; test_acc: 0.39473684210526316\n",
      "epoch 40; loss: 0.0744; train_acc: 0.80741; val_acc: 0.8; test_acc: 0.8157894736842105\n",
      "epoch 41; loss: 0.1017; train_acc: 0.58519; val_acc: 0.46667; test_acc: 0.4473684210526316\n",
      "epoch 42; loss: 0.08988; train_acc: 0.74074; val_acc: 0.73333; test_acc: 0.7105263157894737\n",
      "epoch 43; loss: 0.13126; train_acc: 0.46667; val_acc: 0.46667; test_acc: 0.4473684210526316\n",
      "epoch 44; loss: 0.028; train_acc: 0.78519; val_acc: 0.73333; test_acc: 0.7631578947368421\n",
      "epoch 45; loss: 0.0404; train_acc: 0.85185; val_acc: 0.8; test_acc: 0.868421052631579\n",
      "epoch 46; loss: 0.19332; train_acc: 0.74815; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 47; loss: 0.16392; train_acc: 0.81481; val_acc: 0.8; test_acc: 0.7894736842105263\n",
      "epoch 48; loss: 0.5301; train_acc: 0.75556; val_acc: 0.73333; test_acc: 0.6842105263157895\n",
      "epoch 49; loss: 0.16995; train_acc: 0.88148; val_acc: 0.93333; test_acc: 0.9210526315789473\n",
      "epoch 50; loss: 0.03999; train_acc: 0.78519; val_acc: 0.73333; test_acc: 0.7631578947368421\n",
      "Accuracy: 0.8421052631578947\n",
      "Fold 1/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.2479; train_acc: 0.68148; val_acc: 0.66667; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.76276; train_acc: 0.68148; val_acc: 0.66667; test: 0.61\n",
      "epoch 2; loss: 0.09523; train_acc: 0.68148; val_acc: 0.66667; test: 0.63\n",
      "epoch 3; loss: 0.18369; train_acc: 0.71852; val_acc: 0.73333; test: 0.92\n",
      "best found, save model\n",
      "epoch 4; loss: 0.22295; train_acc: 0.68889; val_acc: 0.73333; test: 0.92\n",
      "epoch 5; loss: 0.13002; train_acc: 0.32593; val_acc: 0.33333; test: 0.39\n",
      "epoch 6; loss: 0.12656; train_acc: 0.40741; val_acc: 0.4; test: 0.45\n",
      "epoch 7; loss: 0.33148; train_acc: 0.40741; val_acc: 0.4; test: 0.45\n",
      "epoch 8; loss: 0.11382; train_acc: 0.51111; val_acc: 0.46667; test: 0.61\n",
      "epoch 9; loss: 0.46281; train_acc: 0.32593; val_acc: 0.33333; test: 0.39\n",
      "epoch 10; loss: 0.10957; train_acc: 0.32593; val_acc: 0.33333; test: 0.39\n",
      "epoch 11; loss: 0.28046; train_acc: 0.32593; val_acc: 0.33333; test: 0.39\n",
      "epoch 12; loss: 0.38152; train_acc: 0.71852; val_acc: 0.66667; test: 0.92\n",
      "epoch 13; loss: 0.09181; train_acc: 0.47407; val_acc: 0.46667; test: 0.55\n",
      "epoch 14; loss: 0.27774; train_acc: 0.7037; val_acc: 0.73333; test: 0.92\n",
      "epoch 15; loss: 0.18108; train_acc: 0.71852; val_acc: 0.86667; test: 0.89\n",
      "best found, save model\n",
      "epoch 16; loss: 0.32691; train_acc: 0.73333; val_acc: 0.73333; test: 0.92\n",
      "epoch 17; loss: 0.17299; train_acc: 0.7037; val_acc: 0.73333; test: 0.92\n",
      "epoch 18; loss: 0.27453; train_acc: 0.7037; val_acc: 0.73333; test: 0.89\n",
      "epoch 19; loss: 0.30418; train_acc: 0.72593; val_acc: 0.73333; test: 0.89\n",
      "epoch 20; loss: 0.17955; train_acc: 0.73333; val_acc: 0.66667; test: 0.92\n",
      "epoch 21; loss: 0.05907; train_acc: 0.74815; val_acc: 0.73333; test: 0.87\n",
      "epoch 22; loss: 0.16966; train_acc: 0.66667; val_acc: 0.6; test: 0.79\n",
      "epoch 23; loss: 0.08704; train_acc: 0.78519; val_acc: 0.73333; test: 0.89\n",
      "epoch 24; loss: 0.10958; train_acc: 0.79259; val_acc: 0.73333; test: 0.74\n",
      "epoch 25; loss: 0.01444; train_acc: 0.76296; val_acc: 0.73333; test: 0.84\n",
      "epoch 26; loss: 0.17509; train_acc: 0.87407; val_acc: 0.86667; test: 0.87\n",
      "epoch 27; loss: 0.20769; train_acc: 0.87407; val_acc: 0.73333; test: 0.89\n",
      "epoch 28; loss: 0.19263; train_acc: 0.78519; val_acc: 0.66667; test: 0.74\n",
      "epoch 29; loss: 0.09186; train_acc: 0.83704; val_acc: 0.73333; test: 0.84\n",
      "epoch 30; loss: 0.19879; train_acc: 0.80741; val_acc: 0.73333; test: 0.74\n",
      "epoch 31; loss: 0.02515; train_acc: 0.77778; val_acc: 0.73333; test: 0.82\n",
      "epoch 32; loss: 0.05101; train_acc: 0.84444; val_acc: 0.8; test: 0.84\n",
      "epoch 33; loss: 1.12458; train_acc: 0.82222; val_acc: 0.66667; test: 0.71\n",
      "epoch 34; loss: 0.3087; train_acc: 0.72593; val_acc: 0.6; test: 0.63\n",
      "epoch 35; loss: 0.00681; train_acc: 0.8963; val_acc: 0.86667; test: 0.84\n",
      "early stop triggered\n",
      "Accuracy: 0.8947368421052632\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.29624; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.32095; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.40167; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.24727; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.24175; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 6; loss: 0.20152; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 7; loss: 0.17209; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.631578947368421\n",
      "epoch 8; loss: 0.17337; train_acc: 0.82222; val_acc: 0.8; test_acc: 0.868421052631579\n",
      "best found, save model\n",
      "epoch 9; loss: 0.1695; train_acc: 0.78519; val_acc: 0.66667; test_acc: 0.7368421052631579\n",
      "epoch 10; loss: 0.12935; train_acc: 0.74074; val_acc: 0.73333; test_acc: 0.6842105263157895\n",
      "epoch 11; loss: 0.04906; train_acc: 0.74074; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 12; loss: 0.20678; train_acc: 0.47407; val_acc: 0.53333; test_acc: 0.5263157894736842\n",
      "epoch 13; loss: 0.04885; train_acc: 0.79259; val_acc: 0.66667; test_acc: 0.8157894736842105\n",
      "epoch 14; loss: 0.1713; train_acc: 0.6963; val_acc: 0.86667; test_acc: 0.7631578947368421\n",
      "best found, save model\n",
      "epoch 15; loss: 0.24308; train_acc: 0.8; val_acc: 0.73333; test_acc: 0.868421052631579\n",
      "epoch 16; loss: 0.03642; train_acc: 0.82222; val_acc: 0.66667; test_acc: 0.7631578947368421\n",
      "epoch 17; loss: 0.13694; train_acc: 0.77778; val_acc: 0.73333; test_acc: 0.8157894736842105\n",
      "epoch 18; loss: 0.17731; train_acc: 0.85185; val_acc: 0.8; test_acc: 0.8947368421052632\n",
      "epoch 19; loss: 0.0764; train_acc: 0.77778; val_acc: 0.73333; test_acc: 0.8157894736842105\n",
      "epoch 20; loss: 0.08589; train_acc: 0.54815; val_acc: 0.53333; test_acc: 0.4473684210526316\n",
      "epoch 21; loss: 0.1553; train_acc: 0.77778; val_acc: 0.73333; test_acc: 0.8421052631578947\n",
      "epoch 22; loss: 0.11798; train_acc: 0.82963; val_acc: 0.66667; test_acc: 0.7894736842105263\n",
      "epoch 23; loss: 0.383; train_acc: 0.88889; val_acc: 0.86667; test_acc: 0.868421052631579\n",
      "epoch 24; loss: 0.10971; train_acc: 0.75556; val_acc: 0.73333; test_acc: 0.6842105263157895\n",
      "epoch 25; loss: 0.31086; train_acc: 0.33333; val_acc: 0.33333; test_acc: 0.39473684210526316\n",
      "epoch 26; loss: 0.02794; train_acc: 0.75556; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 27; loss: 0.14306; train_acc: 0.82963; val_acc: 0.86667; test_acc: 0.8421052631578947\n",
      "epoch 28; loss: 0.04666; train_acc: 0.77037; val_acc: 0.73333; test_acc: 0.7894736842105263\n",
      "epoch 29; loss: 0.26955; train_acc: 0.84444; val_acc: 0.8; test_acc: 0.8421052631578947\n",
      "epoch 30; loss: 0.01192; train_acc: 0.86667; val_acc: 0.66667; test_acc: 0.8157894736842105\n",
      "epoch 31; loss: 0.30313; train_acc: 0.77778; val_acc: 0.8; test_acc: 0.7631578947368421\n",
      "epoch 32; loss: 0.08197; train_acc: 0.88889; val_acc: 0.73333; test_acc: 0.8157894736842105\n",
      "epoch 33; loss: 0.11843; train_acc: 0.88148; val_acc: 0.8; test_acc: 0.8157894736842105\n",
      "epoch 34; loss: 0.16289; train_acc: 0.86667; val_acc: 0.86667; test_acc: 0.8421052631578947\n",
      "early stop triggered\n",
      "Accuracy: 0.7105263157894737\n",
      "Fold 2/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.23287; train_acc: 0.6963; val_acc: 0.53333; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.13639; train_acc: 0.6963; val_acc: 0.53333; test: 0.61\n",
      "epoch 2; loss: 0.03603; train_acc: 0.74815; val_acc: 0.86667; test: 0.79\n",
      "best found, save model\n",
      "epoch 3; loss: 0.0473; train_acc: 0.41481; val_acc: 0.46667; test: 0.45\n",
      "epoch 4; loss: 0.21921; train_acc: 0.54815; val_acc: 0.66667; test: 0.68\n",
      "epoch 5; loss: 0.02981; train_acc: 0.3037; val_acc: 0.46667; test: 0.39\n",
      "epoch 6; loss: 0.14295; train_acc: 0.31111; val_acc: 0.46667; test: 0.39\n",
      "epoch 7; loss: 0.10887; train_acc: 0.3037; val_acc: 0.46667; test: 0.39\n",
      "epoch 8; loss: 0.36; train_acc: 0.31111; val_acc: 0.46667; test: 0.39\n",
      "epoch 9; loss: 0.13252; train_acc: 0.3037; val_acc: 0.46667; test: 0.39\n",
      "epoch 10; loss: 0.06914; train_acc: 0.4; val_acc: 0.46667; test: 0.45\n",
      "epoch 11; loss: 0.4371; train_acc: 0.3037; val_acc: 0.46667; test: 0.39\n",
      "epoch 12; loss: 0.18568; train_acc: 0.3037; val_acc: 0.46667; test: 0.39\n",
      "epoch 13; loss: 0.32719; train_acc: 0.5037; val_acc: 0.66667; test: 0.66\n",
      "epoch 14; loss: 0.73372; train_acc: 0.31111; val_acc: 0.46667; test: 0.39\n",
      "epoch 15; loss: 0.42465; train_acc: 0.3037; val_acc: 0.46667; test: 0.39\n",
      "epoch 16; loss: 0.24751; train_acc: 0.64444; val_acc: 0.73333; test: 0.79\n",
      "epoch 17; loss: 0.14088; train_acc: 0.73333; val_acc: 0.73333; test: 0.79\n",
      "epoch 18; loss: 0.03713; train_acc: 0.55556; val_acc: 0.66667; test: 0.66\n",
      "epoch 19; loss: 0.08062; train_acc: 0.42963; val_acc: 0.4; test: 0.37\n",
      "epoch 20; loss: 0.02152; train_acc: 0.8; val_acc: 0.66667; test: 0.68\n",
      "epoch 21; loss: 0.06545; train_acc: 0.82963; val_acc: 0.73333; test: 0.92\n",
      "epoch 22; loss: 0.26715; train_acc: 0.83704; val_acc: 0.66667; test: 0.76\n",
      "early stop triggered\n",
      "Accuracy: 0.7894736842105263\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.29082; train_acc: 0.6963; val_acc: 0.53333; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.3443; train_acc: 0.6963; val_acc: 0.53333; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.3323; train_acc: 0.6963; val_acc: 0.53333; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.34377; train_acc: 0.6963; val_acc: 0.53333; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.2159; train_acc: 0.6963; val_acc: 0.53333; test_acc: 0.6052631578947368\n",
      "epoch 6; loss: 0.11934; train_acc: 0.73333; val_acc: 0.6; test_acc: 0.6578947368421053\n",
      "best found, save model\n",
      "epoch 7; loss: 0.11009; train_acc: 0.78519; val_acc: 0.73333; test_acc: 0.7631578947368421\n",
      "best found, save model\n",
      "epoch 8; loss: 0.16118; train_acc: 0.73333; val_acc: 0.86667; test_acc: 0.9210526315789473\n",
      "best found, save model\n",
      "epoch 9; loss: 0.36689; train_acc: 0.79259; val_acc: 0.86667; test_acc: 0.8421052631578947\n",
      "epoch 10; loss: 0.22487; train_acc: 0.32593; val_acc: 0.46667; test_acc: 0.42105263157894735\n",
      "epoch 11; loss: 0.05875; train_acc: 0.75556; val_acc: 0.86667; test_acc: 0.8421052631578947\n",
      "epoch 12; loss: 0.10457; train_acc: 0.31111; val_acc: 0.46667; test_acc: 0.39473684210526316\n",
      "epoch 13; loss: 0.20277; train_acc: 0.67407; val_acc: 0.73333; test_acc: 0.7894736842105263\n",
      "epoch 14; loss: 0.06619; train_acc: 0.68889; val_acc: 0.73333; test_acc: 0.868421052631579\n",
      "epoch 15; loss: 0.1468; train_acc: 0.80741; val_acc: 0.73333; test_acc: 0.868421052631579\n",
      "epoch 16; loss: 0.17795; train_acc: 0.84444; val_acc: 0.8; test_acc: 0.868421052631579\n",
      "epoch 17; loss: 0.09099; train_acc: 0.3037; val_acc: 0.46667; test_acc: 0.39473684210526316\n",
      "epoch 18; loss: 0.11194; train_acc: 0.8; val_acc: 0.8; test_acc: 0.8421052631578947\n",
      "epoch 19; loss: 0.15914; train_acc: 0.83704; val_acc: 0.73333; test_acc: 0.7894736842105263\n",
      "epoch 20; loss: 0.10215; train_acc: 0.76296; val_acc: 0.8; test_acc: 0.7368421052631579\n",
      "epoch 21; loss: 0.06406; train_acc: 0.54815; val_acc: 0.33333; test_acc: 0.2894736842105263\n",
      "epoch 22; loss: 0.17691; train_acc: 0.75556; val_acc: 0.66667; test_acc: 0.6578947368421053\n",
      "epoch 23; loss: 0.10983; train_acc: 0.86667; val_acc: 0.66667; test_acc: 0.8421052631578947\n",
      "epoch 24; loss: 0.09876; train_acc: 0.75556; val_acc: 0.66667; test_acc: 0.631578947368421\n",
      "epoch 25; loss: 0.06203; train_acc: 0.37037; val_acc: 0.46667; test_acc: 0.3684210526315789\n",
      "epoch 26; loss: 0.03785; train_acc: 0.75556; val_acc: 0.6; test_acc: 0.6842105263157895\n",
      "epoch 27; loss: 0.19138; train_acc: 0.78519; val_acc: 0.73333; test_acc: 0.7631578947368421\n",
      "epoch 28; loss: 0.01997; train_acc: 0.48148; val_acc: 0.4; test_acc: 0.3684210526315789\n",
      "early stop triggered\n",
      "Accuracy: 0.9473684210526315\n",
      "Fold 3/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.18369; train_acc: 0.66667; val_acc: 0.8; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.23317; train_acc: 0.66667; val_acc: 0.8; test: 0.61\n",
      "epoch 2; loss: 0.1714; train_acc: 0.66667; val_acc: 0.8; test: 0.63\n",
      "epoch 3; loss: 0.24712; train_acc: 0.73333; val_acc: 0.46667; test: 0.89\n",
      "epoch 4; loss: 0.10725; train_acc: 0.71852; val_acc: 0.53333; test: 0.92\n",
      "epoch 5; loss: 0.23669; train_acc: 0.34074; val_acc: 0.2; test: 0.39\n",
      "epoch 6; loss: 0.10989; train_acc: 0.33333; val_acc: 0.2; test: 0.39\n",
      "epoch 7; loss: 0.08436; train_acc: 0.33333; val_acc: 0.2; test: 0.39\n",
      "epoch 8; loss: 0.04539; train_acc: 0.54074; val_acc: 0.2; test: 0.61\n",
      "epoch 9; loss: 0.45025; train_acc: 0.33333; val_acc: 0.2; test: 0.39\n",
      "epoch 10; loss: 0.03228; train_acc: 0.54074; val_acc: 0.2; test: 0.61\n",
      "epoch 11; loss: 0.05917; train_acc: 0.34074; val_acc: 0.2; test: 0.39\n",
      "epoch 12; loss: 0.16932; train_acc: 0.44444; val_acc: 0.2; test: 0.45\n",
      "epoch 13; loss: 0.16212; train_acc: 0.34074; val_acc: 0.2; test: 0.39\n",
      "epoch 14; loss: 0.18947; train_acc: 0.44444; val_acc: 0.2; test: 0.45\n",
      "epoch 15; loss: 0.5449; train_acc: 0.76296; val_acc: 0.53333; test: 0.92\n",
      "epoch 16; loss: 0.54148; train_acc: 0.75556; val_acc: 0.46667; test: 0.89\n",
      "epoch 17; loss: 0.5845; train_acc: 0.75556; val_acc: 0.53333; test: 0.92\n",
      "epoch 18; loss: 0.06692; train_acc: 0.81481; val_acc: 0.66667; test: 0.87\n",
      "epoch 19; loss: 0.15975; train_acc: 0.75556; val_acc: 0.53333; test: 0.92\n",
      "epoch 20; loss: 0.24163; train_acc: 0.33333; val_acc: 0.2; test: 0.39\n",
      "early stop triggered\n",
      "Accuracy: 0.6052631578947368\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.29212; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.24184; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.22782; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.37524; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.14604; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 6; loss: 0.26316; train_acc: 0.67407; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 7; loss: 0.38358; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 8; loss: 0.20538; train_acc: 0.77037; val_acc: 0.8; test_acc: 0.7894736842105263\n",
      "epoch 9; loss: 0.16215; train_acc: 0.67407; val_acc: 0.8; test_acc: 0.631578947368421\n",
      "epoch 10; loss: 0.13234; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 11; loss: 0.36362; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 12; loss: 0.11716; train_acc: 0.71852; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 13; loss: 0.32357; train_acc: 0.84444; val_acc: 0.8; test_acc: 0.7894736842105263\n",
      "epoch 14; loss: 0.08355; train_acc: 0.73333; val_acc: 0.86667; test_acc: 0.7105263157894737\n",
      "best found, save model\n",
      "epoch 15; loss: 0.2891; train_acc: 0.62222; val_acc: 0.6; test_acc: 0.6842105263157895\n",
      "epoch 16; loss: 0.09649; train_acc: 0.73333; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 17; loss: 0.14417; train_acc: 0.81481; val_acc: 0.8; test_acc: 0.7631578947368421\n",
      "epoch 18; loss: 0.2733; train_acc: 0.75556; val_acc: 0.8; test_acc: 0.8421052631578947\n",
      "epoch 19; loss: 0.10583; train_acc: 0.35556; val_acc: 0.26667; test_acc: 0.39473684210526316\n",
      "epoch 20; loss: 0.4883; train_acc: 0.75556; val_acc: 0.8; test_acc: 0.7105263157894737\n",
      "epoch 21; loss: 0.27894; train_acc: 0.33333; val_acc: 0.2; test_acc: 0.39473684210526316\n",
      "epoch 22; loss: 0.08009; train_acc: 0.73333; val_acc: 0.86667; test_acc: 0.6578947368421053\n",
      "epoch 23; loss: 0.10403; train_acc: 0.73333; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 24; loss: 0.06512; train_acc: 0.73333; val_acc: 0.86667; test_acc: 0.7105263157894737\n",
      "epoch 25; loss: 0.46441; train_acc: 0.61481; val_acc: 0.73333; test_acc: 0.5526315789473685\n",
      "epoch 26; loss: 0.48919; train_acc: 0.77037; val_acc: 0.86667; test_acc: 0.7631578947368421\n",
      "epoch 27; loss: 0.1322; train_acc: 0.84444; val_acc: 0.86667; test_acc: 0.8421052631578947\n",
      "epoch 28; loss: 0.21088; train_acc: 0.75556; val_acc: 0.86667; test_acc: 0.7368421052631579\n",
      "epoch 29; loss: 0.04266; train_acc: 0.78519; val_acc: 0.8; test_acc: 0.7105263157894737\n",
      "epoch 30; loss: 0.09705; train_acc: 0.75556; val_acc: 0.8; test_acc: 0.7631578947368421\n",
      "epoch 31; loss: 0.73004; train_acc: 0.82222; val_acc: 0.8; test_acc: 0.8157894736842105\n",
      "epoch 32; loss: 0.04609; train_acc: 0.87407; val_acc: 0.86667; test_acc: 0.868421052631579\n",
      "epoch 33; loss: 0.27892; train_acc: 0.82963; val_acc: 0.86667; test_acc: 0.8157894736842105\n",
      "epoch 34; loss: 0.14366; train_acc: 0.42222; val_acc: 0.33333; test_acc: 0.42105263157894735\n",
      "early stop triggered\n",
      "Accuracy: 0.7368421052631579\n",
      "Fold 4/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.14389; train_acc: 0.68889; val_acc: 0.6; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.08215; train_acc: 0.77778; val_acc: 0.66667; test: 0.84\n",
      "best found, save model\n",
      "epoch 2; loss: 0.20188; train_acc: 0.71111; val_acc: 0.6; test: 0.89\n",
      "epoch 3; loss: 0.0996; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 4; loss: 0.28497; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 5; loss: 0.21831; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 6; loss: 0.05792; train_acc: 0.31111; val_acc: 0.4; test: 0.39\n",
      "epoch 7; loss: 0.07002; train_acc: 0.67407; val_acc: 0.66667; test: 0.84\n",
      "epoch 8; loss: 0.11782; train_acc: 0.31111; val_acc: 0.4; test: 0.39\n",
      "epoch 9; loss: 0.0574; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 10; loss: 0.06458; train_acc: 0.51111; val_acc: 0.6; test: 0.66\n",
      "epoch 11; loss: 0.05453; train_acc: 0.31852; val_acc: 0.4; test: 0.39\n",
      "epoch 12; loss: 0.19136; train_acc: 0.4; val_acc: 0.46667; test: 0.45\n",
      "epoch 13; loss: 0.13937; train_acc: 0.45926; val_acc: 0.6; test: 0.55\n",
      "epoch 14; loss: 0.18066; train_acc: 0.51852; val_acc: 0.6; test: 0.68\n",
      "epoch 15; loss: 0.12679; train_acc: 0.73333; val_acc: 0.66667; test: 0.92\n",
      "epoch 16; loss: 0.48409; train_acc: 0.72593; val_acc: 0.66667; test: 0.92\n",
      "epoch 17; loss: 0.1202; train_acc: 0.31852; val_acc: 0.33333; test: 0.39\n",
      "epoch 18; loss: 0.09979; train_acc: 0.81481; val_acc: 0.66667; test: 0.87\n",
      "epoch 19; loss: 1.81256; train_acc: 0.82222; val_acc: 0.66667; test: 0.87\n",
      "epoch 20; loss: 0.18658; train_acc: 0.8; val_acc: 0.66667; test: 0.84\n",
      "epoch 21; loss: 0.33979; train_acc: 0.82222; val_acc: 0.73333; test: 0.74\n",
      "best found, save model\n",
      "epoch 22; loss: 0.54987; train_acc: 0.82222; val_acc: 0.6; test: 0.82\n",
      "epoch 23; loss: 0.06025; train_acc: 0.77778; val_acc: 0.66667; test: 0.71\n",
      "epoch 24; loss: 0.60516; train_acc: 0.81481; val_acc: 0.6; test: 0.84\n",
      "epoch 25; loss: 0.0173; train_acc: 0.48148; val_acc: 0.53333; test: 0.39\n",
      "epoch 26; loss: 0.1912; train_acc: 0.81481; val_acc: 0.66667; test: 0.79\n",
      "epoch 27; loss: 0.21664; train_acc: 0.76296; val_acc: 0.66667; test: 0.71\n",
      "epoch 28; loss: 0.26736; train_acc: 0.82222; val_acc: 0.73333; test: 0.89\n",
      "epoch 29; loss: 0.05464; train_acc: 0.86667; val_acc: 0.73333; test: 0.89\n",
      "epoch 30; loss: 0.03445; train_acc: 0.84444; val_acc: 0.73333; test: 0.89\n",
      "epoch 31; loss: 0.29111; train_acc: 0.81481; val_acc: 0.66667; test: 0.71\n",
      "epoch 32; loss: 0.257; train_acc: 0.37037; val_acc: 0.53333; test: 0.42\n",
      "epoch 33; loss: 0.31226; train_acc: 0.88889; val_acc: 0.86667; test: 0.92\n",
      "best found, save model\n",
      "epoch 34; loss: 0.03073; train_acc: 0.87407; val_acc: 0.73333; test: 0.82\n",
      "epoch 35; loss: 0.1533; train_acc: 0.75556; val_acc: 0.66667; test: 0.68\n",
      "epoch 36; loss: 0.22891; train_acc: 0.82963; val_acc: 0.73333; test: 0.87\n",
      "epoch 37; loss: 0.26281; train_acc: 0.83704; val_acc: 0.66667; test: 0.87\n",
      "epoch 38; loss: 0.19942; train_acc: 0.80741; val_acc: 0.66667; test: 0.74\n",
      "epoch 39; loss: 0.11041; train_acc: 0.78519; val_acc: 0.73333; test: 0.74\n",
      "epoch 40; loss: 0.05599; train_acc: 0.8; val_acc: 0.66667; test: 0.76\n",
      "epoch 41; loss: 4.4808; train_acc: 0.85185; val_acc: 0.8; test: 0.87\n",
      "epoch 42; loss: 0.18103; train_acc: 0.4963; val_acc: 0.66667; test: 0.45\n",
      "epoch 43; loss: 0.0697; train_acc: 0.77778; val_acc: 0.66667; test: 0.89\n",
      "epoch 44; loss: 0.10424; train_acc: 0.81481; val_acc: 0.66667; test: 0.76\n",
      "epoch 45; loss: 0.04885; train_acc: 0.88889; val_acc: 0.86667; test: 0.82\n",
      "epoch 46; loss: 0.27754; train_acc: 0.81481; val_acc: 0.66667; test: 0.79\n",
      "epoch 47; loss: 0.24464; train_acc: 0.81481; val_acc: 0.66667; test: 0.89\n",
      "epoch 48; loss: 0.01837; train_acc: 0.91111; val_acc: 0.93333; test: 0.84\n",
      "best found, save model\n",
      "epoch 49; loss: 0.03723; train_acc: 0.86667; val_acc: 0.66667; test: 0.92\n",
      "Accuracy: 0.8421052631578947\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.29298; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.28147; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.23954; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.25133; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.20463; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 6; loss: 0.18517; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.631578947368421\n",
      "epoch 7; loss: 0.15032; train_acc: 0.77778; val_acc: 0.6; test_acc: 0.7368421052631579\n",
      "epoch 8; loss: 0.22794; train_acc: 0.74815; val_acc: 0.66667; test_acc: 0.6842105263157895\n",
      "best found, save model\n",
      "epoch 9; loss: 0.164; train_acc: 0.81481; val_acc: 0.66667; test_acc: 0.868421052631579\n",
      "epoch 10; loss: 0.16006; train_acc: 0.32593; val_acc: 0.4; test_acc: 0.39473684210526316\n",
      "epoch 11; loss: 0.22492; train_acc: 0.76296; val_acc: 0.66667; test_acc: 0.7368421052631579\n",
      "epoch 12; loss: 0.22764; train_acc: 0.65926; val_acc: 0.66667; test_acc: 0.7894736842105263\n",
      "epoch 13; loss: 0.20763; train_acc: 0.8; val_acc: 0.53333; test_acc: 0.8421052631578947\n",
      "epoch 14; loss: 0.10644; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.7631578947368421\n",
      "epoch 15; loss: 0.36296; train_acc: 0.79259; val_acc: 0.6; test_acc: 0.8421052631578947\n",
      "epoch 16; loss: 0.11269; train_acc: 0.83704; val_acc: 0.66667; test_acc: 0.8421052631578947\n",
      "epoch 17; loss: 0.04048; train_acc: 0.77037; val_acc: 0.66667; test_acc: 0.7631578947368421\n",
      "epoch 18; loss: 0.05772; train_acc: 0.31852; val_acc: 0.4; test_acc: 0.39473684210526316\n",
      "epoch 19; loss: 0.1955; train_acc: 0.79259; val_acc: 0.66667; test_acc: 0.8157894736842105\n",
      "epoch 20; loss: 0.09853; train_acc: 0.31111; val_acc: 0.4; test_acc: 0.39473684210526316\n",
      "epoch 21; loss: 0.31087; train_acc: 0.85185; val_acc: 0.73333; test_acc: 0.868421052631579\n",
      "best found, save model\n",
      "epoch 22; loss: 0.13113; train_acc: 0.76296; val_acc: 0.66667; test_acc: 0.6842105263157895\n",
      "epoch 23; loss: 0.13562; train_acc: 0.78519; val_acc: 0.66667; test_acc: 0.7631578947368421\n",
      "epoch 24; loss: 0.02334; train_acc: 0.79259; val_acc: 0.66667; test_acc: 0.6842105263157895\n",
      "epoch 25; loss: 0.18741; train_acc: 0.77037; val_acc: 0.66667; test_acc: 0.7105263157894737\n",
      "epoch 26; loss: 0.07834; train_acc: 0.88148; val_acc: 0.86667; test_acc: 0.8157894736842105\n",
      "best found, save model\n",
      "epoch 27; loss: 0.04547; train_acc: 0.73333; val_acc: 0.6; test_acc: 0.6578947368421053\n",
      "epoch 28; loss: 0.06768; train_acc: 0.76296; val_acc: 0.66667; test_acc: 0.7894736842105263\n",
      "epoch 29; loss: 0.27419; train_acc: 0.75556; val_acc: 0.66667; test_acc: 0.631578947368421\n",
      "epoch 30; loss: 0.17427; train_acc: 0.85185; val_acc: 0.73333; test_acc: 0.7894736842105263\n",
      "epoch 31; loss: 0.57316; train_acc: 0.74815; val_acc: 0.66667; test_acc: 0.6842105263157895\n",
      "epoch 32; loss: 0.14199; train_acc: 0.75556; val_acc: 0.66667; test_acc: 0.6578947368421053\n",
      "epoch 33; loss: 0.24034; train_acc: 0.9037; val_acc: 0.8; test_acc: 0.7631578947368421\n",
      "epoch 34; loss: 0.12492; train_acc: 0.77778; val_acc: 0.66667; test_acc: 0.7631578947368421\n",
      "epoch 35; loss: 0.02993; train_acc: 0.82963; val_acc: 0.66667; test_acc: 0.6842105263157895\n",
      "epoch 36; loss: 0.11049; train_acc: 0.77778; val_acc: 0.66667; test_acc: 0.7631578947368421\n",
      "epoch 37; loss: 0.19036; train_acc: 0.77037; val_acc: 0.66667; test_acc: 0.6842105263157895\n",
      "epoch 38; loss: 0.05595; train_acc: 0.77037; val_acc: 0.66667; test_acc: 0.7631578947368421\n",
      "epoch 39; loss: 1.54101; train_acc: 0.85926; val_acc: 0.8; test_acc: 0.7631578947368421\n",
      "epoch 40; loss: 0.17308; train_acc: 0.74815; val_acc: 0.66667; test_acc: 0.7631578947368421\n",
      "epoch 41; loss: 0.027; train_acc: 0.47407; val_acc: 0.53333; test_acc: 0.42105263157894735\n",
      "epoch 42; loss: 0.23043; train_acc: 0.74815; val_acc: 0.66667; test_acc: 0.631578947368421\n",
      "epoch 43; loss: 0.10866; train_acc: 0.82963; val_acc: 0.66667; test_acc: 0.8157894736842105\n",
      "epoch 44; loss: 0.09482; train_acc: 0.80741; val_acc: 0.66667; test_acc: 0.7105263157894737\n",
      "epoch 45; loss: 0.09699; train_acc: 0.82222; val_acc: 0.8; test_acc: 0.8157894736842105\n",
      "epoch 46; loss: 0.11072; train_acc: 0.73333; val_acc: 0.6; test_acc: 0.6578947368421053\n",
      "early stop triggered\n",
      "Accuracy: 0.7894736842105263\n",
      "Fold 5/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.07256; train_acc: 0.67407; val_acc: 0.73333; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.50004; train_acc: 0.7037; val_acc: 0.73333; test: 0.92\n",
      "epoch 2; loss: 0.19192; train_acc: 0.72593; val_acc: 0.66667; test: 0.95\n",
      "epoch 3; loss: 0.07136; train_acc: 0.53333; val_acc: 0.4; test: 0.66\n",
      "epoch 4; loss: 0.83119; train_acc: 0.41481; val_acc: 0.33333; test: 0.45\n",
      "epoch 5; loss: 0.11582; train_acc: 0.32593; val_acc: 0.26667; test: 0.39\n",
      "epoch 6; loss: 0.08185; train_acc: 0.32593; val_acc: 0.26667; test: 0.39\n",
      "epoch 7; loss: 0.36793; train_acc: 0.32593; val_acc: 0.26667; test: 0.39\n",
      "epoch 8; loss: 0.19407; train_acc: 0.32593; val_acc: 0.26667; test: 0.39\n",
      "epoch 9; loss: 0.09406; train_acc: 0.32593; val_acc: 0.26667; test: 0.39\n",
      "epoch 10; loss: 0.16942; train_acc: 0.32593; val_acc: 0.26667; test: 0.39\n",
      "epoch 11; loss: 0.0886; train_acc: 0.33333; val_acc: 0.26667; test: 0.39\n",
      "epoch 12; loss: 0.05195; train_acc: 0.33333; val_acc: 0.26667; test: 0.39\n",
      "epoch 13; loss: 0.19716; train_acc: 0.32593; val_acc: 0.26667; test: 0.39\n",
      "epoch 14; loss: 0.19545; train_acc: 0.33333; val_acc: 0.26667; test: 0.39\n",
      "epoch 15; loss: 0.21224; train_acc: 0.32593; val_acc: 0.26667; test: 0.39\n",
      "epoch 16; loss: 0.45216; train_acc: 0.67407; val_acc: 0.46667; test: 0.79\n",
      "epoch 17; loss: 0.65171; train_acc: 0.33333; val_acc: 0.26667; test: 0.39\n",
      "epoch 18; loss: 0.47995; train_acc: 0.71852; val_acc: 0.6; test: 0.89\n",
      "epoch 19; loss: 0.39417; train_acc: 0.32593; val_acc: 0.26667; test: 0.39\n",
      "epoch 20; loss: 0.07656; train_acc: 0.71852; val_acc: 0.73333; test: 0.92\n",
      "early stop triggered\n",
      "Accuracy: 0.6052631578947368\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.30387; train_acc: 0.67407; val_acc: 0.73333; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.33401; train_acc: 0.67407; val_acc: 0.73333; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.27478; train_acc: 0.67407; val_acc: 0.73333; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.30093; train_acc: 0.67407; val_acc: 0.73333; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.15052; train_acc: 0.67407; val_acc: 0.73333; test_acc: 0.6052631578947368\n",
      "epoch 6; loss: 0.14624; train_acc: 0.68148; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "best found, save model\n",
      "epoch 7; loss: 0.132; train_acc: 0.73333; val_acc: 0.73333; test_acc: 0.6842105263157895\n",
      "epoch 8; loss: 0.15615; train_acc: 0.75556; val_acc: 0.73333; test_acc: 0.8947368421052632\n",
      "epoch 9; loss: 0.11736; train_acc: 0.74074; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 10; loss: 0.17877; train_acc: 0.32593; val_acc: 0.26667; test_acc: 0.39473684210526316\n",
      "epoch 11; loss: 0.32372; train_acc: 0.77778; val_acc: 0.73333; test_acc: 0.8157894736842105\n",
      "epoch 12; loss: 0.15689; train_acc: 0.75556; val_acc: 0.66667; test_acc: 0.7368421052631579\n",
      "epoch 13; loss: 0.14882; train_acc: 0.76296; val_acc: 0.73333; test_acc: 0.8421052631578947\n",
      "epoch 14; loss: 0.15236; train_acc: 0.82222; val_acc: 0.86667; test_acc: 0.8157894736842105\n",
      "best found, save model\n",
      "epoch 15; loss: 0.17338; train_acc: 0.73333; val_acc: 0.73333; test_acc: 0.8157894736842105\n",
      "epoch 16; loss: 0.20021; train_acc: 0.84444; val_acc: 0.73333; test_acc: 0.868421052631579\n",
      "epoch 17; loss: 0.16226; train_acc: 0.76296; val_acc: 0.73333; test_acc: 0.7105263157894737\n",
      "epoch 18; loss: 0.13465; train_acc: 0.85926; val_acc: 0.66667; test_acc: 0.8157894736842105\n",
      "epoch 19; loss: 0.04129; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 20; loss: 0.11434; train_acc: 0.79259; val_acc: 0.6; test_acc: 0.8157894736842105\n",
      "epoch 21; loss: 0.05063; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 22; loss: 0.11482; train_acc: 0.84444; val_acc: 0.73333; test_acc: 0.8157894736842105\n",
      "epoch 23; loss: 0.09994; train_acc: 0.77037; val_acc: 0.66667; test_acc: 0.7631578947368421\n",
      "epoch 24; loss: 0.07826; train_acc: 0.85926; val_acc: 0.86667; test_acc: 0.7631578947368421\n",
      "epoch 25; loss: 0.09357; train_acc: 0.75556; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 26; loss: 0.02187; train_acc: 0.34074; val_acc: 0.33333; test_acc: 0.39473684210526316\n",
      "epoch 27; loss: 0.12255; train_acc: 0.76296; val_acc: 0.66667; test_acc: 0.7894736842105263\n",
      "epoch 28; loss: 0.05926; train_acc: 0.87407; val_acc: 0.6; test_acc: 0.7631578947368421\n",
      "epoch 29; loss: 0.05326; train_acc: 0.76296; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 30; loss: 0.24075; train_acc: 0.78519; val_acc: 0.6; test_acc: 0.7631578947368421\n",
      "epoch 31; loss: 0.72355; train_acc: 0.77037; val_acc: 0.66667; test_acc: 0.8157894736842105\n",
      "epoch 32; loss: 0.27848; train_acc: 0.75556; val_acc: 0.73333; test_acc: 0.6842105263157895\n",
      "epoch 33; loss: 0.12523; train_acc: 0.8963; val_acc: 0.73333; test_acc: 0.8157894736842105\n",
      "epoch 34; loss: 0.03412; train_acc: 0.76296; val_acc: 0.66667; test_acc: 0.8157894736842105\n",
      "early stop triggered\n",
      "Accuracy: 0.8421052631578947\n",
      "Fold 6/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.10188; train_acc: 0.68889; val_acc: 0.6; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.04785; train_acc: 0.71111; val_acc: 0.6; test: 0.61\n",
      "epoch 2; loss: 0.13242; train_acc: 0.74815; val_acc: 0.66667; test: 0.76\n",
      "best found, save model\n",
      "epoch 3; loss: 0.05136; train_acc: 0.4963; val_acc: 0.46667; test: 0.58\n",
      "epoch 4; loss: 0.7525; train_acc: 0.68148; val_acc: 0.8; test: 0.92\n",
      "best found, save model\n",
      "epoch 5; loss: 0.18252; train_acc: 0.4; val_acc: 0.46667; test: 0.45\n",
      "epoch 6; loss: 0.2713; train_acc: 0.31111; val_acc: 0.4; test: 0.39\n",
      "epoch 7; loss: 0.03359; train_acc: 0.53333; val_acc: 0.46667; test: 0.66\n",
      "epoch 8; loss: 0.05398; train_acc: 0.4; val_acc: 0.46667; test: 0.45\n",
      "epoch 9; loss: 0.01342; train_acc: 0.31111; val_acc: 0.4; test: 0.39\n",
      "epoch 10; loss: 0.10544; train_acc: 0.71852; val_acc: 0.8; test: 0.92\n",
      "epoch 11; loss: 0.13971; train_acc: 0.7037; val_acc: 0.8; test: 0.92\n",
      "epoch 12; loss: 0.39073; train_acc: 0.51852; val_acc: 0.46667; test: 0.61\n",
      "epoch 13; loss: 0.22011; train_acc: 0.72593; val_acc: 0.73333; test: 0.89\n",
      "epoch 14; loss: 0.09286; train_acc: 0.66667; val_acc: 0.66667; test: 0.84\n",
      "epoch 15; loss: 0.15699; train_acc: 0.74074; val_acc: 0.73333; test: 0.89\n",
      "epoch 16; loss: 0.31894; train_acc: 0.67407; val_acc: 0.66667; test: 0.87\n",
      "epoch 17; loss: 0.10971; train_acc: 0.64444; val_acc: 0.53333; test: 0.82\n",
      "epoch 18; loss: 0.21515; train_acc: 0.81481; val_acc: 0.86667; test: 0.92\n",
      "best found, save model\n",
      "epoch 19; loss: 0.59083; train_acc: 0.77778; val_acc: 0.8; test: 0.92\n",
      "epoch 20; loss: 0.25124; train_acc: 0.79259; val_acc: 0.86667; test: 0.84\n",
      "epoch 21; loss: 0.19345; train_acc: 0.75556; val_acc: 0.73333; test: 0.92\n",
      "epoch 22; loss: 0.08609; train_acc: 0.36296; val_acc: 0.4; test: 0.39\n",
      "epoch 23; loss: 0.12655; train_acc: 0.76296; val_acc: 0.86667; test: 0.92\n",
      "epoch 24; loss: 0.23593; train_acc: 0.74815; val_acc: 0.66667; test: 0.74\n",
      "epoch 25; loss: 0.20664; train_acc: 0.82222; val_acc: 0.86667; test: 0.84\n",
      "epoch 26; loss: 0.30751; train_acc: 0.76296; val_acc: 0.8; test: 0.68\n",
      "epoch 27; loss: 0.0928; train_acc: 0.78519; val_acc: 0.8; test: 0.79\n",
      "epoch 28; loss: 0.21072; train_acc: 0.82963; val_acc: 0.86667; test: 0.82\n",
      "epoch 29; loss: 0.2847; train_acc: 0.76296; val_acc: 0.66667; test: 0.68\n",
      "epoch 30; loss: 0.24468; train_acc: 0.77778; val_acc: 0.86667; test: 0.66\n",
      "epoch 31; loss: 0.01447; train_acc: 0.78519; val_acc: 0.73333; test: 0.82\n",
      "epoch 32; loss: 0.15318; train_acc: 0.8963; val_acc: 0.93333; test: 0.92\n",
      "best found, save model\n",
      "epoch 33; loss: 0.19259; train_acc: 0.74074; val_acc: 0.8; test: 0.89\n",
      "epoch 34; loss: 0.32816; train_acc: 0.85185; val_acc: 0.93333; test: 0.87\n",
      "epoch 35; loss: 0.02033; train_acc: 0.8; val_acc: 0.86667; test: 0.92\n",
      "epoch 36; loss: 0.03888; train_acc: 0.8; val_acc: 0.86667; test: 0.74\n",
      "epoch 37; loss: 0.18977; train_acc: 0.84444; val_acc: 0.8; test: 0.82\n",
      "epoch 38; loss: 0.2163; train_acc: 0.83704; val_acc: 0.86667; test: 0.92\n",
      "epoch 39; loss: 0.01668; train_acc: 0.82222; val_acc: 0.8; test: 0.79\n",
      "epoch 40; loss: 0.16705; train_acc: 0.8; val_acc: 0.73333; test: 0.66\n",
      "epoch 41; loss: 0.19298; train_acc: 0.83704; val_acc: 0.86667; test: 0.82\n",
      "epoch 42; loss: 0.05732; train_acc: 0.81481; val_acc: 0.86667; test: 0.74\n",
      "epoch 43; loss: 0.24397; train_acc: 0.93333; val_acc: 0.93333; test: 0.95\n",
      "epoch 44; loss: 0.0077; train_acc: 0.8963; val_acc: 0.86667; test: 0.87\n",
      "epoch 45; loss: 0.17872; train_acc: 0.85926; val_acc: 0.8; test: 0.84\n",
      "epoch 46; loss: 0.00198; train_acc: 0.76296; val_acc: 0.66667; test: 0.66\n",
      "epoch 47; loss: 0.08574; train_acc: 0.93333; val_acc: 0.86667; test: 0.89\n",
      "epoch 48; loss: 0.02113; train_acc: 0.78519; val_acc: 0.73333; test: 0.63\n",
      "epoch 49; loss: 0.12151; train_acc: 0.73333; val_acc: 0.53333; test: 0.74\n",
      "Accuracy: 0.9210526315789473\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.32036; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.37356; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.21536; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.29067; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.18507; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 6; loss: 0.24964; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 7; loss: 0.18808; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 8; loss: 0.13126; train_acc: 0.7037; val_acc: 0.6; test_acc: 0.631578947368421\n",
      "epoch 9; loss: 0.17308; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 10; loss: 0.14683; train_acc: 0.74815; val_acc: 0.66667; test_acc: 0.6578947368421053\n",
      "best found, save model\n",
      "epoch 11; loss: 0.3468; train_acc: 0.68889; val_acc: 0.6; test_acc: 0.6052631578947368\n",
      "epoch 12; loss: 0.20114; train_acc: 0.73333; val_acc: 0.66667; test_acc: 0.631578947368421\n",
      "epoch 13; loss: 0.27481; train_acc: 0.74815; val_acc: 0.66667; test_acc: 0.6842105263157895\n",
      "epoch 14; loss: 0.19428; train_acc: 0.78519; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "best found, save model\n",
      "epoch 15; loss: 0.57359; train_acc: 0.84444; val_acc: 0.93333; test_acc: 0.7894736842105263\n",
      "best found, save model\n",
      "epoch 16; loss: 0.1433; train_acc: 0.71111; val_acc: 0.6; test_acc: 0.631578947368421\n",
      "epoch 17; loss: 0.15747; train_acc: 0.74074; val_acc: 0.66667; test_acc: 0.6842105263157895\n",
      "epoch 18; loss: 0.17949; train_acc: 0.60741; val_acc: 0.73333; test_acc: 0.5263157894736842\n",
      "epoch 19; loss: 0.12279; train_acc: 0.74074; val_acc: 0.66667; test_acc: 0.6578947368421053\n",
      "epoch 20; loss: 0.16834; train_acc: 0.74815; val_acc: 0.66667; test_acc: 0.631578947368421\n",
      "epoch 21; loss: 0.19381; train_acc: 0.77778; val_acc: 0.66667; test_acc: 0.7368421052631579\n",
      "epoch 22; loss: 0.06946; train_acc: 0.75556; val_acc: 0.66667; test_acc: 0.6578947368421053\n",
      "epoch 23; loss: 0.10146; train_acc: 0.87407; val_acc: 0.93333; test_acc: 0.8421052631578947\n",
      "epoch 24; loss: 0.20303; train_acc: 0.74815; val_acc: 0.66667; test_acc: 0.631578947368421\n",
      "epoch 25; loss: 0.4487; train_acc: 0.82963; val_acc: 0.86667; test_acc: 0.8157894736842105\n",
      "epoch 26; loss: 0.24604; train_acc: 0.84444; val_acc: 0.93333; test_acc: 0.7894736842105263\n",
      "epoch 27; loss: 0.06632; train_acc: 0.77778; val_acc: 0.66667; test_acc: 0.7631578947368421\n",
      "epoch 28; loss: 0.46024; train_acc: 0.73333; val_acc: 0.66667; test_acc: 0.6578947368421053\n",
      "epoch 29; loss: 0.05578; train_acc: 0.35556; val_acc: 0.53333; test_acc: 0.39473684210526316\n",
      "epoch 30; loss: 0.23966; train_acc: 0.74815; val_acc: 0.66667; test_acc: 0.631578947368421\n",
      "epoch 31; loss: 0.19386; train_acc: 0.85185; val_acc: 0.93333; test_acc: 0.7631578947368421\n",
      "epoch 32; loss: 0.0347; train_acc: 0.72593; val_acc: 0.6; test_acc: 0.631578947368421\n",
      "epoch 33; loss: 0.10463; train_acc: 0.45926; val_acc: 0.66667; test_acc: 0.3684210526315789\n",
      "epoch 34; loss: 0.09388; train_acc: 0.77037; val_acc: 0.66667; test_acc: 0.7105263157894737\n",
      "epoch 35; loss: 0.21221; train_acc: 0.91111; val_acc: 0.86667; test_acc: 0.868421052631579\n",
      "early stop triggered\n",
      "Accuracy: 0.7894736842105263\n",
      "Fold 7/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.29589; train_acc: 0.66667; val_acc: 0.8; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.61721; train_acc: 0.67407; val_acc: 0.8; test: 0.63\n",
      "epoch 2; loss: 0.15689; train_acc: 0.72593; val_acc: 0.93333; test: 0.89\n",
      "best found, save model\n",
      "epoch 3; loss: 0.33069; train_acc: 0.6963; val_acc: 0.66667; test: 0.89\n",
      "epoch 4; loss: 0.42265; train_acc: 0.34074; val_acc: 0.2; test: 0.39\n",
      "epoch 5; loss: 0.56885; train_acc: 0.33333; val_acc: 0.2; test: 0.39\n",
      "epoch 6; loss: 0.22565; train_acc: 0.34074; val_acc: 0.2; test: 0.39\n",
      "epoch 7; loss: 0.30836; train_acc: 0.33333; val_acc: 0.2; test: 0.39\n",
      "epoch 8; loss: 0.2204; train_acc: 0.53333; val_acc: 0.53333; test: 0.66\n",
      "epoch 9; loss: 0.23891; train_acc: 0.34074; val_acc: 0.2; test: 0.39\n",
      "epoch 10; loss: 0.12089; train_acc: 0.44444; val_acc: 0.33333; test: 0.45\n",
      "epoch 11; loss: 0.06357; train_acc: 0.33333; val_acc: 0.2; test: 0.39\n",
      "epoch 12; loss: 0.31559; train_acc: 0.68148; val_acc: 0.6; test: 0.87\n",
      "epoch 13; loss: 0.56362; train_acc: 0.34074; val_acc: 0.2; test: 0.39\n",
      "epoch 14; loss: 0.0991; train_acc: 0.72593; val_acc: 0.66667; test: 0.92\n",
      "epoch 15; loss: 0.18391; train_acc: 0.73333; val_acc: 0.66667; test: 0.89\n",
      "epoch 16; loss: 0.06149; train_acc: 0.71111; val_acc: 0.73333; test: 0.92\n",
      "epoch 17; loss: 0.45514; train_acc: 0.74815; val_acc: 0.66667; test: 0.89\n",
      "epoch 18; loss: 0.2906; train_acc: 0.74074; val_acc: 0.66667; test: 0.89\n",
      "epoch 19; loss: 0.06683; train_acc: 0.66667; val_acc: 0.66667; test: 0.79\n",
      "epoch 20; loss: 0.22652; train_acc: 0.75556; val_acc: 0.66667; test: 0.92\n",
      "epoch 21; loss: 0.15027; train_acc: 0.53333; val_acc: 0.53333; test: 0.68\n",
      "epoch 22; loss: 0.03677; train_acc: 0.76296; val_acc: 0.73333; test: 0.92\n",
      "early stop triggered\n",
      "Accuracy: 0.8947368421052632\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.33632; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.25852; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.22706; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.27005; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.14648; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.631578947368421\n",
      "epoch 6; loss: 0.14317; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 7; loss: 0.33697; train_acc: 0.68889; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 8; loss: 0.17646; train_acc: 0.74815; val_acc: 0.93333; test_acc: 0.7631578947368421\n",
      "best found, save model\n",
      "epoch 9; loss: 0.0835; train_acc: 0.74074; val_acc: 0.8; test_acc: 0.868421052631579\n",
      "epoch 10; loss: 0.15695; train_acc: 0.79259; val_acc: 0.8; test_acc: 0.8947368421052632\n",
      "epoch 11; loss: 0.1849; train_acc: 0.75556; val_acc: 0.8; test_acc: 0.8947368421052632\n",
      "epoch 12; loss: 0.19617; train_acc: 0.75556; val_acc: 0.86667; test_acc: 0.7631578947368421\n",
      "epoch 13; loss: 0.15666; train_acc: 0.36296; val_acc: 0.26667; test_acc: 0.3684210526315789\n",
      "epoch 14; loss: 0.06563; train_acc: 0.71852; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 15; loss: 0.43688; train_acc: 0.41481; val_acc: 0.33333; test_acc: 0.3684210526315789\n",
      "epoch 16; loss: 0.17707; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 17; loss: 0.14784; train_acc: 0.73333; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 18; loss: 0.18265; train_acc: 0.82963; val_acc: 0.93333; test_acc: 0.8421052631578947\n",
      "epoch 19; loss: 0.07873; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 20; loss: 0.31701; train_acc: 0.84444; val_acc: 0.86667; test_acc: 0.7368421052631579\n",
      "epoch 21; loss: 0.10332; train_acc: 0.74815; val_acc: 0.8; test_acc: 0.7105263157894737\n",
      "epoch 22; loss: 0.13127; train_acc: 0.77037; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 23; loss: 0.03465; train_acc: 0.76296; val_acc: 0.86667; test_acc: 0.7631578947368421\n",
      "epoch 24; loss: 0.08613; train_acc: 0.77778; val_acc: 0.86667; test_acc: 0.7631578947368421\n",
      "epoch 25; loss: 0.18496; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.7105263157894737\n",
      "epoch 26; loss: 0.66512; train_acc: 0.85185; val_acc: 0.93333; test_acc: 0.8157894736842105\n",
      "epoch 27; loss: 0.0441; train_acc: 0.71111; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 28; loss: 0.3893; train_acc: 0.60741; val_acc: 0.6; test_acc: 0.5263157894736842\n",
      "early stop triggered\n",
      "Accuracy: 0.7631578947368421\n",
      "Fold 8/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.06693; train_acc: 0.66667; val_acc: 0.8; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.48963; train_acc: 0.66667; val_acc: 0.8; test: 0.61\n",
      "epoch 2; loss: 0.25284; train_acc: 0.71852; val_acc: 0.73333; test: 0.89\n",
      "epoch 3; loss: 0.22486; train_acc: 0.53333; val_acc: 0.46667; test: 0.66\n",
      "epoch 4; loss: 0.16527; train_acc: 0.57037; val_acc: 0.46667; test: 0.68\n",
      "epoch 5; loss: 0.14235; train_acc: 0.33333; val_acc: 0.2; test: 0.39\n",
      "epoch 6; loss: 0.04731; train_acc: 0.57037; val_acc: 0.46667; test: 0.68\n",
      "epoch 7; loss: 0.02818; train_acc: 0.33333; val_acc: 0.2; test: 0.39\n",
      "epoch 8; loss: 0.01181; train_acc: 0.68148; val_acc: 0.73333; test: 0.89\n",
      "epoch 9; loss: 0.13187; train_acc: 0.41481; val_acc: 0.33333; test: 0.45\n",
      "epoch 10; loss: 0.30271; train_acc: 0.34074; val_acc: 0.2; test: 0.39\n",
      "epoch 11; loss: 0.3339; train_acc: 0.64444; val_acc: 0.73333; test: 0.76\n",
      "epoch 12; loss: 0.30905; train_acc: 0.42963; val_acc: 0.33333; test: 0.45\n",
      "epoch 13; loss: 0.19806; train_acc: 0.66667; val_acc: 0.73333; test: 0.87\n",
      "epoch 14; loss: 0.07949; train_acc: 0.44444; val_acc: 0.4; test: 0.5\n",
      "epoch 15; loss: 0.23233; train_acc: 0.71852; val_acc: 0.73333; test: 0.89\n",
      "epoch 16; loss: 0.21822; train_acc: 0.6963; val_acc: 0.73333; test: 0.92\n",
      "epoch 17; loss: 0.13924; train_acc: 0.67407; val_acc: 0.73333; test: 0.89\n",
      "epoch 18; loss: 0.13267; train_acc: 0.72593; val_acc: 0.66667; test: 0.92\n",
      "epoch 19; loss: 1.17707; train_acc: 0.34074; val_acc: 0.2; test: 0.39\n",
      "epoch 20; loss: 0.16087; train_acc: 0.71111; val_acc: 0.73333; test: 0.92\n",
      "early stop triggered\n",
      "Accuracy: 0.6052631578947368\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.30816; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.2533; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.21341; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.24785; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.2247; train_acc: 0.66667; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 6; loss: 0.20698; train_acc: 0.71111; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 7; loss: 0.15807; train_acc: 0.77778; val_acc: 0.8; test_acc: 0.631578947368421\n",
      "epoch 8; loss: 0.24082; train_acc: 0.73333; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 9; loss: 0.1881; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.631578947368421\n",
      "epoch 10; loss: 0.21007; train_acc: 0.74074; val_acc: 0.73333; test_acc: 0.7631578947368421\n",
      "epoch 11; loss: 0.12468; train_acc: 0.33333; val_acc: 0.2; test_acc: 0.39473684210526316\n",
      "epoch 12; loss: 0.1805; train_acc: 0.73333; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 13; loss: 0.47261; train_acc: 0.76296; val_acc: 0.8; test_acc: 0.7105263157894737\n",
      "epoch 14; loss: 0.26956; train_acc: 0.79259; val_acc: 0.8; test_acc: 0.7631578947368421\n",
      "epoch 15; loss: 0.12322; train_acc: 0.54074; val_acc: 0.46667; test_acc: 0.3684210526315789\n",
      "epoch 16; loss: 0.16618; train_acc: 0.73333; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 17; loss: 0.30747; train_acc: 0.74074; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 18; loss: 0.27224; train_acc: 0.73333; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 19; loss: 0.03971; train_acc: 0.84444; val_acc: 0.86667; test_acc: 0.7894736842105263\n",
      "best found, save model\n",
      "epoch 20; loss: 0.12263; train_acc: 0.73333; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 21; loss: 0.13476; train_acc: 0.47407; val_acc: 0.4; test_acc: 0.3157894736842105\n",
      "epoch 22; loss: 0.17357; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6052631578947368\n",
      "epoch 23; loss: 0.0814; train_acc: 0.71111; val_acc: 0.66667; test_acc: 0.6578947368421053\n",
      "epoch 24; loss: 0.05196; train_acc: 0.7037; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 25; loss: 0.26966; train_acc: 0.41481; val_acc: 0.13333; test_acc: 0.34210526315789475\n",
      "epoch 26; loss: 0.18103; train_acc: 0.73333; val_acc: 0.8; test_acc: 0.631578947368421\n",
      "epoch 27; loss: 0.21865; train_acc: 0.75556; val_acc: 0.8; test_acc: 0.7368421052631579\n",
      "epoch 28; loss: 0.43077; train_acc: 0.88148; val_acc: 0.86667; test_acc: 0.7894736842105263\n",
      "epoch 29; loss: 0.07227; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 30; loss: 0.235; train_acc: 0.64444; val_acc: 0.6; test_acc: 0.6578947368421053\n",
      "epoch 31; loss: 0.07108; train_acc: 0.71111; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 32; loss: 0.09219; train_acc: 0.88148; val_acc: 0.86667; test_acc: 0.8157894736842105\n",
      "epoch 33; loss: 0.32262; train_acc: 0.79259; val_acc: 0.8; test_acc: 0.7631578947368421\n",
      "epoch 34; loss: 0.08834; train_acc: 0.8; val_acc: 1.0; test_acc: 0.868421052631579\n",
      "best found, save model\n",
      "epoch 35; loss: 0.02692; train_acc: 0.86667; val_acc: 0.8; test_acc: 0.7894736842105263\n",
      "epoch 36; loss: 0.09312; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.7105263157894737\n",
      "epoch 37; loss: 0.0207; train_acc: 0.34815; val_acc: 0.2; test_acc: 0.39473684210526316\n",
      "epoch 38; loss: 0.03278; train_acc: 0.79259; val_acc: 0.93333; test_acc: 0.8157894736842105\n",
      "epoch 39; loss: 0.20885; train_acc: 0.74074; val_acc: 0.73333; test_acc: 0.6842105263157895\n",
      "epoch 40; loss: 0.04795; train_acc: 0.78519; val_acc: 0.93333; test_acc: 0.7631578947368421\n",
      "epoch 41; loss: 0.11727; train_acc: 0.4; val_acc: 0.26667; test_acc: 0.3684210526315789\n",
      "epoch 42; loss: 0.44702; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 43; loss: 0.01631; train_acc: 0.37778; val_acc: 0.26667; test_acc: 0.3684210526315789\n",
      "epoch 44; loss: 0.21523; train_acc: 0.72593; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 45; loss: 0.09908; train_acc: 0.91852; val_acc: 0.8; test_acc: 0.8157894736842105\n",
      "epoch 46; loss: 0.19938; train_acc: 0.71111; val_acc: 0.8; test_acc: 0.6578947368421053\n",
      "epoch 47; loss: 0.21877; train_acc: 0.85926; val_acc: 0.8; test_acc: 0.7631578947368421\n",
      "epoch 48; loss: 0.15234; train_acc: 0.83704; val_acc: 0.8; test_acc: 0.7105263157894737\n",
      "epoch 49; loss: 0.2163; train_acc: 0.83704; val_acc: 0.8; test_acc: 0.7368421052631579\n",
      "epoch 50; loss: 0.03238; train_acc: 0.34815; val_acc: 0.26667; test_acc: 0.39473684210526316\n",
      "Accuracy: 0.868421052631579\n",
      "Fold 9/10\n",
      "=== Base model vs Experiment ===\n",
      "=== Base model ===\n",
      "epoch 0; loss: 0.29157; train_acc: 0.68148; val_acc: 0.66667; test: 0.61\n",
      "best found, save model\n",
      "epoch 1; loss: 0.34221; train_acc: 0.71111; val_acc: 0.66667; test: 0.89\n",
      "epoch 2; loss: 0.16726; train_acc: 0.76296; val_acc: 0.8; test: 0.82\n",
      "best found, save model\n",
      "epoch 3; loss: 0.76705; train_acc: 0.4; val_acc: 0.46667; test: 0.45\n",
      "epoch 4; loss: 0.3918; train_acc: 0.4; val_acc: 0.46667; test: 0.45\n",
      "epoch 5; loss: 0.3139; train_acc: 0.31852; val_acc: 0.33333; test: 0.39\n",
      "epoch 6; loss: 0.1465; train_acc: 0.57778; val_acc: 0.66667; test: 0.71\n",
      "epoch 7; loss: 0.23843; train_acc: 0.45926; val_acc: 0.53333; test: 0.53\n",
      "epoch 8; loss: 0.07952; train_acc: 0.31852; val_acc: 0.33333; test: 0.39\n",
      "epoch 9; loss: 0.173; train_acc: 0.68889; val_acc: 0.66667; test: 0.89\n",
      "epoch 10; loss: 0.20646; train_acc: 0.31852; val_acc: 0.33333; test: 0.39\n",
      "epoch 11; loss: 0.06429; train_acc: 0.71111; val_acc: 0.6; test: 0.92\n",
      "epoch 12; loss: 0.14559; train_acc: 0.42963; val_acc: 0.46667; test: 0.47\n",
      "epoch 13; loss: 0.78086; train_acc: 0.74815; val_acc: 0.66667; test: 0.89\n",
      "epoch 14; loss: 0.05571; train_acc: 0.68148; val_acc: 0.66667; test: 0.66\n",
      "epoch 15; loss: 0.24025; train_acc: 0.41481; val_acc: 0.46667; test: 0.47\n",
      "epoch 16; loss: 0.13056; train_acc: 0.74074; val_acc: 0.66667; test: 0.92\n",
      "epoch 17; loss: 0.31243; train_acc: 0.77037; val_acc: 0.8; test: 0.92\n",
      "epoch 18; loss: 0.14162; train_acc: 0.76296; val_acc: 0.66667; test: 0.87\n",
      "epoch 19; loss: 0.10152; train_acc: 0.78519; val_acc: 0.73333; test: 0.79\n",
      "epoch 20; loss: 0.16787; train_acc: 0.55556; val_acc: 0.66667; test: 0.71\n",
      "epoch 21; loss: 0.04668; train_acc: 0.78519; val_acc: 0.8; test: 0.71\n",
      "epoch 22; loss: 0.00442; train_acc: 0.78519; val_acc: 0.8; test: 0.89\n",
      "early stop triggered\n",
      "Accuracy: 0.8157894736842105\n",
      "=== Experiment model ===\n",
      "process training\n",
      "epoch 1; loss: 0.31316; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "best found, save model\n",
      "epoch 2; loss: 0.29323; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 3; loss: 0.24995; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 4; loss: 0.25926; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 5; loss: 0.22348; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 6; loss: 0.2459; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 7; loss: 0.21024; train_acc: 0.68148; val_acc: 0.66667; test_acc: 0.6052631578947368\n",
      "epoch 8; loss: 0.21314; train_acc: 0.6963; val_acc: 0.66667; test_acc: 0.6578947368421053\n",
      "epoch 9; loss: 0.20913; train_acc: 0.73333; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "best found, save model\n",
      "epoch 10; loss: 0.19489; train_acc: 0.74074; val_acc: 0.73333; test_acc: 0.6842105263157895\n",
      "epoch 11; loss: 0.18383; train_acc: 0.71852; val_acc: 0.73333; test_acc: 0.6052631578947368\n",
      "epoch 12; loss: 0.19207; train_acc: 0.72593; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 13; loss: 0.09434; train_acc: 0.4; val_acc: 0.53333; test_acc: 0.18421052631578946\n",
      "epoch 14; loss: 0.15541; train_acc: 0.71111; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 15; loss: 0.14254; train_acc: 0.72593; val_acc: 0.73333; test_acc: 0.631578947368421\n",
      "epoch 16; loss: 0.19329; train_acc: 0.6963; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 17; loss: 0.23241; train_acc: 0.71852; val_acc: 0.73333; test_acc: 0.631578947368421\n",
      "epoch 18; loss: 0.13085; train_acc: 0.68148; val_acc: 0.73333; test_acc: 0.631578947368421\n",
      "epoch 19; loss: 0.1614; train_acc: 0.76296; val_acc: 0.8; test_acc: 0.7368421052631579\n",
      "best found, save model\n",
      "epoch 20; loss: 0.16576; train_acc: 0.71852; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 21; loss: 0.09734; train_acc: 0.81481; val_acc: 0.93333; test_acc: 0.6842105263157895\n",
      "best found, save model\n",
      "epoch 22; loss: 0.15679; train_acc: 0.71111; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 23; loss: 0.59618; train_acc: 0.34074; val_acc: 0.4; test_acc: 0.39473684210526316\n",
      "epoch 24; loss: 0.05832; train_acc: 0.71852; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 25; loss: 0.10998; train_acc: 0.74074; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 26; loss: 0.14713; train_acc: 0.77037; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 27; loss: 0.07588; train_acc: 0.72593; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 28; loss: 0.10728; train_acc: 0.86667; val_acc: 0.86667; test_acc: 0.7368421052631579\n",
      "epoch 29; loss: 0.20068; train_acc: 0.71111; val_acc: 0.73333; test_acc: 0.6842105263157895\n",
      "epoch 30; loss: 0.0383; train_acc: 0.85185; val_acc: 0.86667; test_acc: 0.7368421052631579\n",
      "epoch 31; loss: 0.50647; train_acc: 0.75556; val_acc: 0.73333; test_acc: 0.6578947368421053\n",
      "epoch 32; loss: 0.0638; train_acc: 0.82222; val_acc: 0.73333; test_acc: 0.8421052631578947\n",
      "epoch 33; loss: 0.18038; train_acc: 0.74074; val_acc: 0.8; test_acc: 0.6842105263157895\n",
      "epoch 34; loss: 0.05965; train_acc: 0.78519; val_acc: 0.73333; test_acc: 0.7631578947368421\n",
      "epoch 35; loss: 0.06217; train_acc: 0.85185; val_acc: 0.93333; test_acc: 0.8421052631578947\n",
      "epoch 36; loss: 0.32416; train_acc: 0.76296; val_acc: 0.73333; test_acc: 0.7368421052631579\n",
      "epoch 37; loss: 0.05532; train_acc: 0.83704; val_acc: 0.86667; test_acc: 0.7631578947368421\n",
      "epoch 38; loss: 0.12479; train_acc: 0.75556; val_acc: 0.66667; test_acc: 0.6842105263157895\n",
      "epoch 39; loss: 0.32786; train_acc: 0.4963; val_acc: 0.53333; test_acc: 0.39473684210526316\n",
      "epoch 40; loss: 0.11155; train_acc: 0.82222; val_acc: 0.8; test_acc: 0.7368421052631579\n",
      "epoch 41; loss: 0.23685; train_acc: 0.8; val_acc: 0.8; test_acc: 0.7105263157894737\n",
      "early stop triggered\n",
      "Accuracy: 0.7105263157894737\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "train_dataset\n",
    "test_dataset\n",
    "k = 10\n",
    "batch_size = 128\n",
    "\n",
    "splits = KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "k_counter = 0\n",
    "fold_logs = {}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "    print(f'Fold {fold}/{k}')\n",
    "    \n",
    "    fold_train = []\n",
    "    for key in train_idx:\n",
    "        fold_train.append(train_dataset[key])\n",
    "\n",
    "    fold_val = [] \n",
    "    for key in val_idx:\n",
    "        fold_val.append(train_dataset[key])\n",
    "\n",
    "    tr = DataLoader(fold_train, batch_size=batch_size, shuffle=True)\n",
    "    vd = DataLoader(fold_val, batch_size=batch_size, shuffle=True)\n",
    "    ts = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(\"=== Base model vs Experiment ===\")\n",
    "    print(\"=== Base model ===\")\n",
    "    fold_logs[fold] = baseTrain(tr, vd, ts, 50, fold=fold)\n",
    "    print(\"=== Experiment model ===\")\n",
    "    fold_logs[fold] = expTrain(tr, vd, ts, 50, fold=fold)\n",
    "    # break\n",
    "    k_counter += 1\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: None, 1: None, 2: None, 3: None, 4: None, 5: None, 6: None, 7: None, 8: None, 9: None}\n"
     ]
    }
   ],
   "source": [
    "print(fold_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Experiment(dataset=dataset, hidden_channels=128)\n",
    "e.load_state_dict(torch.load(\"model-history/GIN-MUTAG/6.experiment_best_model-gin_data-mutag.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
